{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRYXo36IWDry"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`README.md`**\n",
        "\n",
        "## Quantifying Semantic Shift in Financial NLP: A Robust Evaluation Framework\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2510.0205-b31b1b.svg)](https://arxiv.org/abs/2510.00205v1)\n",
        "[![Conference](https://img.shields.io/badge/Conference-ICAIF%20'25-9cf)](https://icaif.acm.org/2025/)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/quantifying_semantic_shift_financial_nlp)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Financial%20NLP-00529B)](https://github.com/chirindaopensource/quantifying_semantic_shift_financial_nlp)\n",
        "[![Primary Data](https://img.shields.io/badge/Data-Financial%20News%20%7C%20Stock%20Returns-lightgrey)](https://github.com/chirindaopensource/quantifying_semantic_shift_financial_nlp)\n",
        "[![Core Method](https://img.shields.io/badge/Method-Regime--Based%20Robustness%20Testing-orange)](https://github.com/chirindaopensource/quantifying_semantic_shift_financial_nlp)\n",
        "[![Key Metrics](https://img.shields.io/badge/Metrics-FCAS%20%7C%20PCS%20%7C%20TSV%20%7C%20NLICS-red)](https://github.com/chirindaopensource/quantifying_semantic_shift_financial_nlp)\n",
        "[![Models](https://img.shields.io/badge/Models-LSTM%20%7C%20Transformers-blueviolet)](https://github.com/chirindaopensource/quantifying_semantic_shift_financial_nlp)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=flat&logo=PyTorch&logoColor=white)](https://pytorch.org/)\n",
        "[![HuggingFace](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue)](https://huggingface.co/)\n",
        "[![Scikit-learn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=flat&logo=scikit-learn&logoColor=white)](https://scikit-learn.org/)\n",
        "[![OpenAI](https://img.shields.io/badge/OpenAI-412991.svg?style=flat&logo=OpenAI&logoColor=white)](https://openai.com/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "--\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/quantifying_semantic_shift_financial_nlp`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Quantifying Semantic Shift in Financial NLP: Robust Metrics for Market Prediction Stability\"** by:\n",
        "\n",
        "*   Zhongtian Sun\n",
        "*   Chenghao Xiao\n",
        "*   Anoushka Harit\n",
        "*   Jongmin Yu\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's novel evaluation suite for financial NLP models. It delivers a modular, auditable, and extensible pipeline that executes the entire research workflow: from rigorous data validation and regime-based partitioning, through multi-architecture model training and feature engineering, to the computation of four novel diagnostic metrics and a comprehensive suite of analytical studies.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callables](#key-callables)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"Quantifying Semantic Shift in Financial NLP: Robust Metrics for Market Prediction Stability.\" The core of this repository is the iPython Notebook `quantifying_semantic_shift_financial_nlp_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings, from initial data validation to the final generation of all analytical tables and figures.\n",
        "\n",
        "The paper introduces a structured evaluation framework to quantify the robustness of financial NLP models under the stress of macroeconomic regime shifts. It argues that standard metrics like MSE are insufficient and proposes four complementary diagnostic metrics to provide a multi-faceted view of model stability. This codebase operationalizes this advanced evaluation suite, allowing users to:\n",
        "-   Rigorously validate and cleanse time-series financial news and market data.\n",
        "-   Systematically partition data into distinct macroeconomic regimes (e.g., Pre-COVID, COVID).\n",
        "-   Perform chronological train-validation-test splits to prevent lookahead bias.\n",
        "-   Train multiple model architectures (LSTM, Text Transformer, Feature-Enhanced MLP) on a per-regime basis.\n",
        "-   Compute the four novel diagnostic metrics: **FCAS**, **PCS**, **TSV**, and **NLICS**.\n",
        "-   Quantify semantic drift between regimes using **Jensen-Shannon Divergence**.\n",
        "-   Conduct a full suite of analyses, including case studies, ablation studies, and cross-sector generalization tests.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in time-series econometrics, natural language processing, and deep learning.\n",
        "\n",
        "**1. Regime-Based Evaluation:**\n",
        "The framework's foundation is the acknowledgment that financial markets are non-stationary. The data-generating process changes over time, particularly during major economic events. The methodology explicitly partitions the data into distinct macroeconomic regimes, $R = \\{r_1, ..., r_K\\}$, and evaluates models within each regime $r_k$. This allows for a precise measurement of performance degradation under structural breaks.\n",
        "\n",
        "**2. The Four Diagnostic Metrics:**\n",
        "The paper introduces four metrics to create a \"Robustness Profile\" beyond simple prediction error:\n",
        "-   **Financial Causal Attribution Score (FCAS):** Measures if a model's prediction direction aligns with simple causal keywords in the source text.\n",
        "    $$\n",
        "    \\text{FCAS} = \\mathbb{E}[\\mathbb{I}(\\text{sign}(\\text{prediction}) = \\text{sign}(\\text{causal\\_cue}))]\n",
        "    $$\n",
        "-   **Patent Cliff Sensitivity (PCS):** Measures the magnitude of change in a model's prediction when the input text is subjected to a controlled semantic perturbation (e.g., \"growth\" -> \"decline\").\n",
        "    $$\n",
        "    \\text{PCS} = \\mathbb{E}[|f_\\theta(\\mathbf{x}) - f_\\theta(\\tilde{\\mathbf{x}})|]\n",
        "    $$\n",
        "-   **Temporal Semantic Volatility (TSV):** Measures the drift in the underlying meaning of the text corpus over time, calculated as the average Euclidean distance between embeddings of consecutive news articles.\n",
        "    $$\n",
        "    \\text{TSV} = \\frac{1}{N-1} \\sum_{i=1}^{N-1} \\|\\phi(\\mathbf{x}_{i+1}) - \\phi(\\mathbf{x}_i)\\|_2\n",
        "    $$\n",
        "-   **NLI-based Logical Consistency Score (NLICS):** Uses a large language model (LLM) to perform Natural Language Inference, assessing whether the model's prediction is a logical entailment of the source news text.\n",
        "    $$\n",
        "    \\text{NLICS} = \\mathbb{E}[\\text{EntailmentScore}(\\text{text}, \\text{Hypothesis}(\\text{prediction}))]\n",
        "    $$\n",
        "\n",
        "**3. Semantic Drift Quantification:**\n",
        "The linguistic shift between any two regimes is quantified using the **Jensen-Shannon (J-S) Divergence** between their respective vocabulary probability distributions. This provides a formal measure of how much the language used in financial news has changed.\n",
        "$$\n",
        "D_{JS}(P, Q) = \\frac{1}{2}D_{KL}(P || M) + \\frac{1}{2}D_{KL}(Q || M), \\quad M = \\frac{1}{2}(P+Q)\n",
        "$$\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`quantifying_semantic_shift_financial_nlp_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Phase Architecture:** The entire pipeline is broken down into 35 distinct, modular tasks, each with its own orchestrator function, covering validation, partitioning, feature engineering, training, inference, and a full suite of analyses.\n",
        "-   **Configuration-Driven Design:** All experimental parameters are managed in an external `config.yaml` file, allowing for easy customization and replication without code changes.\n",
        "-   **Multi-Architecture Support:** Complete training and evaluation pipelines for three distinct model types: a baseline LSTM, a fine-tuned Text Transformer (DistilBERT), and a hybrid Feature-Enhanced MLP.\n",
        "-   **Idempotent and Resumable Pipelines:** Computationally expensive steps, such as model training and LLM-based evaluations, are designed to be idempotent (resumable), saving checkpoints and caching results to prevent loss of progress and redundant computation.\n",
        "-   **Production-Grade Metric Implementation:** Includes a highly performant, asynchronous, and cached implementation for the NLICS metric and a full-pipeline replication for the computationally intensive PCS metric.\n",
        "-   **Comprehensive Analysis Suite:** Implements all analyses from the paper, including J-S divergence, t-SNE visualization, stock-specific case studies, control experiments, and a full N x N cross-sector generalization matrix.\n",
        "-   **Automated Reporting:** Programmatic generation of all key tables and figures from the paper, as well as a final, synthesized analytical report.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Validation & Cleansing (Tasks 1-3):** Ingests and rigorously validates the raw data and `config.yaml`, performs a deep data quality audit, and standardizes all data.\n",
        "2.  **Data Partitioning (Tasks 4-6):** Partitions the data by macroeconomic regime and performs chronological train/val/test splits.\n",
        "3.  **Feature Engineering (Tasks 7-9):** Generates TF-IDF, sentence embedding, and combined feature sets.\n",
        "4.  **Model Training (Tasks 10-15):** Orchestrates the training of all 12 model-regime pairs with early stopping and checkpointing.\n",
        "5.  **Inference & Evaluation (Tasks 16-24):** Generates predictions on all test sets and computes the full suite of five performance and diagnostic metrics.\n",
        "6.  **Analysis & Ablation (Tasks 25-35):** Executes all higher-level analyses, including semantic drift calculation, visualizations, case studies, and ablation studies.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `quantifying_semantic_shift_financial_nlp_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callables\n",
        "\n",
        "The project is designed around a single, top-level user-facing interface function:\n",
        "\n",
        "-   **`execute_quantifying_semantic_shift_study`:** This master orchestrator function runs the entire automated research pipeline from end-to-end. It handles all data processing, model training, and analysis. It also generates the necessary files for the optional, human-in-the-loop entailment model comparison. A single call to this function reproduces the entire computational portion of the project.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   An OpenAI API key set as an environment variable (`OPENAI_API_KEY`) for the NLICS metric.\n",
        "-   Core dependencies: `pandas`, `numpy`, `scipy`, `scikit-learn`, `pyyaml`, `torch`, `transformers`, `sentence-transformers`, `openai`, `matplotlib`, `seaborn`, `tqdm`, `ipython`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/quantifying_semantic_shift_financial_nlp.git\n",
        "    cd quantifying_semantic_shift_financial_nlp\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install -r requirements.txt\n",
        "    ```\n",
        "\n",
        "4.  **Set Environment Variable:**\n",
        "    ```sh\n",
        "    export OPENAI_API_KEY=\"your_api_key_here\"\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires a single `pandas.DataFrame` and a `config.yaml` file. The script includes a helper function to generate a synthetic, structurally correct DataFrame for testing purposes. The required schema is:\n",
        "-   **Index:** A `pandas.MultiIndex` with three levels:\n",
        "    -   `date` (`DatetimeIndex`): The trading date.\n",
        "    -   `ticker` (`object`): The stock ticker.\n",
        "    -   `sector` (`object`): The GICS sector.\n",
        "-   **Columns:**\n",
        "    -   `Open`, `High`, `Low`, `Close`, `Adj Close` (`float64`): Standard market data.\n",
        "    -   `Volume` (`int64`): Daily trading volume.\n",
        "    -   `aggregated_text` (`object`/`str`): Concatenated daily news text. An empty string is a valid value.\n",
        "    -   `target_return` (`float64`): The forward-looking, next-day adjusted close return.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `quantifying_semantic_shift_financial_nlp_draft.ipynb` notebook provides a complete, step-by-step guide. The primary workflow is to call the top-level orchestrator from a `main.py` script or the final cell of the notebook:\n",
        "\n",
        "```python\n",
        "# main.py\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import yaml\n",
        "\n",
        "# Assuming all pipeline functions are in `pipeline.py`\n",
        "from pipeline import execute_quantifying_semantic_shift_study\n",
        "\n",
        "# Load configuration\n",
        "with open(\"config.yaml\", 'r') as f:\n",
        "    study_config = yaml.safe_load(f)\n",
        "\n",
        "# Load data (or generate synthetic data)\n",
        "raw_df = pd.read_pickle(\"data/financial_data.pkl\")\n",
        "\n",
        "# Run the entire study\n",
        "final_artifacts = execute_quantifying_semantic_shift_study(\n",
        "    raw_df=raw_df,\n",
        "    study_config=study_config\n",
        ")\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The `execute_quantifying_semantic_shift_study` function creates a `results/` directory and returns a dictionary of artifact paths:\n",
        "\n",
        "```\n",
        "{\n",
        "    'data_splits': Path('results/data_splits.pkl'),\n",
        "    'training_results': Path('results/training_results.pkl'),\n",
        "    'enriched_predictions': Path('results/enriched_predictions.pkl'),\n",
        "    'robustness_profile': Path('results/robustness_profile.csv'),\n",
        "    'js_divergence_matrix': Path('results/js_divergence_matrix.csv'),\n",
        "    'nli_benchmark_for_annotation': Path('results/nli_benchmark_for_annotation.csv'),\n",
        "    ...\n",
        "}\n",
        "```\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "quantifying_semantic_shift_financial_nlp/\n",
        "│\n",
        "├── quantifying_semantic_shift_financial_nlp_draft.ipynb # Main implementation notebook\n",
        "├── config.yaml                                          # Master configuration file\n",
        "├── requirements.txt                                     # Python package dependencies\n",
        "├── LICENSE                                              # MIT license file\n",
        "└── README.md                                            # This documentation file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can easily modify all experimental parameters, including regime dates, model architectures, feature engineering settings, and LLM prompts, without altering the core Python code.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **Additional Model Architectures:** Integrating other models like FinBERT or more advanced transformer architectures.\n",
        "-   **Alternative Diagnostic Metrics:** Implementing other measures of model robustness, such as influence functions or prediction confidence calibration.\n",
        "-   **Automated Retraining Triggers:** Building a system that uses the computed drift metrics (like TSV or J-S Divergence) to automatically trigger model retraining when a significant regime shift is detected.\n",
        "-   **Dynamic Feature Selection:** Exploring methods for dynamically adjusting feature importance based on the detected market regime.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@inproceedings{sun2025quantifying,\n",
        "  author    = {Sun, Zhongtian and Xiao, Chenghao and Harit, Anoushka and Yu, Jongmin},\n",
        "  title     = {Quantifying Semantic Shift in Financial NLP: Robust Metrics for Market Prediction Stability},\n",
        "  booktitle = {Proceedings of the 6th ACM International Conference on AI in Finance},\n",
        "  series    = {ICAIF '25},\n",
        "  year      = {2025},\n",
        "  publisher = {ACM}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Professional-Grade Implementation of the \"Quantifying Semantic Shift\" Framework.\n",
        "GitHub repository: https://github.com/chirindaopensource/quantifying_semantic_shift_financial_nlp\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Zhongtian Sun, Chenghao Xiao, Anoushka Harit, and Jongmin Yu** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **Pandas, NumPy, SciPy, Scikit-learn, PyTorch, HuggingFace, and Jupyter**, whose work makes complex computational analysis accessible and robust.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of `quantifying_semantic_shift_financial_nlp_draft.ipynb` and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "ivqx6SvuWUiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Quantifying Semantic Shift in Financial NLP: Robust Metrics for Market Prediction Stability*\"\n",
        "\n",
        "Authors: Zhongtian Sun, Chenghao Xiao, Anoushka Harit, Jongmin Yu\n",
        "\n",
        "E-Journal Submission Date: 30 September 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2510.00205v1\n",
        "\n",
        "Paper's Conference Affiliation: The 6th ACM International Conference on Al in Finance\n",
        "\n",
        "Abstract:\n",
        "\n",
        "Financial news is essential for accurate market prediction, but evolving narratives across macroeconomic regimes introduce semantic and causal drift that weaken model reliability. We present an evaluation framework to quantify robustness in financial NLP under regime shifts. The framework defines four metrics: (1) Financial Causal Attribution Score (FCAS) for alignment with causal cues, (2) Patent Cliff Sensitivity (PCS) for sensitivity to semantic perturbations, (3) Temporal Semantic Volatility (TSV) for drift in latent text representations, and (4) NLI-based Logical Consistency Score (NLICS) for entailment coherence. Applied to LSTM and Transformer models across four economic periods (pre-COVID, COVID, post-COVID, and rate hike), the metrics reveal performance degradation during crises. Semantic volatility and Jensen-Shannon divergence correlate with prediction error. Transformers are more affected by drift, while feature-enhanced variants improve generalisation. A GPT-4 case study confirms that alignment-aware models better preserve causal and logical consistency. The framework supports auditability, stress testing, and adaptive retraining in financial AI systems."
      ],
      "metadata": {
        "id": "lDwuZfu9XgKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### **The Core Problem and Motivation**\n",
        "\n",
        "The authors begin by correctly identifying the Achilles' heel of many financial prediction models: **regime shifts**. In econometrics, we call this structural breaks or non-stationarity. Financial markets are not a stationary process; their underlying dynamics change due to macroeconomic events, policy shifts, and crises.\n",
        "\n",
        "The novel contribution here is framing this problem through the lens of **Natural Language Processing (NLP)**. The paper posits that these macroeconomic regime shifts induce a corresponding **\"semantic and causal drift\"** in the language used in financial news.\n",
        "\n",
        "*   **Semantic Drift:** The meaning and connotation of words change. For example, the word \"unprecedented\" had a different weight and context pre-COVID than it did during the pandemic's peak.\n",
        "*   **Causal Drift:** The relationship between a described event and its market impact changes. A central bank announcing \"liquidity injections\" might be interpreted as a strong bullish signal in a stable market, but as a desperate, bearish signal during a financial panic.\n",
        "\n",
        "Existing models, often optimized for predictive accuracy (e.g., minimizing Mean Squared Error), are blind to this drift. They learn statistical correlations from a specific regime and fail, sometimes catastrophically, when that regime ends. The paper's motivation is to move beyond simple accuracy metrics and create a diagnostic framework to measure model *robustness* in the face of these shifts.\n",
        "\n",
        "### **The Proposed Methodological Framework**\n",
        "\n",
        "This is the heart of the paper. Instead of proposing a new state-of-the-art predictive model, the authors propose a framework for *evaluating* existing models. They introduce a suite of four complementary metrics, designed to form a \"Robustness Profile.\"\n",
        "\n",
        "Let's break down each metric:\n",
        "\n",
        "1.  **Financial Causal Attribution Score (FCAS):** This metric attempts to measure if the model's prediction aligns with explicit causal statements in the source text. For a news article stating \"strong earnings cause stock to rise,\" FCAS checks if the model's prediction is positive. This is a crucial step towards interpretability, moving from pure correlation to a semblance of causal alignment. From a computer science perspective, it's a form of model audit against ground-truth reasoning.\n",
        "\n",
        "2.  **Patent Cliff Sensitivity (PCS):** This is a classic perturbation analysis, akin to adversarial testing. The authors perturb the input text with small, semantically meaningful changes (e.g., \"growth\" to \"decline\") and measure the change in the model's output. A robust model should be sensitive to meaningful changes but not overly volatile. This tests the local stability of the function the model has learned.\n",
        "\n",
        "3.  **Temporal Semantic Volatility (TSV):** This metric operates in the model's latent space. It measures the geometric distance (L2 norm) between the embeddings of texts from consecutive time periods. High TSV indicates that the model's internal representation of the world is changing rapidly, signaling instability and potential semantic drift. It's a direct probe into the model's representational geometry over time.\n",
        "\n",
        "4.  **NLI-based Logical Consistency Score (NLICS):** This is perhaps the most modern of the metrics. It uses a powerful Large Language Model (LLM), like GPT-4, as an external \"umpire.\" The process is:\n",
        "    *   Take the input news article (the *premise*).\n",
        "    *   Take the model's prediction and convert it to a natural language statement, e.g., \"The stock price will increase\" (the *hypothesis*).\n",
        "    *   Ask the LLM umpire if the premise entails the hypothesis.\n",
        "    This metric assesses the high-level logical and semantic coherence between the input data and the model's output.\n",
        "\n",
        "Together, these four metrics provide a multi-faceted view of a model's behavior that goes far beyond a single loss value.\n",
        "\n",
        "### **Experimental Design**\n",
        "\n",
        "The authors' empirical setup is sound and well-designed to test their framework.\n",
        "\n",
        "*   **Models:** They wisely choose three representative architectures:\n",
        "    *   **LSTM:** A classic recurrent neural network, good at capturing sequences but less context-aware than modern architectures.\n",
        "    *   **Transformer (DistilBERT):** The modern standard for NLP, with a powerful attention mechanism to capture complex relationships in text.\n",
        "    *   **Feature-based Transformer:** A hybrid model that combines dense semantic embeddings with sparse, traditional features (TF-IDF). This allows them to test the value of feature engineering.\n",
        "*   **Data & Task:** They use a standard financial news dataset (FNSPID) to predict next-day stock returns for 110 S&P 500 companies. This is a canonical and notoriously difficult task in quantitative finance.\n",
        "*   **Regimes:** Crucially, they partition their data into four distinct, economically meaningful periods: Pre-COVID, COVID, Post-COVID, and the recent Rate-Hike cycle. This partitioning is what allows them to explicitly measure performance degradation across structural breaks.\n",
        "\n",
        "### **Key Empirical Findings**\n",
        "\n",
        "The results validate both the severity of the problem and the utility of their proposed metrics.\n",
        "\n",
        "1.  **Performance Confirms the Problem:** As expected, model performance (measured by MSE) degrades significantly during the volatile COVID and Rate-Hike periods. The standard Transformer is particularly brittle, showing a massive spike in error during the COVID crisis.\n",
        "\n",
        "2.  **LSTM vs. Transformer Dynamics:** The LSTM, while perhaps less powerful, proves to be more stable across regimes. The Transformer is more expressive and performs better in stable periods but is highly sensitive to drift. This highlights a classic bias-variance tradeoff: the Transformer's high capacity allows it to overfit to the narrative of a specific regime.\n",
        "\n",
        "3.  **Semantic Drift Correlates with Error:** The authors show that the Jensen-Shannon (JS) divergence—a measure of how different the vocabulary distributions are between regimes—peaks between the most dissimilar periods (e.g., COVID vs. Rate-Hike). This peak in linguistic drift coincides with the peak in model error, providing strong evidence for their core hypothesis.\n",
        "\n",
        "4.  **Value of Feature Augmentation:** The hybrid Transformer model, which uses both modern embeddings and traditional TF-IDF features, demonstrates improved generalization and lower semantic volatility. This suggests that grounding powerful semantic models with more stable, sparse features can enhance robustness.\n",
        "\n",
        "### **Critical Assessment and Concluding Remarks**\n",
        "\n",
        "As a professor, I would give this work high marks for its methodological contribution and thoughtful experimental design. It moves the field in the right direction—away from a myopic focus on accuracy and towards a more holistic understanding of model reliability.\n",
        "\n",
        "**Strengths:**\n",
        "\n",
        "*   **Problem Formulation:** Excellent framing of a classic econometrics problem (structural breaks) within a modern NLP context.\n",
        "*   **Methodological Rigor:** The four-metric framework is comprehensive, well-motivated, and probes different, complementary facets of model failure.\n",
        "*   **Practical Relevance:** This framework is directly applicable for financial institutions looking to stress-test and audit their AI/ML systems before deployment. It provides the tools for responsible AI development in a high-stakes domain.\n",
        "\n",
        "**Areas for Future Inquiry**\n",
        "\n",
        "*   **Actionability:** The framework is diagnostic. The next logical step is to make it prescriptive. How can these metrics be integrated into the training loop? Could we, for instance, use TSV as a regularization term to encourage more stable representations?\n",
        "*   **The Oracle Problem:** The NLICS metric relies on an external LLM (GPT-4). This introduces a dependency on a costly, opaque, and potentially biased \"oracle.\" How do we ensure the umpire itself is robust and unbiased across these same regimes?\n",
        "*   **Endogeneity of Regimes:** The regime boundaries were defined ex-post. A more advanced system might employ online changepoint detection algorithms to identify regime shifts automatically, triggering model recalibration or the activation of a more robust, crisis-specific model.\n",
        "\n",
        "**Final Verdict:**\n",
        "\n",
        "This paper is a significant contribution to the *metrology* of financial AI—the science of measurement. It provides a much-needed toolkit for quantifying the risks associated with semantic and causal drift. By developing robust diagnostics, the authors pave the way for building more adaptive and trustworthy AI systems for finance, which is an absolute necessity for moving these technologies from the research lab to live, mission-critical deployment. An excellent piece of work."
      ],
      "metadata": {
        "id": "Qlkh8oogb7MQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "Hf7AnKdUmo1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================\n",
        "#\n",
        "#  Quantifying Semantic Shift in Financial NLP: A Robust Evaluation Framework\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"Quantifying Semantic Shift in Financial\n",
        "#  NLP: Robust Metrics for Market Prediction Stability\" by Sun et al. (2025).\n",
        "#  It delivers a comprehensive suite of tools for evaluating the robustness of\n",
        "#  financial NLP models under macroeconomic regime shifts, enabling rigorous\n",
        "#  auditing, stress testing, and governance of AI systems in non-stationary\n",
        "#  market environments.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Regime-based data partitioning and chronological cross-validation.\n",
        "#  • Feature engineering pipeline for sparse (TF-IDF) and dense (Transformer) text representations.\n",
        "#  • Training and inference pipelines for LSTM, Text Transformer, and Feature-Enhanced models.\n",
        "#  • Implementation of four novel diagnostic metrics for model robustness:\n",
        "#    1. Financial Causal Attribution Score (FCAS): Alignment with textual causal cues.\n",
        "#    2. Patent Cliff Sensitivity (PCS): Stability under semantic perturbation.\n",
        "#    3. Temporal Semantic Volatility (TSV): Drift in latent text representations.\n",
        "#    4. NLI-based Logical Consistency Score (NLICS): Coherence via language models.\n",
        "#  • Quantitative analysis of semantic drift using Jensen-Shannon Divergence.\n",
        "#  • Comprehensive ablation and cross-domain generalization studies.\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Modular, reusable, and idempotent orchestrators for managing complex experiments.\n",
        "#  • Robust data validation, cleaning, and artifact management with checksums.\n",
        "#  • High-performance deep learning pipeline with GPU acceleration and mixed precision.\n",
        "#  • Asynchronous, cached, and rate-limited API interaction for LLM-based metrics.\n",
        "#  • Professional-grade visualization and reporting of all analytical results.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Sun, Z., Xiao, C., Harit, A., & Yu, J. (2025). Quantifying Semantic Shift\n",
        "#  in Financial NLP: Robust Metrics for Market Prediction Stability.\n",
        "#  In Proceedings of the 6th ACM International Conference on AI in Finance (ICAIF '25).\n",
        "#  arXiv preprint arXiv:2510.00205. https://arxiv.org/abs/2510.00205v1\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Standard Library Imports ---\n",
        "import asyncio\n",
        "import hashlib\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
        "from datetime import datetime\n",
        "from itertools import combinations, permutations\n",
        "from pathlib import Path\n",
        "from typing import (Any, Callable, Dict, List, Optional, Set, Tuple, Union)\n",
        "\n",
        "# --- Third-Party Library Imports ---\n",
        "\n",
        "# Core Scientific Computing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.stats import entropy, spearmanr\n",
        "\n",
        "# Machine Learning (Scikit-learn)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Deep Learning (PyTorch and associated)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Natural Language Processing (HuggingFace and SentenceTransformers)\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import (AutoTokenizer, DistilBertModel, PreTrainedModel,\n",
        "                          PreTrainedTokenizerBase, pipeline, Pipeline)\n",
        "\n",
        "# LLM API Interaction\n",
        "from openai import AsyncOpenAI, RateLimitError\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Progress Bars\n",
        "from tqdm import tqdm\n",
        "from tqdm.auto import tqdm as auto_tqdm\n",
        "from tqdm.asyncio import tqdm as async_tqdm\n",
        "\n",
        "# Define type aliases for clarity and maintainability.\n",
        "DataSplits = Dict[str, Dict[str, pd.DataFrame]]\n",
        "TfidfFeatures = Dict[str, Dict[str, csr_matrix]]\n",
        "EmbeddingFeatures = Dict[str, Dict[str, np.ndarray]]\n",
        "CombinedFeatures = Dict[str, Dict[str, np.ndarray]]\n",
        "\n",
        "# Configure a logger for clear, standardized output.\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n"
      ],
      "metadata": {
        "id": "IS10n-NCm3_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "ZKt_ncYxnE94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### **Functional Specification Document**\n",
        "\n",
        "#### **Phase 1: Configuration Validation and Data Quality Assurance**\n",
        "\n",
        "*   **Callable:** `run_config_validation_suite`\n",
        "    *   **Inputs:** The main `study_config` dictionary.\n",
        "    *   **Processes:**\n",
        "        1.  Calls `validate_regime_definitions` to parse date strings, ensure they are in chronological order, and verify that the time intervals `[start_date, end_date]` are non-overlapping.\n",
        "        2.  Calls `validate_data_split_ratios` to confirm that the `training`, `validation`, and `testing` ratios are valid probabilities that sum to 1.0, using a numerically stable comparison.\n",
        "        3.  Calls `validate_feature_engineering_config` to check that `max_features` is a positive integer, that the specified HuggingFace models are accessible, and that the configured dimensions match the models' actual output dimensions.\n",
        "    *   **Outputs:** None. Raises a `ValueError` with a consolidated report if any validation fails.\n",
        "    *   **Transformation:** This function validates the configuration but does not transform data.\n",
        "    *   **Research Role:** Implements the preliminary sanity checks on all experimental parameters defined in **Section 4.2, 5.1, and Table 1**. It ensures the entire study is based on a valid and coherent set of hyperparameters and design choices before any computation begins.\n",
        "\n",
        "*   **Callable:** `run_dataframe_validation_suite`\n",
        "    *   **Inputs:** The raw `pd.DataFrame`.\n",
        "    *   **Processes:**\n",
        "        1.  Calls `validate_multi_index_integrity` to verify the DataFrame has a unique, sorted, 3-level MultiIndex of `(date, ticker, sector)` with the correct data types.\n",
        "        2.  Calls `validate_column_schema` to ensure the presence and correct `dtype` of all required columns and checks for non-positive values in price/volume columns.\n",
        "        3.  Calls `validate_target_return_construction` to rigorously verify that the `target_return` column was correctly computed as a forward-looking return, preventing lookahead bias. It mathematically confirms the relationship `target_return[t] = (Adj_Close[t+1] - Adj_Close[t]) / Adj_Close[t]` on a per-ticker basis.\n",
        "    *   **Outputs:** None. Raises a `ValueError` if validation fails.\n",
        "    *   **Transformation:** This function validates the input data structure but does not transform it.\n",
        "    *   **Research Role:** Implements the critical data integrity checks required for any quantitative financial study. It ensures the raw data conforms to the expected panel data structure and that the target variable is constructed without methodological flaws, as described in **Section 5.1 (\"News-Price Alignment\")**.\n",
        "\n",
        "*   **Callable:** `run_data_quality_and_cleansing_suite`\n",
        "    *   **Inputs:** The validated raw `pd.DataFrame`.\n",
        "    *   **Processes:**\n",
        "        1.  Calls `assess_missing_data` to generate a report distinguishing between `NaN` values and meaningful empty strings (`''`).\n",
        "        2.  Calls `validate_financial_data_consistency` to check for valid OHLC relationships and flag unrealistic daily returns.\n",
        "        3.  Calls `clean_and_standardize_text_data` to perform cleansing operations: it replaces `NaN`s in the text column with `''`, standardizes encoding to UTF-8, and removes null bytes.\n",
        "    *   **Outputs:** A cleaned `pd.DataFrame`.\n",
        "    *   **Transformation:** Transforms the input DataFrame by cleaning and standardizing its `aggregated_text` column.\n",
        "    *   **Research Role:** Implements the data pre-processing steps described in **Section 5.1 (\"Text Processing\")**. It prepares the raw data for the subsequent feature engineering and modeling phases.\n",
        "\n",
        "#### **Phase 2: Data Partitioning**\n",
        "\n",
        "*   **Callable:** `run_regime_assignment_suite`\n",
        "    *   **Inputs:** The cleaned `pd.DataFrame`, `study_config`.\n",
        "    *   **Processes:**\n",
        "        1.  Calls `assign_regime_labels` which maps each row's date to a macroeconomic regime (`Pre-COVID`, `COVID`, etc.) based on the intervals in `study_config`.\n",
        "        2.  Filters out any data points that do not fall within a defined regime.\n",
        "        3.  Calls `validate_regime_assignment` to confirm that the resulting DataFrame contains data for all expected regimes and that the sample counts are adequate.\n",
        "    *   **Outputs:** A `pd.DataFrame` with an added `regime` column, containing only data from the specified periods.\n",
        "    *   **Transformation:** Transforms the time-series DataFrame by annotating each row with a regime label and filtering it temporally.\n",
        "    *   **Research Role:** Implements the \"Regime Assignment\" step described in **Section 5.1** and **Table 1**, which is the core of the paper's experimental design for studying model performance under structural breaks.\n",
        "\n",
        "*   **Callable:** `run_chronological_splitting_suite`\n",
        "    *   **Inputs:** The regime-assigned `pd.DataFrame`, `study_config`.\n",
        "    *   **Processes:**\n",
        "        1.  Calls `create_regime_data_subsets`, which iterates through each unique regime.\n",
        "        2.  For each regime, it calls `perform_chronological_split`, which sorts the regime's data by date and partitions it into non-overlapping `training` (first 60%), `validation` (next 20%), and `testing` (final 20%) sets.\n",
        "        3.  Calls `validate_split_quality` to perform a final, critical check ensuring there is no temporal overlap between the splits, guaranteeing the absence of lookahead bias.\n",
        "    *   **Outputs:** A nested dictionary `DataSplits` containing the 12 DataFrame subsets.\n",
        "    *   **Transformation:** Transforms the regime-annotated DataFrame into a structured collection of 12 distinct train/validation/test sets.\n",
        "    *   **Research Role:** Implements the \"Chronological Splits\" methodology described in **Section 5.1**. This is the fundamental procedure for correctly evaluating time-series models.\n",
        "\n",
        "*   **Callable:** `run_cross_regime_consistency_suite`\n",
        "    *   **Inputs:** The regime-assigned `pd.DataFrame`.\n",
        "    *   **Processes:**\n",
        "        1.  Calls `analyze_ticker_consistency` to check for changes in the asset universe across regimes.\n",
        "        2.  Calls `analyze_return_distribution_stability` to compute the first four statistical moments of returns in each regime and performs pairwise Kolmogorov-Smirnov tests to formally check if the return distributions are statistically different.\n",
        "        3.  Calls `analyze_news_coverage_consistency` to quantify the availability and volume of text data in each regime.\n",
        "    *   **Outputs:** None. Prints a detailed diagnostic report to the console.\n",
        "    *   **Transformation:** This is a pure analysis function; it does not transform data.\n",
        "    *   **Research Role:** This function provides the quantitative justification for the regime-based approach. By demonstrating that the statistical properties of both the target variable (`target_return`) and the input features (`aggregated_text`) are significantly different across regimes, it validates the paper's core premise that the data generating process is non-stationary.\n",
        "\n",
        "#### **Phase 3: Feature Engineering**\n",
        "\n",
        "*   **Callable:** `run_tfidf_vectorization_suite`\n",
        "    *   **Inputs:** `DataSplits` dictionary, `study_config`.\n",
        "    *   **Processes:**\n",
        "        1.  Calls `create_and_fit_tfidf_vectorizer`, which first assembles a global corpus from all *training* splits across all regimes.\n",
        "        2.  It fits a single `TfidfVectorizer` on this global corpus to create a unified vocabulary.\n",
        "        3.  It then uses this single fitted vectorizer to `.transform()` the text from all 12 data subsets.\n",
        "        4.  Calls `validate_tfidf_features` to check the shape, sparsity, and L2 normalization of the resulting sparse matrices.\n",
        "    *   **Outputs:** The fitted `TfidfVectorizer` object and a nested dictionary `TfidfFeatures` containing the 12 sparse feature matrices.\n",
        "    *   **Transformation:** Transforms the raw text in the `DataSplits` into sparse numerical feature matrices.\n",
        "    *   **Research Role:** Implements the TF-IDF feature engineering described in **Section 4.2** and **5.1**. The use of a single global vocabulary is a critical, rigorous step to ensure feature comparability across all regimes.\n",
        "\n",
        "*   **Callable:** `run_embedding_extraction_suite`\n",
        "    *   **Inputs:** `DataSplits` dictionary, `study_config`.\n",
        "    *   **Processes:**\n",
        "        1.  Calls `initialize_sentence_transformer` to load the pre-trained `all-MiniLM-L6-v2` model and move it to the GPU.\n",
        "        2.  Calls `extract_sentence_embeddings`, which iterates through all 12 data subsets and uses the model's `.encode()` method in batches to convert the text into dense embedding vectors.\n",
        "        3.  Calls `validate_embedding_features` to check the shape, numerical integrity, and L2 normalization of the resulting NumPy arrays.\n",
        "    *   **Outputs:** A nested dictionary `EmbeddingFeatures` containing the 12 dense feature matrices.\n",
        "    *   **Transformation:** Transforms the raw text in the `DataSplits` into dense numerical feature matrices.\n",
        "    *   **Research Role:** Implements the sentence embedding feature engineering described in **Section 4.2** and **5.1**.\n",
        "\n",
        "*   **Callable:** `run_feature_concatenation_suite`\n",
        "    *   **Inputs:** `TfidfFeatures` dictionary, `EmbeddingFeatures` dictionary.\n",
        "    *   **Processes:**\n",
        "        1.  Calls `concatenate_features`, which iterates through all 12 splits. For each split, it converts the sparse TF-IDF matrix to a dense array and horizontally stacks it with the corresponding dense embedding array.\n",
        "        2.  The resulting matrix implements the formulation $\\mathbf{x}_{\\text{combined}} = [\\mathbf{v}_{\\text{tfidf}} \\mid\\mid \\mathbf{e}_{\\text{sentence}}]$.\n",
        "        3.  Calls `validate_combined_features` to check the shape and numerical integrity of the final concatenated matrices.\n",
        "    *   **Outputs:** A nested dictionary `CombinedFeatures` containing the 12 combined feature matrices.\n",
        "    *   **Transformation:** Transforms two sets of feature matrices into a single, unified set of feature matrices for the hybrid model.\n",
        "    *   **Research Role:** Implements the feature set for the \"Feature-based Transformer\" model described in **Section 4.2**.\n",
        "\n",
        "#### **Phase 4 & 5: Model Training**\n",
        "\n",
        "*   **Callable:** `run_regime_specific_training_pipeline`\n",
        "    *   **Inputs:** All data splits and feature set dictionaries, `study_config`.\n",
        "    *   **Processes:** This is a master orchestrator that iterates through all 12 regime-model pairs. For each pair, it:\n",
        "        1.  Calls `_get_model_and_features_for_run` to instantiate the correct model class (`LSTMRegressionModel`, `TextTransformerRegressionModel`, or `FeatureEnhancedMLP`) and select the corresponding feature set.\n",
        "        2.  Calls `create_dataloaders` to prepare the data for batch processing.\n",
        "        3.  Calls `setup_optimization_components` to create the `MSELoss` criterion, `Adam` optimizer, and `ReduceLROnPlateau` scheduler.\n",
        "        4.  Calls `run_training_orchestrator`, which executes the main training loop, incorporating `train_epoch` and `validate_epoch` helpers, early stopping, and checkpointing of the best model.\n",
        "        5.  It is idempotent, checking for existing checkpoints to allow for resumption.\n",
        "    *   **Outputs:** A dictionary `training_results` containing the file paths to the 12 best-performing model checkpoints and their training histories.\n",
        "    *   **Transformation:** This is the core computational step of the project, transforming the feature sets and untrained model architectures into a set of 12 trained, specialized predictive models.\n",
        "    *   **Research Role:** Implements the entire model training workflow described across **Section 4.2, 5.3, and 5.5**.\n",
        "\n",
        "#### **Phase 6: Prediction and Evaluation**\n",
        "\n",
        "*   **Callable:** `run_inference_pipeline`\n",
        "    *   **Inputs:** `training_results` dictionary, all data splits and feature sets, `study_config`.\n",
        "    *   **Processes:** Iterates through the 12 trained model-regime pairs. For each pair, it:\n",
        "        1.  Loads the appropriate model architecture and its saved checkpoint weights.\n",
        "        2.  Prepares the `test` set `DataLoader` for that pair.\n",
        "        3.  Calls `generate_predictions_for_split`, which runs the model in inference mode (`model.eval()`, `torch.no_grad()`) to generate predictions on the unseen test data.\n",
        "        4.  Collates all predictions into a single, tidy `pd.DataFrame`.\n",
        "    *   **Outputs:** A single `pd.DataFrame` containing predictions, ground truth, and metadata for all 12 test sets.\n",
        "    *   **Transformation:** Transforms the test set features into model predictions using the trained models.\n",
        "    *   **Research Role:** Implements the model evaluation step, generating the raw predictions that are the basis for all subsequent analysis.\n",
        "\n",
        "*   **Callable:** `enrich_and_store_predictions`\n",
        "    *   **Inputs:** The tidy predictions `pd.DataFrame`.\n",
        "    *   **Processes:** Adds per-sample diagnostic columns (`squared_error`, `absolute_error`, `directional_accuracy`), embeds metadata, and saves the resulting enriched DataFrame to a file with a corresponding SHA256 checksum file for integrity validation.\n",
        "    *   **Outputs:** The enriched `pd.DataFrame`.\n",
        "    *   **Transformation:** Augments the predictions DataFrame with additional analytical columns.\n",
        "    *   **Research Role:** Creates a robust, auditable, and comprehensive artifact of the model's predictive output, which is a best practice for reproducible research.\n",
        "\n",
        "*   **Callable:** `run_mse_evaluation_suite`\n",
        "    *   **Inputs:** The enriched predictions `pd.DataFrame`.\n",
        "    *   **Processes:**\n",
        "        1.  Calls `compute_and_validate_mse`, which calculates the Mean Squared Error for each of the 12 experimental cells by implementing the equation $\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2$ on a per-group basis. It also computes bootstrap confidence intervals for these estimates.\n",
        "        2.  Calls `generate_mse_summary_table` to format the results into a publication-quality table.\n",
        "    *   **Outputs:** The raw MSE `pd.DataFrame`. Displays a styled table.\n",
        "    *   **Transformation:** Transforms the per-sample predictions into an aggregate performance summary.\n",
        "    *   **Research Role:** Implements the baseline performance evaluation and generates the results for **Table 3**.\n",
        "\n",
        "#### **Phase 7-11: Full Analysis Suite**\n",
        "\n",
        "*   **Callable:** `run_diagnostic_metrics_orchestrator`\n",
        "    *   **Inputs:** All major data artifacts.\n",
        "    *   **Processes:** This is a master orchestrator for the four diagnostic metrics. It calls:\n",
        "        1.  `compute_fcas`: Implements $\\text{FCAS} = \\mathbb{E}[\\mathbb{I}(\\text{sign}(\\hat{y}) = \\text{sign}(c))]$ by counting keyword matches to determine the sign of the causal cue `c`.\n",
        "        2.  `compute_pcs`: Implements $\\text{PCS} = \\mathbb{E}[|f_\\theta(\\mathbf{x}) - f_\\theta(\\tilde{\\mathbf{x}})|]$ by creating perturbed texts `\\tilde{\\mathbf{x}}`, re-running the entire feature and inference pipeline to get `f_\\theta(\\tilde{\\mathbf{x}})`, and averaging the absolute differences.\n",
        "        3.  `compute_tsv`: Implements $\\text{TSV} = \\frac{1}{N-1} \\sum \\|\\phi(\\mathbf{x}_{i+1}) - \\phi(\\mathbf{x}_i)\\|_2$ by calculating the mean Euclidean distance between consecutive chronologically sorted sentence embeddings.\n",
        "        4.  `compute_nlics`: Implements $\\text{NLICS} = \\mathbb{E}[\\text{EntailmentScore}(\\mathbf{x}, H(\\hat{y}))]$ by using the OpenAI API to evaluate the logical entailment between the news text `\\mathbf{x}` and a hypothesis `H` generated from the prediction `\\hat{y}`.\n",
        "    *   **Outputs:** A single `pd.DataFrame`, the `robustness_profile`, containing all five metrics for all 12 experimental cells.\n",
        "    *   **Transformation:** Transforms the predictions and source data into the final, multi-faceted performance profile.\n",
        "    *   **Research Role:** Implements the core contribution of the paper: the calculation of the four novel diagnostic metrics as defined in **Section 3, Equations (2-5)**.\n",
        "\n",
        "*   **Callable:** `run_diagnostic_validation_suite`\n",
        "    *   **Inputs:** The `robustness_profile` DataFrame.\n",
        "    *   **Processes:** Performs sanity checks on the final metrics, ensuring their values fall within their theoretical ranges (e.g., FCAS in `[0, 1]`). It also computes a diagnostic correlation matrix and generates a styled, publication-quality summary table.\n",
        "    *   **Outputs:** The validated `robustness_profile` and a styled table object.\n",
        "    *   **Transformation:** This is a validation and reporting function.\n",
        "    *   **Research Role:** Serves as the final quality assurance step on the computed results.\n",
        "\n",
        "*   **Callable:** `compute_js_divergence_matrix`\n",
        "    *   **Inputs:** `DataSplits` dictionary, `study_config`.\n",
        "    *   **Processes:** Implements the Jensen-Shannon Divergence calculation to quantify semantic shift. It creates a unified vocabulary, builds a probability distribution of term usage for each regime, and then calculates the pairwise J-S divergence using the formula $D_{JS}(P, Q) = 0.5 \\times D_{KL}(P || M) + 0.5 \\times D_{KL}(Q || M)$, where $M=0.5(P+Q)$.\n",
        "    *   **Outputs:** A symmetric `pd.DataFrame` containing the pairwise J-S divergence values.\n",
        "    *   **Transformation:** Transforms the raw text corpora of the regimes into a matrix of statistical distances.\n",
        "    *   **Research Role:** Implements the \"Vocabulary Shift\" analysis described in **Section 6.2** and generates the data for **Figure 2**.\n",
        "\n",
        "*   **Callable:** `run_tsne_visualization_suite`\n",
        "    *   **Inputs:** `DataSplits` and `EmbeddingFeatures` dictionaries.\n",
        "    *   **Processes:** Aggregates all test set sentence embeddings, performs stratified subsampling for efficiency, and applies the t-SNE algorithm to project the 384-dimensional vectors into a 2D space. It then generates scatter plots colored by regime and sector.\n",
        "    *   **Outputs:** A `pd.DataFrame` of the 2D coordinates. Displays the plots.\n",
        "    *   **Transformation:** Transforms high-dimensional semantic vectors into a low-dimensional visualization.\n",
        "    *   **Research Role:** Implements the visualization of the \"Regime-Aware Representations\" described in **Section 6.4** and reproduces **Figures 3 and 4**.\n",
        "\n",
        "*   **Callable:** `run_cross_regime_analysis_suite`\n",
        "    *   **Inputs:** `js_divergence_matrix`, `robustness_profile`.\n",
        "    *   **Processes:** Aligns the J-S divergence values with the absolute difference in MSE between corresponding regime pairs. It then computes the Spearman rank correlation and p-value to test the association between semantic drift and model performance degradation.\n",
        "    *   **Outputs:** A `pd.DataFrame` summarizing the correlation results. Displays a plot.\n",
        "    *   **Transformation:** Synthesizes two different analytical results into a single statistical test and visualization.\n",
        "    *   **Research Role:** Implements the core hypothesis test of the paper, as discussed in **Section 6.2**, linking semantic drift to prediction error.\n",
        "\n",
        "*   **Callable:** `run_stock_specific_case_study`\n",
        "    *   **Inputs:** All major data artifacts.\n",
        "    *   **Processes:** Filters all data artifacts for a specific list of tickers (e.g., 'JPM', 'AAPL') and then re-runs the entire diagnostic metric computation pipeline (`compute_fcas`, `compute_pcs`, etc.) on these small, single-stock subsets.\n",
        "    *   **Outputs:** A `pd.DataFrame` containing the diagnostic metrics for the specified stocks across all regimes.\n",
        "    *   **Transformation:** Filters the entire dataset and re-applies the full analysis to produce a granular, stock-level view.\n",
        "    *   **Research Role:** Implements the case study analysis presented in **Section 6.6** and **Table 6**.\n",
        "\n",
        "*   **Callable:** `perform_metric_ablation_analysis`, `perform_feature_augmentation_ablation`, `perform_entailment_model_ablation`\n",
        "    *   **Inputs:** Various result artifacts.\n",
        "    *   **Processes:** These functions implement the ablation studies described in **Section 7** and presented in **Tables 7, 8, and 9**. They involve re-running parts of the pipeline on different data slices (e.g., cross-sector) or with different components (e.g., BART-NLI) and presenting the comparative results.\n",
        "    *   **Outputs:** Formatted `pd.DataFrame` summary tables.\n",
        "    *   **Transformation:** These are high-level analytical functions that orchestrate new, smaller experiments to answer specific questions about the components of the main framework.\n",
        "    *   **Research Role:** Implements the full suite of ablation studies from **Section 7**.\n",
        "\n",
        "*   **Callable:** `run_control_experiment`\n",
        "    *   **Inputs:** All major data artifacts.\n",
        "    *   **Processes:** Identifies a specific event type (earnings reports) across all regimes. It then re-runs the PCS and TSV metric calculations on these homogenous subsets, both within and across regimes, to disentangle situational from linguistic drift.\n",
        "    *   **Outputs:** A `pd.DataFrame` summary table.\n",
        "    *   **Transformation:** Performs highly specific data filtering and re-applies metric computations.\n",
        "    *   **Research Role:** Implements the sophisticated control experiment described in **Section 6.5** and **Table 5**.\n",
        "\n",
        "*   **Callable:** `run_multi_sector_robustness_validation`\n",
        "    *   **Inputs:** All major data artifacts.\n",
        "    *   **Processes:** Orchestrates a large-scale experiment by iterating through all possible pairs of GICS sectors. For each pair, it trains a model on the source sector and evaluates it on the target sector, reusing the core training and inference functions. It calculates a \"transferability ratio\" to normalize the results.\n",
        "    *   **Outputs:** A dictionary of `pd.DataFrame` transferability matrices.\n",
        "    *   **Transformation:** Orchestrates a massive set of training and evaluation runs to produce a comprehensive summary of model generalization.\n",
        "    *   **Research Role:** Implements the \"Sector Transferability\" analysis described in **Section 6.3** and **Table 4**, but extends it to a full, comprehensive matrix.\n",
        "\n",
        "#### **Top-Level Orchestrators**\n",
        "\n",
        "*   **Callable:** `run_full_research_pipeline`\n",
        "    *   **Inputs:** Raw `pd.DataFrame`, `study_config`.\n",
        "    *   **Processes:** Serves as the master orchestrator for all *automated* computational tasks. It calls the orchestrator for each phase in the correct sequence, from data validation to the final analyses, managing the flow of artifacts between them. It is designed to be idempotent and resumable.\n",
        "    *   **Outputs:** A dictionary of file paths to all major generated artifacts.\n",
        "    *   **Transformation:** Orchestrates the entire end-to-end transformation of raw data into a complete set of research results.\n",
        "    *   **Research Role:** Represents the full, reproducible script to run the entire computational portion of the paper.\n",
        "\n",
        "*   **Callable:** `execute_quantifying_semantic_shift_study`\n",
        "    *   **Inputs:** Raw `pd.DataFrame`, `study_config`, and user flags.\n",
        "    *   **Processes:** Acts as the ultimate user-facing entry point. It calls `run_full_research_pipeline` to perform the main computation. It then manages the optional, human-dependent `run_entailment_ablation_analysis` step by verifying the existence of the required annotated file. Finally, it calls the master reporting and synthesis functions to generate the final outputs.\n",
        "    *   **Outputs:** A dictionary containing all key final artifacts and reports.\n",
        "    *   **Transformation:** Orchestrates the entire study lifecycle, including the interface with the manual annotation step.\n",
        "    *   **Research Role:** Represents the complete, user-runnable script that reproduces the entire study, including its human-in-the-loop component.\n",
        "\n",
        "\n",
        "<br><br>\n",
        "\n",
        "### **Usage Example: Final `main.py`**\n",
        "\n",
        "Below is a Python script which shows how to use the orchestrator callables:\n",
        "\n",
        "```python\n",
        "#!/usr/bin/env python3\n",
        "# ==============================================================================\n",
        "#\n",
        "#  Main Execution Script for \"Quantifying Semantic Shift in Financial NLP\"\n",
        "#\n",
        "#  This script serves as the main entry point to run the entire research\n",
        "#  pipeline. It includes a synthetic data generator to ensure the script is\n",
        "#  fully executable out-of-the-box for demonstration and validation purposes.\n",
        "#\n",
        "#  Workflow:\n",
        "#  1. Generates a synthetic, structurally correct illustrative DataFrame.\n",
        "#  2. Loads the main study configuration from `config.yaml`.\n",
        "#  3. Resolves secrets (like API keys) from environment variables.\n",
        "#  4. Calls the main pipeline orchestrator `execute_quantifying_semantic_shift_study`.\n",
        "#  5. Provides a commented-out example for running the separate human-in-the-loop\n",
        "#     analysis after manual annotation is complete.\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yaml\n",
        "\n",
        "# Assume all previously defined orchestrator functions are in a module\n",
        "# called `pipeline` for clean importing.\n",
        "# from pipeline import (\n",
        "#     execute_quantifying_semantic_shift_study,\n",
        "#     run_entailment_ablation_analysis\n",
        "# )\n",
        "\n",
        "# For this self-contained example, we assume the functions are in the same file.\n",
        "\n",
        "\n",
        "def _create_synthetic_input_data(\n",
        "    output_path: Path,\n",
        "    num_tickers: int = 10,\n",
        "    days: int = 500\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates a realistic, synthetic DataFrame matching the required input schema.\n",
        "\n",
        "    This function creates a mock dataset for demonstrating and testing the full\n",
        "    pipeline. The data is structurally and methodologically sound, but the\n",
        "    values are synthetic and not suitable for actual model training.\n",
        "\n",
        "    Args:\n",
        "        output_path: The path to save the generated DataFrame.\n",
        "        num_tickers: The number of synthetic tickers to create.\n",
        "        days: The number of trading days to generate.\n",
        "\n",
        "    Returns:\n",
        "        The generated pandas DataFrame.\n",
        "    \"\"\"\n",
        "    logging.info(\"Generating synthetic input data...\")\n",
        "    \n",
        "    # --- 1. Define Universe ---\n",
        "    tickers = [f\"TICK{i}\" for i in range(num_tickers)]\n",
        "    sectors = ['Technology', 'Financials', 'Health Care']\n",
        "    ticker_to_sector = {t: sectors[i % len(sectors)] for i, t in enumerate(tickers)}\n",
        "    \n",
        "    # --- 2. Create Multi-Index ---\n",
        "    dates = pd.to_datetime(pd.date_range(end='2023-01-01', periods=days, freq='B'))\n",
        "    index = pd.MultiIndex.from_product([dates, tickers], names=['date', 'ticker'])\n",
        "    \n",
        "    # --- 3. Generate Synthetic Data ---\n",
        "    df = pd.DataFrame(index=index)\n",
        "    df['sector'] = df.index.get_level_values('ticker').map(ticker_to_sector)\n",
        "    df = df.set_index('sector', append=True) # Add sector to the index\n",
        "\n",
        "    # Generate a random walk for adjusted close prices for each ticker\n",
        "    np.random.seed(42)\n",
        "    # Use cumsum on random returns to simulate a price series\n",
        "    returns = np.random.randn(len(dates), num_tickers) * 0.02\n",
        "    price_series = 100 * np.exp(np.cumsum(returns, axis=0))\n",
        "    df['Adj Close'] = price_series.flatten(order='F')\n",
        "\n",
        "    # Create consistent OHLC data\n",
        "    noise = np.random.rand(len(df))\n",
        "    df['Open'] = df['Adj Close'] * (1 - 0.01 * noise)\n",
        "    df['Close'] = df['Adj Close'] * (1 + 0.01 * noise)\n",
        "    df['High'] = pd.concat([df['Open'], df['Close'], df['Adj Close'] * (1 + 0.02 * noise)], axis=1).max(axis=1)\n",
        "    df['Low'] = pd.concat([df['Open'], df['Close'], df['Adj Close'] * (1 - 0.02 * noise)], axis=1).min(axis=1)\n",
        "    df['Volume'] = np.random.randint(1_000_000, 10_000_000, size=len(df))\n",
        "\n",
        "    # --- 4. Generate Synthetic Text Data ---\n",
        "    sentences = [\n",
        "        \"The company reported strong growth and record earnings this quarter.\",\n",
        "        \"Concerns about a decline in revenue led to a stock price fall.\",\n",
        "        \"Quarterly results show a significant profit increase, beating all forecasts.\",\n",
        "        \"The firm announced a major expansion, signaling positive future guidance.\",\n",
        "        \"A weak economic outlook may cause a contraction in the next fiscal period.\",\n",
        "        \"The stock will likely miss its earnings target.\",\n",
        "        \"\" # CRITICAL: Include empty strings to simulate no-news days\n",
        "    ]\n",
        "    df['aggregated_text'] = np.random.choice(sentences, size=len(df), p=[0.15, 0.15, 0.1, 0.1, 0.1, 0.1, 0.3])\n",
        "\n",
        "    # --- 5. Generate Target Variable with Correct Methodology ---\n",
        "    # This is the crucial step: create the forward-looking return.\n",
        "    df['target_return'] = df.groupby(level='ticker')['Adj Close'].pct_change(1).shift(-1) * 100\n",
        "    \n",
        "    # Sort the index for pipeline compatibility.\n",
        "    df.sort_index(inplace=True)\n",
        "    \n",
        "    # Save the synthetic data to the specified path.\n",
        "    df.to_pickle(output_path)\n",
        "    logging.info(f\"Synthetic data with shape {df.shape} saved to '{output_path}'.\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def load_configuration(config_path: Path) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Loads the YAML configuration file and resolves environment variables.\n",
        "    \"\"\"\n",
        "    # Open and read the YAML configuration file.\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    \n",
        "    # --- Rigorous Secret Management ---\n",
        "    api_key_placeholder = config['diagnostics']['nlics_metric']['api_key']\n",
        "    if api_key_placeholder == \"ENV_OPENAI_API_KEY\":\n",
        "        api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise ValueError(\n",
        "                \"The 'OPENAI_API_KEY' environment variable is not set. \"\n",
        "                \"Please set it to run the NLICS metric computation.\"\n",
        "            )\n",
        "        config['diagnostics']['nlics_metric']['api_key'] = api_key\n",
        "        \n",
        "    return config\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to execute the entire research study.\n",
        "    \"\"\"\n",
        "    # --- 1. Configure Logging ---\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "        datefmt='%Y-%m-%d %H:%M:%S'\n",
        "    )\n",
        "    \n",
        "    # --- 2. Define File Paths ---\n",
        "    config_path = Path(\"config.yaml\")\n",
        "    data_dir = Path(\"data\")\n",
        "    data_dir.mkdir(exist_ok=True)\n",
        "    raw_data_path = data_dir / \"synthetic_financial_data.pkl\"\n",
        "    results_dir = Path(\"results\")\n",
        "\n",
        "    # --- 3. Load Inputs: Configuration and Data ---\n",
        "    logging.info(f\"Loading configuration from '{config_path}'...\")\n",
        "    if not config_path.exists():\n",
        "        raise FileNotFoundError(f\"Configuration file not found at '{config_path}'. Please create it.\")\n",
        "    study_config = load_configuration(config_path)\n",
        "\n",
        "    # Generate synthetic data if it doesn't exist.\n",
        "    if not raw_data_path.exists():\n",
        "        _create_synthetic_input_data(raw_data_path)\n",
        "    \n",
        "    logging.info(f\"Loading data from '{raw_data_path}'...\")\n",
        "    raw_df = pd.read_pickle(raw_data_path)\n",
        "\n",
        "    # --- 4. Execute the Main Automated Pipeline ---\n",
        "    try:\n",
        "        # This function runs all computational tasks (Tasks 1-33), generating\n",
        "        # all models, predictions, and analyses. It is idempotent and will skip\n",
        "        # steps if artifacts already exist.\n",
        "        artifact_paths = execute_quantifying_semantic_shift_study(\n",
        "            raw_df=raw_df,\n",
        "            study_config=study_config,\n",
        "            results_dir=results_dir,\n",
        "            # NOTE: The main pipeline now handles the human-in-the-loop step\n",
        "            # by generating the required file and providing instructions.\n",
        "            # The `run_entailment_ablation` flag is part of the top-level orchestrator.\n",
        "            run_entailment_ablation=False,\n",
        "            force_rerun_main_training=False,\n",
        "            force_rerun_cross_sector=False\n",
        "        )\n",
        "        logging.info(\"\\nMain research pipeline completed successfully.\")\n",
        "        logging.info(\"Generated artifacts are described by the following paths:\")\n",
        "        for name, path in artifact_paths.items():\n",
        "            logging.info(f\"  - {name}: {path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(\"The main research pipeline failed.\", exc_info=True)\n",
        "        return\n",
        "\n",
        "    # --- 5. (Optional) Execute the Human-in-the-Loop Analysis ---\n",
        "    # This section demonstrates how a user would run the final piece of the\n",
        "    # analysis AFTER they have manually annotated the benchmark file.\n",
        "    \n",
        "    # To run this, a user would:\n",
        "    # 1. Find the file 'results/nli_benchmark_for_annotation.csv'.\n",
        "    # 2. Open it and replace all \"ANNOTATE_HERE\" with 1.0, 0.5, or 0.0.\n",
        "    # 3. Save it as 'results/nli_benchmark_annotated.csv'.\n",
        "    # 4. Uncomment and run the code block below.\n",
        "    \n",
        "    # --------------------------------------------------------------------------\n",
        "    # UNCOMMENT THE BLOCK BELOW TO RUN THE ENTAILMENT ABLATION ANALYSIS\n",
        "    # --------------------------------------------------------------------------\n",
        "    # try:\n",
        "    #     logging.info(\"\\n\" + \"=\"*80 + \"\\nATTEMPTING TO RUN ENTAILMENT ABLATION ANALYSIS\\n\" + \"=\"*80)\n",
        "    #     \n",
        "    #     # Define the path to the now-annotated file.\n",
        "    #     annotated_file = results_dir / \"nli_benchmark_annotated.csv\"\n",
        "    #     \n",
        "    #     # Run the dedicated analysis function.\n",
        "    #     entailment_results = run_entailment_ablation_analysis(\n",
        "    #         enriched_predictions_df_path=artifact_paths['enriched_predictions'],\n",
        "    #         annotated_benchmark_path=annotated_file,\n",
        "    #         study_config=study_config\n",
        "    #     )\n",
        "    #     \n",
        "    #     logging.info(\"Entailment ablation analysis completed successfully.\")\n",
        "    #     logging.info(\"Results:\\n\" + entailment_results.to_string())\n",
        "    #     \n",
        "    # except (FileNotFoundError, ValueError) as e:\n",
        "    #     logging.error(f\"Could not run entailment ablation analysis: {e}\")\n",
        "    # --------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This block makes the script executable from the command line.\n",
        "    main()\n",
        "```"
      ],
      "metadata": {
        "id": "kQxibPxk49Vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Configuration Parameter Validation\n",
        "\n",
        "def validate_regime_definitions(\n",
        "    regime_definitions: List[Dict[str, str]]\n",
        ") -> Tuple[bool, List[str]]:\n",
        "    \"\"\"\n",
        "    Validates the temporal consistency of regime definitions.\n",
        "\n",
        "    This function performs a series of rigorous checks on the provided regime\n",
        "    definitions to ensure they are temporally coherent and correctly specified.\n",
        "    It verifies:\n",
        "    1.  Each regime dictionary contains the required 'start_date' and 'end_date' keys.\n",
        "    2.  All date strings are in the valid ISO 8601 format ('YYYY-MM-DD').\n",
        "    3.  Regimes are chronologically ordered and do not overlap. The end date of\n",
        "        one regime must be strictly before the start date of the next.\n",
        "\n",
        "    Args:\n",
        "        regime_definitions: A list of dictionaries, where each dictionary\n",
        "                            defines a regime with 'regime_name', 'start_date',\n",
        "                            and 'end_date'.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A boolean indicating if all validation checks passed (True) or not (False).\n",
        "        - A list of strings describing any validation errors that were found.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect detailed error messages.\n",
        "    errors = []\n",
        "\n",
        "    # --- Input Structure and Type Validation ---\n",
        "    # Ensure the input is a non-empty list.\n",
        "    if not isinstance(regime_definitions, list) or not regime_definitions:\n",
        "        # If the structure is invalid, return immediately.\n",
        "        return False, [\"'regime_definitions' must be a non-empty list.\"]\n",
        "\n",
        "    # --- DataFrame Transformation for Vectorized Validation ---\n",
        "    try:\n",
        "        # Convert the list of dictionaries into a pandas DataFrame for efficient processing.\n",
        "        regimes_df = pd.DataFrame(regime_definitions)\n",
        "\n",
        "        # Check for the presence of required columns.\n",
        "        required_cols = {'regime_name', 'start_date', 'end_date'}\n",
        "        if not required_cols.issubset(regimes_df.columns):\n",
        "            # Identify and report missing columns.\n",
        "            missing = required_cols - set(regimes_df.columns)\n",
        "            errors.append(f\"Missing required keys in regime definitions: {missing}\")\n",
        "            # Return immediately as further checks are not possible.\n",
        "            return False, errors\n",
        "\n",
        "        # --- Step 1: Validate Date Format and Convert to Timestamp ---\n",
        "        # Attempt to convert date strings to timezone-aware (UTC) timestamps.\n",
        "        # Using a strict format and raising errors ensures format compliance.\n",
        "        regimes_df['start_date'] = pd.to_datetime(\n",
        "            regimes_df['start_date'], format='%Y-%m-%d', errors='raise', utc=True\n",
        "        )\n",
        "        regimes_df['end_date'] = pd.to_datetime(\n",
        "            regimes_df['end_date'], format='%Y-%m-%d', errors='raise', utc=True\n",
        "        )\n",
        "\n",
        "    # Catch errors from DataFrame creation or date conversion.\n",
        "    except (ValueError, TypeError, KeyError) as e:\n",
        "        # Report specific parsing or structural errors.\n",
        "        errors.append(f\"Invalid date format or structure in regime definitions: {e}\")\n",
        "        return False, errors\n",
        "\n",
        "    # --- Step 2: Validate Chronological Order within each Regime ---\n",
        "    # Ensure that for every regime, the start_date is before the end_date.\n",
        "    invalid_intervals = regimes_df[regimes_df['start_date'] >= regimes_df['end_date']]\n",
        "    # Check if any such invalid intervals exist.\n",
        "    if not invalid_intervals.empty:\n",
        "        # Report each regime where the start date is not before the end date.\n",
        "        for _, row in invalid_intervals.iterrows():\n",
        "            errors.append(\n",
        "                f\"In regime '{row['regime_name']}', start_date \"\n",
        "                f\"({row['start_date'].date()}) must be before end_date \"\n",
        "                f\"({row['end_date'].date()}).\"\n",
        "            )\n",
        "\n",
        "    # --- Step 3: Validate Chronological Order and Non-Overlap between Regimes ---\n",
        "    # Sort the DataFrame by start_date to ensure regimes are in chronological order.\n",
        "    regimes_df = regimes_df.sort_values(by='start_date').reset_index(drop=True)\n",
        "\n",
        "    # Check for temporal overlaps between consecutive regimes.\n",
        "    # The end date of regime `i` must be strictly less than the start date of regime `i+1`.\n",
        "    # We use `shift(-1)` to get the start date of the next regime for comparison.\n",
        "    next_start_dates = regimes_df['start_date'].shift(-1)\n",
        "\n",
        "    # Identify overlaps where the current end date is on or after the next start date.\n",
        "    # The last regime will have a NaT for `next_start_dates`, so we exclude it.\n",
        "    overlaps = regimes_df[regimes_df['end_date'] >= next_start_dates]\n",
        "\n",
        "    # Check if any overlaps were found.\n",
        "    if not overlaps.empty:\n",
        "        # Report each detected overlap for clear debugging.\n",
        "        for index, row in overlaps.iterrows():\n",
        "            # Get the name of the next regime that is causing the overlap.\n",
        "            next_regime_name = regimes_df.loc[index + 1, 'regime_name']\n",
        "            errors.append(\n",
        "                f\"Overlap detected: Regime '{row['regime_name']}' \"\n",
        "                f\"(ends {row['end_date'].date()}) overlaps with or touches \"\n",
        "                f\"regime '{next_regime_name}' \"\n",
        "                f\"(starts {next_start_dates.loc[index].date()}).\"\n",
        "            )\n",
        "\n",
        "    # --- Final Validation Result ---\n",
        "    # The validation is successful if and only if the errors list is empty.\n",
        "    is_valid = not errors\n",
        "    # Return the overall validity status and the list of detailed errors.\n",
        "    return is_valid, errors\n",
        "\n",
        "\n",
        "def validate_data_split_ratios(\n",
        "    data_split_ratios: Dict[str, float]\n",
        ") -> Tuple[bool, List[str]]:\n",
        "    \"\"\"\n",
        "    Validates the mathematical consistency of data split ratios.\n",
        "\n",
        "    This function ensures that the data split ratios for training, validation,\n",
        "    and testing are mathematically sound. It verifies:\n",
        "    1.  The dictionary contains exactly 'training', 'validation', and 'testing' keys.\n",
        "    2.  All ratio values are numeric (float or int).\n",
        "    3.  Each ratio is strictly greater than 0 and less than 1.\n",
        "    4.  The sum of the ratios is equal to 1.0, within a small tolerance\n",
        "        to account for floating-point arithmetic inaccuracies.\n",
        "\n",
        "    Args:\n",
        "        data_split_ratios: A dictionary containing the split ratios, e.g.,\n",
        "                           {'training': 0.6, 'validation': 0.2, 'testing': 0.2}.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A boolean indicating if all validation checks passed (True) or not (False).\n",
        "        - A list of strings describing any validation errors that were found.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect detailed error messages.\n",
        "    errors = []\n",
        "\n",
        "    # --- Input Structure and Type Validation ---\n",
        "    # Define the expected keys for the dictionary.\n",
        "    expected_keys = {'training', 'validation', 'testing'}\n",
        "    # Check if the provided keys match the expected keys.\n",
        "    if set(data_split_ratios.keys()) != expected_keys:\n",
        "        # Report any discrepancy in keys.\n",
        "        errors.append(f\"data_split_ratios must contain exactly these keys: {expected_keys}\")\n",
        "        # Return immediately as further checks are invalid.\n",
        "        return False, errors\n",
        "\n",
        "    # --- Step 1: Validate Individual Ratio Properties ---\n",
        "    # Iterate through each split type and its corresponding ratio.\n",
        "    for split_name, ratio in data_split_ratios.items():\n",
        "        # Check if the ratio is a valid number (int or float).\n",
        "        if not isinstance(ratio, (int, float)):\n",
        "            # Report a type error if the ratio is not numeric.\n",
        "            errors.append(f\"Ratio for '{split_name}' must be a number, but got {type(ratio)}.\")\n",
        "            # Continue to the next item to check all ratios.\n",
        "            continue\n",
        "\n",
        "        # Check if the ratio is within the valid range (0, 1).\n",
        "        if not (0 < ratio < 1):\n",
        "            # Report an error if the ratio is outside the valid bounds.\n",
        "            errors.append(\n",
        "                f\"Ratio for '{split_name}' must be between 0 and 1 (exclusive), but got {ratio}.\"\n",
        "            )\n",
        "\n",
        "    # If there were errors with individual ratios, return now.\n",
        "    if errors:\n",
        "        return False, errors\n",
        "\n",
        "    # --- Step 2: Validate the Sum of Ratios ---\n",
        "    # Calculate the sum of all ratio values.\n",
        "    total_ratio = sum(data_split_ratios.values())\n",
        "\n",
        "    # Use numpy.isclose for robust floating-point comparison.\n",
        "    # This checks if `total_ratio` is approximately equal to 1.0.\n",
        "    # Equation: |total_ratio - 1.0| <= atol + rtol * |1.0|\n",
        "    if not np.isclose(total_ratio, 1.0, atol=1e-9):\n",
        "        # Report an error if the sum is not 1.0.\n",
        "        errors.append(\n",
        "            f\"The sum of data split ratios must be 1.0, but it is {total_ratio}.\"\n",
        "        )\n",
        "\n",
        "    # --- Final Validation Result ---\n",
        "    # The validation is successful if and only if the errors list is empty.\n",
        "    is_valid = not errors\n",
        "    # Return the overall validity status and the list of detailed errors.\n",
        "    return is_valid, errors\n",
        "\n",
        "\n",
        "def validate_feature_engineering_config(\n",
        "    feature_config: Dict[str, Any],\n",
        "    model_arch_config: Dict[str, Any]\n",
        ") -> Tuple[bool, List[str]]:\n",
        "    \"\"\"\n",
        "    Validates the feature engineering and related model parameters.\n",
        "\n",
        "    This function checks the integrity of the feature engineering configuration.\n",
        "    It verifies:\n",
        "    1.  TF-IDF `max_features` is a positive integer.\n",
        "    2.  The sentence-transformer model identifier is valid and the model can be\n",
        "        loaded from HuggingFace Hub.\n",
        "    3.  The configured `embedding_dimension` matches the actual output dimension\n",
        "        of the loaded sentence-transformer model.\n",
        "    4.  The `input_size` for the `feature_transformer` architecture correctly\n",
        "        matches the sum of TF-IDF features and embedding dimensions.\n",
        "\n",
        "    Args:\n",
        "        feature_config: The 'feature_engineering' section of the study config.\n",
        "        model_arch_config: The 'architectures' section of the model training config.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A boolean indicating if all validation checks passed (True) or not (False).\n",
        "        - A list of strings describing any validation errors that were found.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect detailed error messages.\n",
        "    errors = []\n",
        "\n",
        "    # --- Step 1: Validate TF-IDF Configuration ---\n",
        "    try:\n",
        "        # Retrieve the max_features parameter for TF-IDF.\n",
        "        max_features = feature_config['tfidf']['max_features']\n",
        "        # Check if it's an integer and is positive.\n",
        "        if not isinstance(max_features, int) or max_features <= 0:\n",
        "            # Report an error if the validation fails.\n",
        "            errors.append(f\"'tfidf.max_features' must be a positive integer, but got {max_features}.\")\n",
        "    # Catch errors if the keys are missing in the config dictionary.\n",
        "    except KeyError:\n",
        "        errors.append(\"Missing 'tfidf' or 'max_features' configuration.\")\n",
        "    except TypeError:\n",
        "        errors.append(\"'tfidf' configuration is not correctly structured.\")\n",
        "\n",
        "    # --- Step 2: Validate Sentence Embedding Configuration ---\n",
        "    try:\n",
        "        # Retrieve sentence embedding parameters.\n",
        "        model_id = feature_config['sentence_embeddings']['model_identifier']\n",
        "        config_dim = feature_config['sentence_embeddings']['embedding_dimension']\n",
        "\n",
        "        # --- Sub-step 2a: Validate Model Accessibility ---\n",
        "        try:\n",
        "            # Attempt to load the sentence transformer model. This is the definitive\n",
        "            # test of whether the model identifier is valid and accessible.\n",
        "            logging.info(f\"Attempting to load sentence transformer model: '{model_id}'...\")\n",
        "            model = SentenceTransformer(model_id)\n",
        "            logging.info(\"Model loaded successfully.\")\n",
        "\n",
        "            # --- Sub-step 2b: Validate Embedding Dimension ---\n",
        "            # Get the actual embedding dimension from the loaded model.\n",
        "            actual_dim = model.get_sentence_embedding_dimension()\n",
        "            # Check if the configured dimension matches the actual dimension.\n",
        "            if actual_dim != config_dim:\n",
        "                errors.append(\n",
        "                    f\"Configured 'embedding_dimension' ({config_dim}) does not match \"\n",
        "                    f\"the actual dimension of model '{model_id}' ({actual_dim}).\"\n",
        "                )\n",
        "        # Catch exceptions related to model loading (e.g., not found, network error).\n",
        "        except Exception as e:\n",
        "            errors.append(\n",
        "                f\"Failed to load sentence transformer model '{model_id}'. \"\n",
        "                f\"Check model identifier and network connection. Error: {e}\"\n",
        "            )\n",
        "            # Set actual_dim to None as it couldn't be determined.\n",
        "            actual_dim = None\n",
        "\n",
        "    # Catch errors if the keys are missing in the config dictionary.\n",
        "    except KeyError:\n",
        "        errors.append(\"Missing 'sentence_embeddings' configuration.\")\n",
        "    except TypeError:\n",
        "        errors.append(\"'sentence_embeddings' configuration is not correctly structured.\")\n",
        "\n",
        "    # --- Step 3: Cross-Validate Feature Transformer Input Size ---\n",
        "    try:\n",
        "        # Retrieve the configured input size for the feature-enhanced transformer.\n",
        "        feature_transformer_input_size = model_arch_config['feature_transformer']['input_size']\n",
        "\n",
        "        # This check can only be performed if previous steps were successful.\n",
        "        if 'max_features' in locals() and 'actual_dim' in locals() and actual_dim is not None:\n",
        "            # Calculate the expected input size from the sum of feature dimensions.\n",
        "            expected_input_size = max_features + actual_dim\n",
        "            # Compare the expected size with the configured size.\n",
        "            if feature_transformer_input_size != expected_input_size:\n",
        "                errors.append(\n",
        "                    f\"'feature_transformer.input_size' is configured as \"\n",
        "                    f\"{feature_transformer_input_size}, but the expected sum of \"\n",
        "                    f\"TF-IDF features ({max_features}) and embedding dimension \"\n",
        "                    f\"({actual_dim}) is {expected_input_size}.\"\n",
        "                )\n",
        "    # Catch errors if the keys are missing in the config dictionary.\n",
        "    except KeyError:\n",
        "        errors.append(\"Missing 'feature_transformer' or 'input_size' configuration.\")\n",
        "    except TypeError:\n",
        "        errors.append(\"'feature_transformer' configuration is not correctly structured.\")\n",
        "\n",
        "    # --- Final Validation Result ---\n",
        "    # The validation is successful if and only if the errors list is empty.\n",
        "    is_valid = not errors\n",
        "    # Return the overall validity status and the list of detailed errors.\n",
        "    return is_valid, errors\n",
        "\n",
        "\n",
        "def run_config_validation_suite(\n",
        "    study_config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Executes the full suite of configuration validation checks.\n",
        "\n",
        "    This orchestrator function runs all validation steps for Task 1 and\n",
        "    provides a consolidated report of the outcome. It is the main entry\n",
        "    point for validating the entire study configuration dictionary.\n",
        "\n",
        "    Args:\n",
        "        study_config: The complete study configuration dictionary.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any of the validation checks fail, this exception is\n",
        "                    raised with a detailed report of all errors.\n",
        "    \"\"\"\n",
        "    # Initialize a list to aggregate all errors from the validation suite.\n",
        "    all_errors = []\n",
        "\n",
        "    # --- Execute Task 1, Step 1 ---\n",
        "    logging.info(\"--- Running Task 1, Step 1: Regime Definition Validation ---\")\n",
        "    # Validate the temporal consistency of regime definitions.\n",
        "    regime_valid, regime_errors = validate_regime_definitions(\n",
        "        study_config['experimental_design']['regime_definitions']\n",
        "    )\n",
        "    # Check if the validation passed.\n",
        "    if not regime_valid:\n",
        "        # If not, add the errors to the master list.\n",
        "        all_errors.extend(regime_errors)\n",
        "        logging.error(\"Regime definition validation FAILED.\")\n",
        "    else:\n",
        "        # Log success if validation passed.\n",
        "        logging.info(\"Regime definition validation PASSED.\")\n",
        "\n",
        "    # --- Execute Task 1, Step 2 ---\n",
        "    logging.info(\"\\n--- Running Task 1, Step 2: Data Split Ratio Validation ---\")\n",
        "    # Validate the mathematical consistency of data split ratios.\n",
        "    split_valid, split_errors = validate_data_split_ratios(\n",
        "        study_config['experimental_design']['data_split_ratios']\n",
        "    )\n",
        "    # Check if the validation passed.\n",
        "    if not split_valid:\n",
        "        # If not, add the errors to the master list.\n",
        "        all_errors.extend(split_errors)\n",
        "        logging.error(\"Data split ratio validation FAILED.\")\n",
        "    else:\n",
        "        # Log success if validation passed.\n",
        "        logging.info(\"Data split ratio validation PASSED.\")\n",
        "\n",
        "    # --- Execute Task 1, Step 3 ---\n",
        "    logging.info(\"\\n--- Running Task 1, Step 3: Feature Engineering Config Validation ---\")\n",
        "    # Validate the feature engineering parameters and their cross-dependencies.\n",
        "    feature_valid, feature_errors = validate_feature_engineering_config(\n",
        "        study_config['feature_engineering'],\n",
        "        study_config['model_training']['architectures']\n",
        "    )\n",
        "    # Check if the validation passed.\n",
        "    if not feature_valid:\n",
        "        # If not, add the errors to the master list.\n",
        "        all_errors.extend(feature_errors)\n",
        "        logging.error(\"Feature engineering config validation FAILED.\")\n",
        "    else:\n",
        "        # Log success if validation passed.\n",
        "        logging.info(\"Feature engineering config validation PASSED.\")\n",
        "\n",
        "    # --- Final Report ---\n",
        "    # Check if any errors were collected during the entire suite.\n",
        "    if all_errors:\n",
        "        # If there are errors, format them into a single report.\n",
        "        error_report = \"\\n\".join([f\"- {error}\" for error in all_errors])\n",
        "        # Raise a ValueError with the consolidated report.\n",
        "        raise ValueError(f\"Configuration validation failed with the following errors:\\n{error_report}\")\n",
        "    else:\n",
        "        # If no errors were found, log the overall success.\n",
        "        logging.info(\"\\n>>> All configuration parameters validated successfully. <<<\")\n"
      ],
      "metadata": {
        "id": "dDKP5VHJnVce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: DataFrame Structure and Index Validation\n",
        "\n",
        "def validate_multi_index_integrity(\n",
        "    df: pd.DataFrame\n",
        ") -> Tuple[bool, List[str]]:\n",
        "    \"\"\"\n",
        "    Validates the structural integrity of the DataFrame's MultiIndex.\n",
        "\n",
        "    This function performs a rigorous, production-grade validation of the\n",
        "    DataFrame's index to ensure it conforms to the expected panel data\n",
        "    structure: a 3-level MultiIndex of ('date', 'ticker', 'sector').\n",
        "    It verifies:\n",
        "    1.  The index is a MultiIndex with exactly three levels.\n",
        "    2.  The names of the levels are exactly ['date', 'ticker', 'sector'].\n",
        "    3.  The data types of the levels are correct: DatetimeIndex for 'date',\n",
        "        and string/object for 'ticker' and 'sector'.\n",
        "    4.  The index is unique, ensuring no duplicate (date, ticker, sector) entries.\n",
        "    5.  The index is monotonically increasing, which is critical for correct\n",
        "        time-series operations and preventing lookahead bias.\n",
        "\n",
        "    Args:\n",
        "        df: The pandas DataFrame to be validated.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A boolean, True if the index is valid, False otherwise.\n",
        "        - A list of strings, containing detailed descriptions of any validation failures.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect detailed error messages.\n",
        "    errors = []\n",
        "\n",
        "    # --- Step 1: Validate MultiIndex Presence and Level Count ---\n",
        "    # Check if the index is an instance of pandas MultiIndex.\n",
        "    if not isinstance(df.index, pd.MultiIndex):\n",
        "        # If not, this is a fundamental structural failure.\n",
        "        errors.append(\"DataFrame index is not a MultiIndex.\")\n",
        "        # Return immediately as subsequent checks are invalid.\n",
        "        return False, errors\n",
        "\n",
        "    # Check if the number of levels in the MultiIndex is exactly 3.\n",
        "    if df.index.nlevels != 3:\n",
        "        # Report the actual number of levels found.\n",
        "        errors.append(\n",
        "            f\"Expected 3 index levels, but found {df.index.nlevels}.\"\n",
        "        )\n",
        "        # Return immediately.\n",
        "        return False, errors\n",
        "\n",
        "    # --- Step 2: Validate Index Level Names ---\n",
        "    # Define the exact, ordered list of expected index level names.\n",
        "    expected_names = ['date', 'ticker', 'sector']\n",
        "    # Compare the actual names with the expected names.\n",
        "    if list(df.index.names) != expected_names:\n",
        "        # Report the discrepancy.\n",
        "        errors.append(\n",
        "            f\"Expected index names {expected_names}, but found {list(df.index.names)}.\"\n",
        "        )\n",
        "\n",
        "    # --- Step 3: Validate Index Level Data Types ---\n",
        "    # Check if the first level ('date') is a DatetimeIndex.\n",
        "    if not isinstance(df.index.levels[0], pd.DatetimeIndex):\n",
        "        errors.append(\n",
        "            f\"Expected index level 0 ('date') to be a DatetimeIndex, \"\n",
        "            f\"but found {type(df.index.levels[0])}.\"\n",
        "        )\n",
        "\n",
        "    # Check if the second level ('ticker') has a string/object dtype.\n",
        "    if not pd.api.types.is_string_dtype(df.index.levels[1]):\n",
        "        errors.append(\n",
        "            f\"Expected index level 1 ('ticker') to have a string dtype, \"\n",
        "            f\"but found {df.index.levels[1].dtype}.\"\n",
        "        )\n",
        "\n",
        "    # Check if the third level ('sector') has a string/object dtype.\n",
        "    if not pd.api.types.is_string_dtype(df.index.levels[2]):\n",
        "        errors.append(\n",
        "            f\"Expected index level 2 ('sector') to have a string dtype, \"\n",
        "            f\"but found {df.index.levels[2].dtype}.\"\n",
        "        )\n",
        "\n",
        "    # --- Step 4: Validate Index Uniqueness ---\n",
        "    # The `is_unique` property provides a highly optimized check for duplicates.\n",
        "    if not df.index.is_unique:\n",
        "        # Report that duplicate index entries exist.\n",
        "        errors.append(\"Duplicate entries found in the MultiIndex. Index must be unique.\")\n",
        "\n",
        "    # --- Step 5: Validate Index Sorting ---\n",
        "    # The `is_monotonic_increasing` property checks if the index is sorted.\n",
        "    # This is crucial for time-series integrity.\n",
        "    if not df.index.is_monotonic_increasing:\n",
        "        # Report the sorting failure and suggest a remedy.\n",
        "        errors.append(\n",
        "            \"Index is not monotonically increasing. Please sort the DataFrame \"\n",
        "            \"using `df.sort_index(inplace=True)` before proceeding.\"\n",
        "        )\n",
        "\n",
        "    # --- Final Validation Result ---\n",
        "    # The validation is successful if and only if the errors list is empty.\n",
        "    is_valid = not errors\n",
        "    # Return the overall validity status and the list of detailed errors.\n",
        "    return is_valid, errors\n",
        "\n",
        "\n",
        "def validate_column_schema(\n",
        "    df: pd.DataFrame\n",
        ") -> Tuple[bool, List[str]]:\n",
        "    \"\"\"\n",
        "    Validates the DataFrame's column schema, data types, and basic data integrity.\n",
        "\n",
        "    This function ensures the DataFrame has the exact required columns and that\n",
        "    each column conforms to its specified data type. It also performs a crucial\n",
        "    integrity check for financial data: ensuring all price and volume figures\n",
        "    are positive, as negative values are nonsensical and indicate data corruption.\n",
        "\n",
        "    It verifies:\n",
        "    1.  The set of column names exactly matches the expected schema.\n",
        "    2.  Each column has the correct, precise data type (e.g., float64, int64).\n",
        "    3.  All values in numeric price and volume columns are strictly positive.\n",
        "\n",
        "    Args:\n",
        "        df: The pandas DataFrame to be validated.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A boolean, True if the schema is valid, False otherwise.\n",
        "        - A list of strings, containing detailed descriptions of any validation failures.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect detailed error messages.\n",
        "    errors = []\n",
        "\n",
        "    # --- Step 1: Validate Column Presence and Names ---\n",
        "    # Define the exact set of expected column names. Using a set is robust to order.\n",
        "    expected_columns: Set[str] = {\n",
        "        'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume',\n",
        "        'aggregated_text', 'target_return'\n",
        "    }\n",
        "    # Compare the set of actual columns to the expected set.\n",
        "    if set(df.columns) != expected_columns:\n",
        "        # Report missing and/or unexpected columns for precise debugging.\n",
        "        missing = expected_columns - set(df.columns)\n",
        "        extra = set(df.columns) - expected_columns\n",
        "        if missing:\n",
        "            errors.append(f\"Missing required columns: {sorted(list(missing))}\")\n",
        "        if extra:\n",
        "            errors.append(f\"Found unexpected columns: {sorted(list(extra))}\")\n",
        "        # Return immediately if the column set is wrong.\n",
        "        return False, errors\n",
        "\n",
        "    # --- Step 2: Validate Column Data Types ---\n",
        "    # Define the precise expected data type for each column.\n",
        "    expected_dtypes: Dict[str, np.dtype] = {\n",
        "        'Open': np.dtype('float64'),\n",
        "        'High': np.dtype('float64'),\n",
        "        'Low': np.dtype('float64'),\n",
        "        'Close': np.dtype('float64'),\n",
        "        'Adj Close': np.dtype('float64'),\n",
        "        'Volume': np.dtype('int64'),\n",
        "        'aggregated_text': np.dtype('object'),\n",
        "        'target_return': np.dtype('float64')\n",
        "    }\n",
        "    # Iterate through the expected dtypes and compare with the actual dtypes.\n",
        "    for col, expected_dtype in expected_dtypes.items():\n",
        "        # Check for exact dtype match.\n",
        "        if df[col].dtype != expected_dtype:\n",
        "            errors.append(\n",
        "                f\"Column '{col}' has incorrect dtype. Expected {expected_dtype}, \"\n",
        "                f\"but found {df[col].dtype}.\"\n",
        "            )\n",
        "\n",
        "    # --- Step 3: Validate Data Integrity (Positivity of Numeric Columns) ---\n",
        "    # Define the columns that must contain only positive values.\n",
        "    positive_cols = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
        "    # Perform a highly efficient, vectorized check for non-positive values.\n",
        "    # This check ignores NaNs, which are handled separately.\n",
        "    non_positive_mask = (df[positive_cols] <= 0)\n",
        "    # Check if any non-positive values were found in any of these columns.\n",
        "    if non_positive_mask.any().any():\n",
        "        # Identify which columns contain non-positive values.\n",
        "        cols_with_issues = non_positive_mask.any()[non_positive_mask.any()].index.tolist()\n",
        "        errors.append(\n",
        "            f\"Non-positive values found in the following columns which should \"\n",
        "            f\"be strictly positive: {cols_with_issues}.\"\n",
        "        )\n",
        "\n",
        "    # --- Final Validation Result ---\n",
        "    # The validation is successful if and only if the errors list is empty.\n",
        "    is_valid = not errors\n",
        "    # Return the overall validity status and the list of detailed errors.\n",
        "    return is_valid, errors\n",
        "\n",
        "\n",
        "def validate_target_return_construction(\n",
        "    df: pd.DataFrame\n",
        ") -> Tuple[bool, List[str]]:\n",
        "    \"\"\"\n",
        "    Validates the temporal alignment and construction of the 'target_return'.\n",
        "\n",
        "    This function provides a critical check against lookahead bias by verifying\n",
        "    that the `target_return` for day `t` is correctly calculated using the\n",
        "    `Adj Close` prices of day `t` and day `t+1`. It performs this check on a\n",
        "    per-ticker basis to respect the time-series nature of the data.\n",
        "\n",
        "    It verifies:\n",
        "    1.  The `target_return` value at each row mathematically corresponds to the\n",
        "        percentage change in `Adj Close` from the current day to the next.\n",
        "    2.  The last observation for each ticker has a `NaN` `target_return`, as\n",
        "        its future return cannot be known.\n",
        "\n",
        "    Args:\n",
        "        df: The pandas DataFrame to be validated. Must have a sorted MultiIndex.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A boolean, True if the target construction is valid, False otherwise.\n",
        "        - A list of strings, containing tickers for which validation failed.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect tickers with validation failures.\n",
        "    failed_tickers = []\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the DataFrame is not empty and has the required columns.\n",
        "    if df.empty or 'Adj Close' not in df.columns or 'target_return' not in df.columns:\n",
        "        return False, [\"Input DataFrame is empty or missing required columns.\"]\n",
        "\n",
        "    # --- Step 1: Re-calculate the Target Return in a Vectorized, Grouped Manner ---\n",
        "    # Group by the 'ticker' level of the index to perform calculations per security.\n",
        "    # This is essential to prevent data leakage across securities.\n",
        "    # Equation: target_return[t] = (Adj_Close[t+1] - Adj_Close[t]) / Adj_Close[t] * 100\n",
        "    # `pct_change(1)` calculates return from t-1 to t.\n",
        "    # `shift(-1)` moves this value to row t-1, aligning it as the return from t to t+1.\n",
        "    recalculated_return = df.groupby(level='ticker')['Adj Close'] \\\n",
        "                            .pct_change(1).shift(-1) * 100\n",
        "\n",
        "    # --- Step 2: Compare Recalculated Return with Existing Target ---\n",
        "    # Use `numpy.isclose` for a robust floating-point comparison.\n",
        "    # `equal_nan=True` is critical, as we expect NaNs to match at the end of each series.\n",
        "    is_correct = np.isclose(\n",
        "        df['target_return'],\n",
        "        recalculated_return,\n",
        "        atol=1e-9,\n",
        "        equal_nan=True\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Identify Tickers with Mismatches ---\n",
        "    # If not all values are correct, identify the specific tickers that failed.\n",
        "    if not is_correct.all():\n",
        "        # Find the rows where the comparison failed.\n",
        "        mismatch_df = df[~is_correct]\n",
        "        # Get the unique tickers from these mismatched rows.\n",
        "        failed_tickers = mismatch_df.index.get_level_values('ticker').unique().tolist()\n",
        "\n",
        "    # --- Final Validation Result ---\n",
        "    # The validation is successful if and only if no tickers failed.\n",
        "    is_valid = not failed_tickers\n",
        "    # Return the overall validity status and the list of failed tickers.\n",
        "    return is_valid, failed_tickers\n",
        "\n",
        "\n",
        "def run_dataframe_validation_suite(\n",
        "    df: pd.DataFrame\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Executes the full suite of DataFrame validation checks for Task 2.\n",
        "\n",
        "    This orchestrator function runs all validation steps for the input DataFrame\n",
        "    and provides a consolidated report. It is the main entry point for\n",
        "    validating the data structure before any processing.\n",
        "\n",
        "    Args:\n",
        "        df: The complete input pandas DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any of the validation checks fail, this exception is\n",
        "                    raised with a detailed report of all errors.\n",
        "    \"\"\"\n",
        "    # Initialize a list to aggregate all errors from the validation suite.\n",
        "    all_errors = []\n",
        "\n",
        "    # --- Execute Task 2, Step 1 ---\n",
        "    logging.info(\"--- Running Task 2, Step 1: Multi-Index Integrity Validation ---\")\n",
        "    index_valid, index_errors = validate_multi_index_integrity(df)\n",
        "    if not index_valid:\n",
        "        all_errors.extend(index_errors)\n",
        "        logging.error(\"Multi-Index integrity validation FAILED.\")\n",
        "    else:\n",
        "        logging.info(\"Multi-Index integrity validation PASSED.\")\n",
        "\n",
        "    # --- Execute Task 2, Step 2 ---\n",
        "    logging.info(\"\\n--- Running Task 2, Step 2: Column Schema Validation ---\")\n",
        "    schema_valid, schema_errors = validate_column_schema(df)\n",
        "    if not schema_valid:\n",
        "        all_errors.extend(schema_errors)\n",
        "        logging.error(\"Column schema validation FAILED.\")\n",
        "    else:\n",
        "        logging.info(\"Column schema validation PASSED.\")\n",
        "\n",
        "    # --- Execute Task 2, Step 3 ---\n",
        "    # This check is only meaningful if the index is sorted correctly.\n",
        "    if index_valid and 'Index is not monotonically increasing' not in str(index_errors):\n",
        "        logging.info(\"\\n--- Running Task 2, Step 3: Target Return Construction Validation ---\")\n",
        "        target_valid, target_errors = validate_target_return_construction(df)\n",
        "        if not target_valid:\n",
        "            # Format the error message for failed tickers.\n",
        "            error_msg = f\"Target return construction is incorrect for tickers: {target_errors}\"\n",
        "            all_errors.append(error_msg)\n",
        "            logging.error(\"Target return construction validation FAILED.\")\n",
        "        else:\n",
        "            logging.info(\"Target return construction validation PASSED.\")\n",
        "    else:\n",
        "        # Skip this check if the index is not sorted, as it would be invalid.\n",
        "        logging.warning(\n",
        "            \"\\n--- SKIPPING Task 2, Step 3: Target Return Construction Validation --- \"\n",
        "            \"Reason: Index is not valid or not sorted.\"\n",
        "        )\n",
        "\n",
        "    # --- Final Report ---\n",
        "    # Check if any errors were collected during the entire suite.\n",
        "    if all_errors:\n",
        "        # Format them into a single, readable report.\n",
        "        error_report = \"\\n\".join([f\"- {error}\" for error in all_errors])\n",
        "        # Raise a ValueError with the consolidated report.\n",
        "        raise ValueError(f\"DataFrame validation failed with the following errors:\\n{error_report}\")\n",
        "    else:\n",
        "        # If no errors were found, log the overall success.\n",
        "        logging.info(\"\\n>>> All DataFrame structure and integrity checks passed successfully. <<<\")\n"
      ],
      "metadata": {
        "id": "3eyw1SrYsQH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Data Quality Assessment and Cleansing\n",
        "\n",
        "def assess_missing_data(\n",
        "    df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates a detailed report on missing and empty data patterns.\n",
        "\n",
        "    This function provides a diagnostic overview of data completeness. Crucially,\n",
        "    it distinguishes between truly missing values (NaN) and intentionally empty\n",
        "    strings (''), which is vital for the 'aggregated_text' column where an\n",
        "    empty string signifies a day with no news, a meaningful data point.\n",
        "\n",
        "    Args:\n",
        "        df: The input pandas DataFrame to assess.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame summarizing the missing data statistics for each column,\n",
        "        including counts and percentages of NaN values and empty strings.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Step 1: Calculate NaN Statistics ---\n",
        "    # Calculate the number of NaN values in each column.\n",
        "    nan_counts = df.isnull().sum()\n",
        "    # Calculate the percentage of NaN values.\n",
        "    nan_percentages = (nan_counts / len(df)) * 100\n",
        "\n",
        "    # --- Step 2: Calculate Empty String Statistics ---\n",
        "    # Initialize a series for empty string counts with zeros.\n",
        "    empty_string_counts = pd.Series(0, index=df.columns, dtype=int)\n",
        "    # Iterate through columns with object dtype to check for empty strings.\n",
        "    for col in df.select_dtypes(include=['object']).columns:\n",
        "        # This vectorized operation is highly efficient.\n",
        "        empty_string_counts[col] = (df[col] == '').sum()\n",
        "    # Calculate the percentage of empty strings.\n",
        "    empty_string_percentages = (empty_string_counts / len(df)) * 100\n",
        "\n",
        "    # --- Step 3: Assemble the Report ---\n",
        "    # Create a DataFrame from the calculated statistics.\n",
        "    report = pd.DataFrame({\n",
        "        'nan_count': nan_counts,\n",
        "        'nan_percentage': nan_percentages,\n",
        "        'empty_string_count': empty_string_counts,\n",
        "        'empty_string_percentage': empty_string_percentages\n",
        "    })\n",
        "    # Return the comprehensive report.\n",
        "    return report\n",
        "\n",
        "\n",
        "def validate_financial_data_consistency(\n",
        "    df: pd.DataFrame,\n",
        "    return_threshold: float = 50.0\n",
        ") -> Dict[str, pd.Series]:\n",
        "    \"\"\"\n",
        "    Performs consistency checks specific to financial market data.\n",
        "\n",
        "    This function validates the integrity of financial data by checking for\n",
        "    logical impossibilities, such as incorrect price relationships or extreme,\n",
        "    likely erroneous, daily returns.\n",
        "\n",
        "    It validates:\n",
        "    1.  OHLC consistency: Low <= {Open, Close} <= High for each row.\n",
        "    2.  Unrealistic returns: Flags any daily adjusted close returns exceeding a\n",
        "        specified percentage threshold (e.g., +/- 50%).\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame with a valid MultiIndex.\n",
        "        return_threshold: The percentage threshold to define an unrealistic\n",
        "                          daily return. Defaults to 50.0.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are validation check names and values are\n",
        "        boolean pandas Series. A `True` value in a series indicates the\n",
        "        corresponding row passed that specific check.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "    required_cols = {'Open', 'High', 'Low', 'Close', 'Adj Close'}\n",
        "    if not required_cols.issubset(df.columns):\n",
        "        raise ValueError(f\"DataFrame is missing required financial columns: {required_cols - set(df.columns)}\")\n",
        "\n",
        "    # --- Step 1: Validate OHLC (Open, High, Low, Close) Relationships ---\n",
        "    # This vectorized boolean mask efficiently checks all rows at once.\n",
        "    # A row is valid if all conditions are met.\n",
        "    ohlc_valid_mask = (df['Low'] <= df['Open']) & \\\n",
        "                      (df['Low'] <= df['Close']) & \\\n",
        "                      (df['High'] >= df['Open']) & \\\n",
        "                      (df['High'] >= df['Close'])\n",
        "\n",
        "    # --- Step 2: Validate for Unrealistic Returns ---\n",
        "    # Calculate daily returns per ticker to avoid data leakage.\n",
        "    # This is the only methodologically sound way to compute returns in panel data.\n",
        "    daily_returns = df.groupby(level='ticker')['Adj Close'].pct_change() * 100\n",
        "    # Create a mask where `True` indicates a realistic return.\n",
        "    # The check `daily_returns.abs() <= return_threshold` identifies valid returns.\n",
        "    # We must fill NaNs (first day of each ticker) with True as they are not \"unrealistic\".\n",
        "    realistic_return_mask = (daily_returns.abs() <= return_threshold).fillna(True)\n",
        "\n",
        "    # --- Step 3: Assemble and Return Results ---\n",
        "    # Return a dictionary of boolean masks for detailed, row-level reporting.\n",
        "    return {\n",
        "        'ohlc_valid': ohlc_valid_mask,\n",
        "        'realistic_return': realistic_return_mask\n",
        "    }\n",
        "\n",
        "\n",
        "def clean_and_standardize_text_data(\n",
        "    df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cleans and standardizes the 'aggregated_text' column.\n",
        "\n",
        "    This function performs essential text cleansing operations to prepare the\n",
        "    data for NLP processing. It operates on a copy of the DataFrame to ensure\n",
        "    the original data remains untouched (functional purity).\n",
        "\n",
        "    The cleansing pipeline is:\n",
        "    1.  Replace any `NaN` values in 'aggregated_text' with empty strings ('').\n",
        "    2.  Ensure consistent UTF-8 encoding to prevent downstream errors.\n",
        "    3.  Remove any null bytes ('\\\\x00'), which can corrupt text processing pipelines.\n",
        "\n",
        "    Args:\n",
        "        df: The input pandas DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        A new pandas DataFrame with the 'aggregated_text' column cleaned.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "    if 'aggregated_text' not in df.columns:\n",
        "        raise ValueError(\"DataFrame is missing the 'aggregated_text' column.\")\n",
        "\n",
        "    # --- Operate on a copy to avoid side effects ---\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # --- Step 1: Handle NaN values ---\n",
        "    # Replace any NaN values specifically in this column with an empty string.\n",
        "    # This standardizes the representation of \"no text available\".\n",
        "    df_clean['aggregated_text'] = df_clean['aggregated_text'].fillna('')\n",
        "\n",
        "    # --- Step 2: Ensure Consistent UTF-8 Encoding ---\n",
        "    # This two-step process robustly handles potential encoding issues.\n",
        "    # It encodes to bytes (ignoring errors) and decodes back to a clean UTF-8 string.\n",
        "    df_clean['aggregated_text'] = df_clean['aggregated_text'].str.encode(\n",
        "        'utf-8', 'ignore'\n",
        "    ).str.decode('utf-8')\n",
        "\n",
        "    # --- Step 3: Remove Null Bytes ---\n",
        "    # Null bytes (\\x00) are problematic for many tools. This removes them.\n",
        "    # `regex=False` ensures this is a fast, literal string replacement.\n",
        "    df_clean['aggregated_text'] = df_clean['aggregated_text'].str.replace(\n",
        "        '\\x00', '', regex=False\n",
        "    )\n",
        "\n",
        "    # Return the cleaned DataFrame.\n",
        "    return df_clean\n",
        "\n",
        "\n",
        "def run_data_quality_and_cleansing_suite(\n",
        "    df: pd.DataFrame,\n",
        "    return_threshold: float = 50.0\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the full data quality assessment and cleansing pipeline.\n",
        "\n",
        "    This function executes the complete suite of checks and cleansing operations\n",
        "    from Task 3. It first generates diagnostic reports and then applies the\n",
        "    necessary cleaning steps, returning a new, validated, and cleaned DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df: The raw input pandas DataFrame.\n",
        "        return_threshold: The percentage threshold for flagging unrealistic returns.\n",
        "\n",
        "    Returns:\n",
        "        A new, cleaned, and validated pandas DataFrame ready for the next phase.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If critical data consistency checks fail.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "\n",
        "    logging.info(\"--- Running Task 3: Data Quality Assessment and Cleansing Suite ---\")\n",
        "\n",
        "    # --- Step 1: Assess Missing Data ---\n",
        "    logging.info(\"\\nStep 1: Assessing missing data patterns...\")\n",
        "    # Generate the missing data report.\n",
        "    missing_data_report = assess_missing_data(df)\n",
        "    logging.info(\"Missing Data Report:\\n\" + missing_data_report.to_string())\n",
        "    # The key takeaway is understanding the data; cleansing happens in Step 3.\n",
        "\n",
        "    # --- Step 2: Validate Financial Data Consistency ---\n",
        "    logging.info(\"\\nStep 2: Validating financial data consistency...\")\n",
        "    # Get the boolean masks for financial data validation.\n",
        "    validation_results = validate_financial_data_consistency(df, return_threshold)\n",
        "\n",
        "    # Aggregate any failures from the consistency checks.\n",
        "    all_errors = []\n",
        "    # Check for OHLC violations.\n",
        "    if not validation_results['ohlc_valid'].all():\n",
        "        # Count the number of rows with invalid OHLC data.\n",
        "        num_invalid_ohlc = (~validation_results['ohlc_valid']).sum()\n",
        "        all_errors.append(f\"{num_invalid_ohlc} rows failed OHLC consistency check.\")\n",
        "\n",
        "    # Check for unrealistic returns.\n",
        "    if not validation_results['realistic_return'].all():\n",
        "        # Count the number of rows with unrealistic returns.\n",
        "        num_unrealistic_returns = (~validation_results['realistic_return']).sum()\n",
        "        # This is treated as a warning, not a fatal error, but is reported.\n",
        "        logging.warning(\n",
        "            f\"{num_unrealistic_returns} rows have unrealistic daily returns \"\n",
        "            f\"(> +/-{return_threshold}%).\"\n",
        "        )\n",
        "\n",
        "    # If there are critical errors, raise an exception.\n",
        "    if all_errors:\n",
        "        error_report = \"\\n\".join([f\"- {error}\" for error in all_errors])\n",
        "        raise ValueError(f\"Critical data consistency validation failed:\\n{error_report}\")\n",
        "    else:\n",
        "        logging.info(\"Financial data consistency validation PASSED.\")\n",
        "\n",
        "    # --- Step 3: Clean and Standardize Text Data ---\n",
        "    logging.info(\"\\nStep 3: Cleaning and standardizing text data...\")\n",
        "    # Apply the text cleaning pipeline. This returns a new, cleaned DataFrame.\n",
        "    df_cleaned = clean_and_standardize_text_data(df)\n",
        "    logging.info(\"Text data has been standardized (NaNs -> '', UTF-8, null bytes removed).\")\n",
        "\n",
        "    # --- Final Output ---\n",
        "    logging.info(\"\\n>>> Data quality assessment and cleansing suite completed successfully. <<<\")\n",
        "    # Return the final, cleaned DataFrame.\n",
        "    return df_cleaned\n"
      ],
      "metadata": {
        "id": "vWZoCrNstjjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Temporal Regime Assignment\n",
        "\n",
        "def _create_regime_classifier(\n",
        "    regime_definitions: List[Dict[str, str]]\n",
        ") -> callable:\n",
        "    \"\"\"\n",
        "    Internal helper to create a fast, memoized regime classification function.\n",
        "\n",
        "    This helper pre-processes the regime definitions from the config into a\n",
        "    more efficient structure for lookup and returns a closure that can be\n",
        "    applied to a DatetimeIndex.\n",
        "\n",
        "    Args:\n",
        "        regime_definitions: The list of regime definition dictionaries from the\n",
        "                            study configuration.\n",
        "\n",
        "    Returns:\n",
        "        A callable function that takes a pandas Timestamp and returns the\n",
        "        corresponding regime name as a string, or None if it's outside all regimes.\n",
        "    \"\"\"\n",
        "    # --- Pre-processing for Efficiency ---\n",
        "    # Convert the list of dicts to a DataFrame and parse dates to timezone-aware UTC.\n",
        "    # This is done once, making the returned classifier function highly efficient.\n",
        "    processed_regimes = pd.DataFrame(regime_definitions)\n",
        "    processed_regimes['start_date'] = pd.to_datetime(\n",
        "        processed_regimes['start_date'], utc=True\n",
        "    )\n",
        "    processed_regimes['end_date'] = pd.to_datetime(\n",
        "        processed_regimes['end_date'], utc=True\n",
        "    )\n",
        "\n",
        "    # Convert to a list of tuples for fast iteration.\n",
        "    regime_tuples = list(processed_regimes.itertuples(index=False, name=None))\n",
        "\n",
        "    def classify_date(ts: pd.Timestamp) -> Optional[str]:\n",
        "        \"\"\"Classifies a single timestamp into a regime.\"\"\"\n",
        "        # --- Timezone Canonicalization ---\n",
        "        # Ensure the input timestamp is timezone-aware (UTC) for correct comparison.\n",
        "        if ts.tzinfo is None:\n",
        "            # If timezone-naive, localize to UTC.\n",
        "            ts_utc = ts.tz_localize('utc')\n",
        "        else:\n",
        "            # If already timezone-aware, convert to UTC.\n",
        "            ts_utc = ts.tz_convert('utc')\n",
        "\n",
        "        # --- Linear Scan for Classification ---\n",
        "        # For a small number of regimes (like 4), a linear scan is optimal.\n",
        "        # The boundary condition is inclusive: start_date <= ts <= end_date.\n",
        "        for name, start, end in regime_tuples:\n",
        "            if start <= ts_utc <= end:\n",
        "                # Return the name of the first matching regime.\n",
        "                return name\n",
        "\n",
        "        # If no regime matches, return None.\n",
        "        return None\n",
        "\n",
        "    # Return the configured classification function.\n",
        "    return classify_date\n",
        "\n",
        "\n",
        "def assign_regime_labels(\n",
        "    df: pd.DataFrame,\n",
        "    regime_definitions: List[Dict[str, str]]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Assigns a 'regime' label to each row and filters out non-regime data.\n",
        "\n",
        "    This function applies a classification function to the DataFrame's date\n",
        "    index to label each row with its corresponding macroeconomic regime. It then\n",
        "    removes any rows that do not fall within one of the defined regimes.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame with a DatetimeIndex at level 0.\n",
        "        regime_definitions: The list of regime definition dictionaries.\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame with an added 'regime' column, containing only the\n",
        "        data within the specified regimes. The 'regime' column is set to a\n",
        "        memory-efficient categorical dtype.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "    if not isinstance(df.index, pd.MultiIndex) or not isinstance(df.index.levels[0], pd.DatetimeIndex):\n",
        "        raise ValueError(\"DataFrame must have a MultiIndex with a DatetimeIndex at level 0.\")\n",
        "\n",
        "    # --- Operate on a copy to avoid side effects ---\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # --- Step 1: Create the Classification Function ---\n",
        "    # This helper pre-processes the definitions for efficient application.\n",
        "    regime_classifier = _create_regime_classifier(regime_definitions)\n",
        "\n",
        "    # --- Step 2: Apply Regime Labels via Optimized Mapping ---\n",
        "    # Use the highly optimized `.map()` method on the date index level.\n",
        "    # This is significantly faster than row-wise `.apply()`.\n",
        "    date_index = df_processed.index.get_level_values('date')\n",
        "    regime_labels = date_index.map(regime_classifier)\n",
        "\n",
        "    # Assign the generated labels to a new 'regime' column.\n",
        "    df_processed['regime'] = regime_labels\n",
        "\n",
        "    # --- Step 3: Filter Out Data Outside Regimes ---\n",
        "    # Keep only the rows where a regime label was successfully assigned.\n",
        "    rows_before = len(df_processed)\n",
        "    df_processed = df_processed[df_processed['regime'].notna()]\n",
        "    rows_after = len(df_processed)\n",
        "    logging.info(\n",
        "        f\"Filtered out {rows_before - rows_after} rows that are outside \"\n",
        "        \"any defined regime.\"\n",
        "    )\n",
        "\n",
        "    # --- Step 4: Optimize Memory Usage ---\n",
        "    # Convert the 'regime' column to a categorical dtype.\n",
        "    # This is highly memory-efficient and can speed up subsequent group-by operations.\n",
        "    df_processed['regime'] = df_processed['regime'].astype('category')\n",
        "\n",
        "    # Return the processed DataFrame.\n",
        "    return df_processed\n",
        "\n",
        "\n",
        "def validate_regime_assignment(\n",
        "    df_regimes: pd.DataFrame,\n",
        "    regime_definitions: List[Dict[str, str]]\n",
        ") -> Tuple[bool, List[str]]:\n",
        "    \"\"\"\n",
        "    Validates the outcome of the regime assignment process.\n",
        "\n",
        "    This function performs a post-assignment audit to ensure that the data\n",
        "    has been correctly partitioned. It verifies:\n",
        "    1.  All defined regimes are present in the resulting DataFrame.\n",
        "    2.  The observed date ranges for each regime in the data are consistent\n",
        "        with the configuration.\n",
        "    3.  Each regime contains a sufficient number of samples for analysis.\n",
        "\n",
        "    Args:\n",
        "        df_regimes: The DataFrame after regime labels have been assigned.\n",
        "        regime_definitions: The original list of regime definition dictionaries.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A boolean, True if the assignment is valid, False otherwise.\n",
        "        - A list of strings, containing detailed descriptions of any validation failures.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect detailed error messages.\n",
        "    errors = []\n",
        "\n",
        "    # --- Pre-process config for easy lookup ---\n",
        "    config_df = pd.DataFrame(regime_definitions)\n",
        "    config_df['start_date'] = pd.to_datetime(config_df['start_date'], utc=True)\n",
        "    config_df['end_date'] = pd.to_datetime(config_df['end_date'], utc=True)\n",
        "    config_df = config_df.set_index('regime_name')\n",
        "\n",
        "    # --- Step 1: Validate Presence of All Regimes ---\n",
        "    # Get the set of regime names from the config and the data.\n",
        "    expected_regimes = set(config_df.index)\n",
        "    found_regimes = set(df_regimes['regime'].unique())\n",
        "\n",
        "    # Check if any expected regimes are missing from the data.\n",
        "    if expected_regimes != found_regimes:\n",
        "        missing = expected_regimes - found_regimes\n",
        "        errors.append(f\"Regimes defined in config but not found in data: {missing}\")\n",
        "        # Return early if regimes are missing, as further checks are invalid.\n",
        "        return False, errors\n",
        "\n",
        "    # --- Step 2: Validate Temporal Boundaries and Sample Counts per Regime ---\n",
        "    # Group by the newly assigned 'regime' column.\n",
        "    # Use `.agg()` for an efficient, single-pass calculation of all required stats.\n",
        "    regime_stats = df_regimes.groupby('regime').agg(\n",
        "        observed_start=('date', 'min'),\n",
        "        observed_end=('date', 'max'),\n",
        "        sample_count=('date', 'size')\n",
        "    )\n",
        "\n",
        "    # Iterate through the calculated stats for each regime.\n",
        "    for regime_name, stats in regime_stats.iterrows():\n",
        "        # Retrieve the configured boundaries for the current regime.\n",
        "        config_start = config_df.loc[regime_name, 'start_date']\n",
        "        config_end = config_df.loc[regime_name, 'end_date']\n",
        "\n",
        "        # Check if the observed date range is within the configured boundaries.\n",
        "        if not (stats['observed_start'] >= config_start and stats['observed_end'] <= config_end):\n",
        "            errors.append(\n",
        "                f\"Regime '{regime_name}' has data outside its defined boundaries. \"\n",
        "                f\"Expected: [{config_start.date()}-{config_end.date()}], \"\n",
        "                f\"Found: [{stats['observed_start'].date()}-{stats['observed_end'].date()}].\"\n",
        "            )\n",
        "\n",
        "        # Check for sufficient sample size (e.g., minimum of 100 observations).\n",
        "        min_samples = 100\n",
        "        if stats['sample_count'] < min_samples:\n",
        "            # This is a warning rather than a fatal error, but important to note.\n",
        "            logging.warning(\n",
        "                f\"Regime '{regime_name}' has a low sample count \"\n",
        "                f\"({stats['sample_count']}), which may affect model robustness.\"\n",
        "            )\n",
        "\n",
        "    # --- Final Validation Result ---\n",
        "    is_valid = not errors\n",
        "    return is_valid, errors\n",
        "\n",
        "\n",
        "def run_regime_assignment_suite(\n",
        "    df: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the full temporal regime assignment and validation pipeline.\n",
        "\n",
        "    This function executes the complete workflow for Task 4. It takes the raw\n",
        "    DataFrame and the study configuration, assigns regime labels, filters the\n",
        "    data, validates the result, and returns the final, regime-partitioned DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df: The raw input pandas DataFrame.\n",
        "        study_config: The complete study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame containing only data from the defined regimes, with a\n",
        "        validated 'regime' column.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the post-assignment validation fails.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Running Task 4: Temporal Regime Assignment Suite ---\")\n",
        "\n",
        "    # Retrieve regime definitions from the configuration.\n",
        "    regime_definitions = study_config['experimental_design']['regime_definitions']\n",
        "\n",
        "    # --- Step 1 & 2: Assign Regime Labels and Filter Data ---\n",
        "    logging.info(\"\\nStep 1 & 2: Assigning regime labels and filtering data...\")\n",
        "    df_with_regimes = assign_regime_labels(df, regime_definitions)\n",
        "    logging.info(\"Regime assignment and filtering complete.\")\n",
        "    logging.info(f\"DataFrame shape after regime assignment: {df_with_regimes.shape}\")\n",
        "    logging.info(\"Sample counts per regime:\\n\" + str(df_with_regimes['regime'].value_counts()))\n",
        "\n",
        "    # --- Step 3: Validate the Assignment ---\n",
        "    logging.info(\"\\nStep 3: Validating regime assignment...\")\n",
        "    is_valid, errors = validate_regime_assignment(df_with_regimes, regime_definitions)\n",
        "\n",
        "    # Check the validation outcome.\n",
        "    if not is_valid:\n",
        "        # If validation fails, construct a detailed error report and raise an exception.\n",
        "        error_report = \"\\n\".join([f\"- {error}\" for error in errors])\n",
        "        raise ValueError(f\"Post-assignment validation failed:\\n{error_report}\")\n",
        "    else:\n",
        "        logging.info(\"Regime assignment validation PASSED.\")\n",
        "\n",
        "    # --- Final Output ---\n",
        "    logging.info(\"\\n>>> Temporal regime assignment suite completed successfully. <<<\")\n",
        "    # Return the final, validated, and regime-partitioned DataFrame.\n",
        "    return df_with_regimes\n"
      ],
      "metadata": {
        "id": "HDf6TvN6uRVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Chronological Data Splitting Within Regimes\n",
        "\n",
        "# Define a type alias for clarity and maintainability.\n",
        "DataSplits = Dict[str, Dict[str, pd.DataFrame]]\n",
        "\n",
        "\n",
        "def perform_chronological_split(\n",
        "    regime_df: pd.DataFrame,\n",
        "    split_ratios: Dict[str, float]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Splits a single regime's DataFrame into chronological train/val/test sets.\n",
        "\n",
        "    This function is the core of the temporal splitting logic. It ensures that\n",
        "    the data is partitioned without lookahead bias by first sorting the data\n",
        "    by date and then slicing it based on the provided ratios.\n",
        "\n",
        "    Args:\n",
        "        regime_df: A DataFrame containing data for a single, isolated regime.\n",
        "        split_ratios: A dictionary specifying the ratios for 'training',\n",
        "                      'validation', and 'testing'.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the 'training', 'validation', and 'testing'\n",
        "        DataFrames.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the input DataFrame is too small to be split meaningfully.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    min_samples_for_split = 10\n",
        "    if len(regime_df) < min_samples_for_split:\n",
        "        raise ValueError(\n",
        "            f\"Cannot split regime data with fewer than {min_samples_for_split} \"\n",
        "            f\"samples. Found: {len(regime_df)}.\"\n",
        "        )\n",
        "\n",
        "    # --- Step 1: Ensure Chronological Order ---\n",
        "    # Sort the DataFrame by the 'date' level of the index. This is non-negotiable\n",
        "    # for preventing lookahead bias. We operate on a sorted view.\n",
        "    sorted_df = regime_df.sort_index(level='date')\n",
        "    n_samples = len(sorted_df)\n",
        "\n",
        "    # --- Step 2: Calculate Split Indices ---\n",
        "    # Use math.floor to get integer indices, ensuring clean boundaries.\n",
        "    train_ratio = split_ratios['training']\n",
        "    val_ratio = split_ratios['validation']\n",
        "\n",
        "    # Calculate the index where the training set ends.\n",
        "    train_end_idx = math.floor(train_ratio * n_samples)\n",
        "    # Calculate the index where the validation set ends.\n",
        "    val_end_idx = train_end_idx + math.floor(val_ratio * n_samples)\n",
        "\n",
        "    # --- Step 3: Perform Positional Slicing ---\n",
        "    # Use .iloc for positional slicing on the sorted DataFrame.\n",
        "    # Create deep copies to ensure the resulting splits are independent objects.\n",
        "    train_df = sorted_df.iloc[:train_end_idx].copy(deep=True)\n",
        "    val_df = sorted_df.iloc[train_end_idx:val_end_idx].copy(deep=True)\n",
        "    test_df = sorted_df.iloc[val_end_idx:].copy(deep=True)\n",
        "\n",
        "    # Log the size of each created split for auditability.\n",
        "    logging.info(\n",
        "        f\"Split {n_samples} samples -> \"\n",
        "        f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\"\n",
        "    )\n",
        "\n",
        "    # Return the dictionary of data splits.\n",
        "    return {'training': train_df, 'validation': val_df, 'testing': test_df}\n",
        "\n",
        "\n",
        "def create_regime_data_subsets(\n",
        "    df_regimes: pd.DataFrame,\n",
        "    split_ratios: Dict[str, float]\n",
        ") -> DataSplits:\n",
        "    \"\"\"\n",
        "    Orchestrates the splitting of the main DataFrame into 12 subsets.\n",
        "\n",
        "    This function iterates through each unique regime found in the data,\n",
        "    isolates the data for that regime, and then applies the chronological\n",
        "    splitting logic to generate train, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "        df_regimes: The DataFrame after regime labels have been assigned.\n",
        "        split_ratios: A dictionary specifying the split ratios.\n",
        "\n",
        "    Returns:\n",
        "        A nested dictionary of the structure:\n",
        "        {regime_name: {'training': df, 'validation': df, 'testing': df}}\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if 'regime' not in df_regimes.columns:\n",
        "        raise ValueError(\"Input DataFrame must contain a 'regime' column.\")\n",
        "\n",
        "    # Get the list of unique regimes to process.\n",
        "    regimes = df_regimes['regime'].unique()\n",
        "\n",
        "    # Initialize the nested dictionary to hold all 12 data subsets.\n",
        "    all_splits: DataSplits = {}\n",
        "\n",
        "    # --- Iterate and Split Each Regime ---\n",
        "    for regime_name in regimes:\n",
        "        logging.info(f\"--- Processing and splitting regime: {regime_name} ---\")\n",
        "        # Isolate the DataFrame for the current regime.\n",
        "        regime_specific_df = df_regimes[df_regimes['regime'] == regime_name]\n",
        "\n",
        "        # Delegate the actual splitting to the specialized function.\n",
        "        regime_splits = perform_chronological_split(regime_specific_df, split_ratios)\n",
        "\n",
        "        # Store the resulting splits in the main dictionary.\n",
        "        all_splits[regime_name] = regime_splits\n",
        "\n",
        "    return all_splits\n",
        "\n",
        "\n",
        "def validate_split_quality(\n",
        "    data_splits: DataSplits\n",
        ") -> Tuple[bool, List[str]]:\n",
        "    \"\"\"\n",
        "    Performs a comprehensive quality audit on the generated data splits.\n",
        "\n",
        "    This is a critical validation step to ensure the integrity of the\n",
        "    experimental setup. It verifies:\n",
        "    1.  Absolute temporal separation between train/val/test sets within each regime.\n",
        "    2.  No new tickers appear in validation or test sets that were not in the\n",
        "        training set of the same regime.\n",
        "    3.  Each split is non-empty.\n",
        "\n",
        "    Args:\n",
        "        data_splits: The nested dictionary of data splits.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A boolean, True if all quality checks pass, False otherwise.\n",
        "        - A list of strings, containing detailed descriptions of any validation failures.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect detailed error messages.\n",
        "    errors = []\n",
        "\n",
        "    # Iterate through each regime and its corresponding splits.\n",
        "    for regime_name, splits in data_splits.items():\n",
        "        train_df = splits['training']\n",
        "        val_df = splits['validation']\n",
        "        test_df = splits['testing']\n",
        "\n",
        "        # --- Step 1: Check for Empty Splits ---\n",
        "        if train_df.empty:\n",
        "            errors.append(f\"Regime '{regime_name}': Training set is empty.\")\n",
        "        if val_df.empty:\n",
        "            # A warning is more appropriate for val/test as they can be small.\n",
        "            logging.warning(f\"Regime '{regime_name}': Validation set is empty.\")\n",
        "        if test_df.empty:\n",
        "            logging.warning(f\"Regime '{regime_name}': Test set is empty.\")\n",
        "        # If training set is empty, further checks are meaningless.\n",
        "        if train_df.empty: continue\n",
        "\n",
        "        # --- Step 2: Validate Temporal Separation (No Lookahead Bias) ---\n",
        "        # Get the last date in training and first date in validation.\n",
        "        last_train_date = train_df.index.get_level_values('date').max()\n",
        "\n",
        "        if not val_df.empty:\n",
        "            first_val_date = val_df.index.get_level_values('date').min()\n",
        "            # The last training day must be strictly before the first validation day.\n",
        "            if not last_train_date < first_val_date:\n",
        "                errors.append(\n",
        "                    f\"Regime '{regime_name}': Temporal overlap detected between \"\n",
        "                    f\"training (ends {last_train_date.date()}) and validation \"\n",
        "                    f\"(starts {first_val_date.date()}).\"\n",
        "                )\n",
        "\n",
        "        if not val_df.empty and not test_df.empty:\n",
        "            last_val_date = val_df.index.get_level_values('date').max()\n",
        "            first_test_date = test_df.index.get_level_values('date').min()\n",
        "            # The last validation day must be strictly before the first test day.\n",
        "            if not last_val_date < first_test_date:\n",
        "                errors.append(\n",
        "                    f\"Regime '{regime_name}': Temporal overlap detected between \"\n",
        "                    f\"validation (ends {last_val_date.date()}) and test \"\n",
        "                    f\"(starts {first_test_date.date()}).\"\n",
        "                )\n",
        "\n",
        "        # --- Step 3: Validate Ticker Representation ---\n",
        "        # A model should not be evaluated on tickers it has never seen.\n",
        "        train_tickers = set(train_df.index.get_level_values('ticker').unique())\n",
        "\n",
        "        if not val_df.empty:\n",
        "            val_tickers = set(val_df.index.get_level_values('ticker').unique())\n",
        "            # Check for tickers in validation that are not in training.\n",
        "            unseen_in_val = val_tickers - train_tickers\n",
        "            if unseen_in_val:\n",
        "                errors.append(\n",
        "                    f\"Regime '{regime_name}': Unseen tickers found in validation set: \"\n",
        "                    f\"{unseen_in_val}.\"\n",
        "                )\n",
        "\n",
        "        if not test_df.empty:\n",
        "            test_tickers = set(test_df.index.get_level_values('ticker').unique())\n",
        "            # Check for tickers in test that are not in training.\n",
        "            unseen_in_test = test_tickers - train_tickers\n",
        "            if unseen_in_test:\n",
        "                errors.append(\n",
        "                    f\"Regime '{regime_name}': Unseen tickers found in test set: \"\n",
        "                    f\"{unseen_in_test}.\"\n",
        "                )\n",
        "\n",
        "    # --- Final Validation Result ---\n",
        "    is_valid = not errors\n",
        "    return is_valid, errors\n",
        "\n",
        "\n",
        "def run_chronological_splitting_suite(\n",
        "    df_regimes: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> DataSplits:\n",
        "    \"\"\"\n",
        "    Orchestrates the full chronological data splitting and validation pipeline.\n",
        "\n",
        "    This function executes the complete workflow for Task 5. It takes the\n",
        "    regime-partitioned DataFrame, splits it into 12 train/val/test subsets,\n",
        "    performs a rigorous quality audit on the splits, and returns the final\n",
        "    structured data.\n",
        "\n",
        "    Args:\n",
        "        df_regimes: The DataFrame after regime labels have been assigned.\n",
        "        study_config: The complete study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A nested dictionary containing the 12 validated data subsets.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the post-splitting validation fails.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Running Task 5: Chronological Data Splitting Suite ---\")\n",
        "\n",
        "    # Retrieve split ratios from the configuration.\n",
        "    split_ratios = study_config['experimental_design']['data_split_ratios']\n",
        "\n",
        "    # --- Step 1 & 2: Create Regime-Specific Data Subsets ---\n",
        "    logging.info(\"\\nStep 1 & 2: Creating 12 chronological data subsets (4 regimes x 3 splits)...\")\n",
        "    data_splits = create_regime_data_subsets(df_regimes, split_ratios)\n",
        "    logging.info(\"Data splitting complete.\")\n",
        "\n",
        "    # --- Step 3: Validate Split Quality ---\n",
        "    logging.info(\"\\nStep 3: Validating split quality (temporal separation, ticker consistency)...\")\n",
        "    is_valid, errors = validate_split_quality(data_splits)\n",
        "\n",
        "    # Check the validation outcome.\n",
        "    if not is_valid:\n",
        "        # If validation fails, construct a detailed error report and raise an exception.\n",
        "        error_report = \"\\n\".join([f\"- {error}\" for error in errors])\n",
        "        raise ValueError(f\"Data split quality validation failed:\\n{error_report}\")\n",
        "    else:\n",
        "        logging.info(\"Data split quality validation PASSED.\")\n",
        "\n",
        "    # --- Final Output ---\n",
        "    logging.info(\"\\n>>> Chronological data splitting suite completed successfully. <<<\")\n",
        "    # Return the final, validated data splits.\n",
        "    return data_splits\n"
      ],
      "metadata": {
        "id": "DEUs4-UWvL_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Cross-Regime Data Consistency Validation\n",
        "\n",
        "def analyze_ticker_consistency(\n",
        "    df_regimes: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Analyzes the consistency of the ticker universe across different regimes.\n",
        "\n",
        "    This function assesses the stability of the asset pool over time by\n",
        "    reporting how many unique tickers are present in each regime and what\n",
        "    percentage of the total historical universe this represents.\n",
        "\n",
        "    Args:\n",
        "        df_regimes: The DataFrame after regime labels have been assigned.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame indexed by regime name, detailing the ticker count,\n",
        "        universe coverage, and a list of tickers absent from that regime.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if 'regime' not in df_regimes.columns:\n",
        "        raise ValueError(\"Input DataFrame must contain a 'regime' column.\")\n",
        "    if not isinstance(df_regimes.index, pd.MultiIndex) or 'ticker' not in df_regimes.index.names:\n",
        "        raise ValueError(\"DataFrame must have a MultiIndex with a 'ticker' level.\")\n",
        "\n",
        "    # --- Step 1: Define the Master Universe of Tickers ---\n",
        "    # This is the set of all unique tickers ever observed in the dataset.\n",
        "    master_universe = set(df_regimes.index.get_level_values('ticker').unique())\n",
        "    n_universe = len(master_universe)\n",
        "\n",
        "    # --- Step 2: Calculate Per-Regime Ticker Inventories ---\n",
        "    # Group by regime and find the unique tickers within each group.\n",
        "    regime_tickers = df_regimes.groupby('regime').apply(\n",
        "        lambda df: set(df.index.get_level_values('ticker').unique())\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Generate the Consistency Report ---\n",
        "    # Create a report by iterating through the regime inventories.\n",
        "    report_data = []\n",
        "    for regime_name, tickers in regime_tickers.items():\n",
        "        # Calculate the number of tickers present in the regime.\n",
        "        ticker_count = len(tickers)\n",
        "        # Calculate the percentage of the master universe covered.\n",
        "        coverage_pct = (ticker_count / n_universe) * 100 if n_universe > 0 else 0\n",
        "        # Identify which tickers from the master universe are missing.\n",
        "        missing_tickers = sorted(list(master_universe - tickers))\n",
        "\n",
        "        # Append the structured data for this regime to our report list.\n",
        "        report_data.append({\n",
        "            'regime_name': regime_name,\n",
        "            'ticker_count': ticker_count,\n",
        "            'universe_coverage_pct': coverage_pct,\n",
        "            'missing_tickers': missing_tickers\n",
        "        })\n",
        "\n",
        "    # Convert the list of dictionaries to a DataFrame and set the index.\n",
        "    report_df = pd.DataFrame(report_data).set_index('regime_name')\n",
        "    return report_df\n",
        "\n",
        "\n",
        "def analyze_return_distribution_stability(\n",
        "    df_regimes: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Analyzes the stability of the 'target_return' distribution across regimes.\n",
        "\n",
        "    This function provides a quantitative justification for regime-based\n",
        "    modeling by showing how the statistical properties of stock returns change\n",
        "    across different macroeconomic periods. It calculates:\n",
        "    1.  The first four statistical moments (mean, std, skew, kurtosis) for each regime.\n",
        "    2.  A matrix of p-values from pairwise Kolmogorov-Smirnov tests to formally\n",
        "        assess if the return distributions are statistically different.\n",
        "\n",
        "    Args:\n",
        "        df_regimes: The DataFrame after regime labels have been assigned.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A DataFrame of statistical moments for each regime.\n",
        "        - A DataFrame (p-value matrix) from pairwise KS tests between regimes.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if 'regime' not in df_regimes.columns or 'target_return' not in df_regimes.columns:\n",
        "        raise ValueError(\"Input DataFrame must contain 'regime' and 'target_return' columns.\")\n",
        "\n",
        "    # --- Step 1: Calculate Statistical Moments per Regime ---\n",
        "    # Use .agg() for a single, efficient pass to compute all required statistics.\n",
        "    # We explicitly use scipy's functions for skew and kurtosis for consistency.\n",
        "    moment_stats = df_regimes.groupby('regime')['target_return'].agg(\n",
        "        mean='mean',\n",
        "        std='std',\n",
        "        skew=lambda x: stats.skew(x.dropna()),\n",
        "        kurtosis=lambda x: stats.kurtosis(x.dropna()) # Fisher's definition (normal=0)\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Perform Pairwise Kolmogorov-Smirnov Tests ---\n",
        "    # Get the unique regime names for pairwise comparisons.\n",
        "    regimes = df_regimes['regime'].unique().tolist()\n",
        "    # Initialize a DataFrame to store the p-values of the KS tests.\n",
        "    ks_matrix = pd.DataFrame(np.nan, index=regimes, columns=regimes)\n",
        "\n",
        "    # Iterate through all unique pairs of regimes.\n",
        "    for regime1, regime2 in combinations(regimes, 2):\n",
        "        # Extract the non-NaN return series for each regime in the pair.\n",
        "        returns1 = df_regimes[df_regimes['regime'] == regime1]['target_return'].dropna()\n",
        "        returns2 = df_regimes[df_regimes['regime'] == regime2]['target_return'].dropna()\n",
        "\n",
        "        # Perform the two-sample KS test.\n",
        "        ks_statistic, p_value = stats.ks_2samp(returns1, returns2)\n",
        "\n",
        "        # Store the p-value in the matrix (it's symmetric).\n",
        "        ks_matrix.loc[regime1, regime2] = p_value\n",
        "        ks_matrix.loc[regime2, regime1] = p_value\n",
        "\n",
        "    # Set the diagonal to 1.0, as a distribution is identical to itself.\n",
        "    np.fill_diagonal(ks_matrix.values, 1.0)\n",
        "\n",
        "    return moment_stats, ks_matrix\n",
        "\n",
        "\n",
        "def analyze_news_coverage_consistency(\n",
        "    df_regimes: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Analyzes the consistency of news coverage across different regimes.\n",
        "\n",
        "    This function quantifies the volume and availability of the text data,\n",
        "    which is crucial for understanding the reliability of NLP models.\n",
        "\n",
        "    It calculates for each regime:\n",
        "    1.  News Coverage %: The percentage of observations that have non-empty text.\n",
        "    2.  Avg. Text Length: The average character length of the news articles,\n",
        "        calculated only on non-empty entries to avoid skew.\n",
        "\n",
        "    Args:\n",
        "        df_regimes: The DataFrame after regime labels have been assigned.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame indexed by regime name, detailing the news coverage metrics.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if 'regime' not in df_regimes.columns or 'aggregated_text' not in df_regimes.columns:\n",
        "        raise ValueError(\"Input DataFrame must contain 'regime' and 'aggregated_text' columns.\")\n",
        "\n",
        "    # --- Define Custom Aggregation Functions ---\n",
        "    # This function calculates the percentage of non-empty strings in a Series.\n",
        "    def coverage_percentage(series: pd.Series) -> float:\n",
        "        if len(series) == 0:\n",
        "            return 0.0\n",
        "        non_empty_count = (series != '').sum()\n",
        "        return (non_empty_count / len(series)) * 100\n",
        "\n",
        "    # This function calculates the mean length of only the non-empty strings.\n",
        "    def average_text_length(series: pd.Series) -> float:\n",
        "        non_empty_series = series[series != '']\n",
        "        if len(non_empty_series) == 0:\n",
        "            return 0.0\n",
        "        return non_empty_series.str.len().mean()\n",
        "\n",
        "    # --- Step 1: Calculate News Metrics per Regime ---\n",
        "    # Use .agg() with the custom functions for a clean and efficient calculation.\n",
        "    news_stats = df_regimes.groupby('regime')['aggregated_text'].agg(\n",
        "        news_coverage_pct=coverage_percentage,\n",
        "        avg_text_length_chars=average_text_length\n",
        "    )\n",
        "\n",
        "    return news_stats\n",
        "\n",
        "\n",
        "def run_cross_regime_consistency_suite(\n",
        "    df_regimes: pd.DataFrame\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Orchestrates the full suite of cross-regime data consistency checks.\n",
        "\n",
        "    This function executes all validation steps from Task 6 and prints a\n",
        "    comprehensive diagnostic report to the console, justifying the need for\n",
        "    a regime-based modeling approach.\n",
        "\n",
        "    Args:\n",
        "        df_regimes: The DataFrame after regime labels have been assigned.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Running Task 6: Cross-Regime Data Consistency Validation Suite ---\")\n",
        "\n",
        "    # --- Step 1: Analyze Ticker Consistency ---\n",
        "    logging.info(\"\\nStep 1: Analyzing Ticker Universe Consistency...\")\n",
        "    ticker_report = analyze_ticker_consistency(df_regimes)\n",
        "    logging.info(\"Ticker Consistency Report:\\n\" + ticker_report.to_string(\n",
        "        formatters={'universe_coverage_pct': '{:.2f}%'.format}\n",
        "    ))\n",
        "\n",
        "    # --- Step 2: Analyze Target Return Distribution Stability ---\n",
        "    logging.info(\"\\nStep 2: Analyzing Target Return Distribution Stability...\")\n",
        "    moment_stats, ks_matrix = analyze_return_distribution_stability(df_regimes)\n",
        "    logging.info(\"Statistical Moments of Target Return per Regime:\\n\" + moment_stats.to_string(float_format='{:.4f}'.format))\n",
        "    logging.info(\"\\nPairwise KS-Test p-values (H0: Distributions are identical):\\n\" + ks_matrix.to_string(float_format='{:.4f}'.format))\n",
        "    # Interpretation note for the user.\n",
        "    logging.info(\"(Note: Low p-values, e.g., < 0.05, suggest distributions are significantly different)\")\n",
        "\n",
        "    # --- Step 3: Analyze News Coverage Consistency ---\n",
        "    logging.info(\"\\nStep 3: Analyzing News Coverage Consistency...\")\n",
        "    news_report = analyze_news_coverage_consistency(df_regimes)\n",
        "    logging.info(\"News Coverage Report:\\n\" + news_report.to_string(\n",
        "        formatters={\n",
        "            'news_coverage_pct': '{:.2f}%'.format,\n",
        "            'avg_text_length_chars': '{:.1f}'.format\n",
        "        }\n",
        "    ))\n",
        "\n",
        "    logging.info(\"\\n>>> Cross-regime consistency validation suite completed successfully. <<<\")\n"
      ],
      "metadata": {
        "id": "bhd5TWplwQqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: TF-IDF Vectorization Implementation\n",
        "\n",
        "def create_and_fit_tfidf_vectorizer(\n",
        "    data_splits: DataSplits,\n",
        "    tfidf_params: Dict[str, Any]\n",
        ") -> Tuple[TfidfVectorizer, TfidfFeatures]:\n",
        "    \"\"\"\n",
        "    Initializes, fits, and transforms text data using a TF-IDF vectorizer.\n",
        "\n",
        "    This function implements the methodologically critical \"fit on train,\n",
        "    transform all\" paradigm. It creates a single, unified vocabulary by\n",
        "    fitting the vectorizer exclusively on the combined training data from all\n",
        "    regimes. This ensures a consistent feature space across the entire\n",
        "    experiment, which is paramount for valid cross-regime model comparison.\n",
        "\n",
        "    Args:\n",
        "        data_splits: The nested dictionary of data splits from Task 5.\n",
        "        tfidf_params: A dictionary of parameters for the TfidfVectorizer,\n",
        "                      e.g., {'max_features': 2000}.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - The single, fitted TfidfVectorizer object.\n",
        "        - A nested dictionary containing the transformed sparse matrices for\n",
        "          all 12 data subsets, mirroring the input structure.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not data_splits:\n",
        "        raise ValueError(\"data_splits dictionary cannot be empty.\")\n",
        "    if 'max_features' not in tfidf_params:\n",
        "        raise ValueError(\"tfidf_params must contain 'max_features'.\")\n",
        "\n",
        "    # --- Step 1: Configure TF-IDF Vectorizer ---\n",
        "    # Instantiate the vectorizer with the exact parameters from the config.\n",
        "    # These parameters are derived from the LaTeX context (Section 5.1 and 7).\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=tfidf_params.get('max_features', 2000),\n",
        "        ngram_range=tfidf_params.get('ngram_range', (1, 2)),\n",
        "        min_df=tfidf_params.get('min_df', 2),\n",
        "        max_df=tfidf_params.get('max_df', 0.95),\n",
        "        stop_words='english',\n",
        "        lowercase=True,\n",
        "        norm='l2',\n",
        "        use_idf=True,\n",
        "        smooth_idf=True,\n",
        "        sublinear_tf=False\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Assemble the Global Training Corpus ---\n",
        "    # Concatenate the 'aggregated_text' from all training splits across all regimes.\n",
        "    # This is the ONLY data the vectorizer's vocabulary will be based on.\n",
        "    logging.info(\"Assembling global training corpus for TF-IDF fitting...\")\n",
        "    training_corpora = [\n",
        "        splits['training']['aggregated_text']\n",
        "        for regime, splits in data_splits.items()\n",
        "        if not splits['training'].empty\n",
        "    ]\n",
        "    global_training_corpus = pd.concat(training_corpora, ignore_index=True)\n",
        "    logging.info(f\"Fitting TF-IDF on a corpus of {len(global_training_corpus)} training documents.\")\n",
        "\n",
        "    # --- Step 3: Fit the Vectorizer ---\n",
        "    # Fit the vectorizer exclusively on the assembled training data.\n",
        "    vectorizer.fit(global_training_corpus)\n",
        "\n",
        "    # --- Step 4: Transform All 12 Data Subsets ---\n",
        "    # Initialize the nested dictionary to store the output sparse matrices.\n",
        "    tfidf_features: TfidfFeatures = {regime: {} for regime in data_splits}\n",
        "\n",
        "    # Iterate through every regime and every split.\n",
        "    for regime_name, splits in data_splits.items():\n",
        "        for split_name, df in splits.items():\n",
        "            logging.info(f\"Transforming text for {regime_name} - {split_name} split...\")\n",
        "            # Use the already-fitted vectorizer to transform the data.\n",
        "            # This ensures the same feature mapping is used everywhere.\n",
        "            if not df.empty:\n",
        "                # Transform the text and store the resulting sparse matrix.\n",
        "                transformed_matrix = vectorizer.transform(df['aggregated_text'])\n",
        "                tfidf_features[regime_name][split_name] = transformed_matrix\n",
        "            else:\n",
        "                # Handle empty splits by creating an empty sparse matrix with the correct shape.\n",
        "                tfidf_features[regime_name][split_name] = csr_matrix(\n",
        "                    (0, tfidf_params['max_features'])\n",
        "                )\n",
        "\n",
        "    return vectorizer, tfidf_features\n",
        "\n",
        "\n",
        "def validate_tfidf_features(\n",
        "    tfidf_features: TfidfFeatures,\n",
        "    data_splits: DataSplits,\n",
        "    expected_features: int\n",
        ") -> Tuple[bool, List[str]]:\n",
        "    \"\"\"\n",
        "    Performs a quality and integrity audit on the generated TF-IDF features.\n",
        "\n",
        "    This function validates the output of the vectorization process to ensure\n",
        "    that the resulting feature matrices are correctly shaped, normalized, and\n",
        "    consistent with the input data.\n",
        "\n",
        "    Args:\n",
        "        tfidf_features: The nested dictionary of transformed sparse matrices.\n",
        "        data_splits: The original nested dictionary of data splits.\n",
        "        expected_features: The expected number of features (e.g., 2000).\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A boolean, True if all checks pass, False otherwise.\n",
        "        - A list of strings detailing any validation failures.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect detailed error messages.\n",
        "    errors = []\n",
        "\n",
        "    # Iterate through every generated feature matrix.\n",
        "    for regime_name, splits in tfidf_features.items():\n",
        "        for split_name, matrix in splits.items():\n",
        "            # Get the corresponding original DataFrame for row count comparison.\n",
        "            original_df = data_splits[regime_name][split_name]\n",
        "\n",
        "            # --- Step 1: Validate Matrix Shape ---\n",
        "            # Check number of rows.\n",
        "            if matrix.shape[0] != len(original_df):\n",
        "                errors.append(\n",
        "                    f\"[{regime_name}][{split_name}]: Row count mismatch. \"\n",
        "                    f\"Expected {len(original_df)}, found {matrix.shape[0]}.\"\n",
        "                )\n",
        "            # Check number of columns (features).\n",
        "            if matrix.shape[1] != expected_features:\n",
        "                errors.append(\n",
        "                    f\"[{regime_name}][{split_name}]: Feature count mismatch. \"\n",
        "                    f\"Expected {expected_features}, found {matrix.shape[1]}.\"\n",
        "                )\n",
        "\n",
        "            # Skip further checks for empty matrices.\n",
        "            if matrix.shape[0] == 0:\n",
        "                continue\n",
        "\n",
        "            # --- Step 2: Validate L2 Normalization ---\n",
        "            # Calculate the L2 norm for each row directly on the sparse matrix.\n",
        "            # This is memory-efficient as it avoids creating a dense matrix.\n",
        "            row_norms = np.sqrt(matrix.power(2).sum(axis=1))\n",
        "            # Check if all norms are close to 1.0 (or 0.0 for empty documents).\n",
        "            # A document with no terms from the vocabulary will have a zero vector.\n",
        "            if not np.all(np.isclose(row_norms, 1.0) | np.isclose(row_norms, 0.0)):\n",
        "                errors.append(\n",
        "                    f\"[{regime_name}][{split_name}]: L2 normalization validation failed. \"\n",
        "                    \"Not all rows are unit vectors.\"\n",
        "                )\n",
        "\n",
        "            # --- Step 3: Report on Sparsity ---\n",
        "            # Sparsity = 1 - (non-zero elements / total elements)\n",
        "            sparsity = 1.0 - (matrix.nnz / (matrix.shape[0] * matrix.shape[1]))\n",
        "            logging.info(\n",
        "                f\"[{regime_name}][{split_name}]: Matrix shape: {matrix.shape}, \"\n",
        "                f\"Sparsity: {sparsity:.2%}\"\n",
        "            )\n",
        "\n",
        "    is_valid = not errors\n",
        "    return is_valid, errors\n",
        "\n",
        "\n",
        "def run_tfidf_vectorization_suite(\n",
        "    data_splits: DataSplits,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Tuple[TfidfVectorizer, TfidfFeatures]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full TF-IDF feature engineering and validation pipeline.\n",
        "\n",
        "    Args:\n",
        "        data_splits: The nested dictionary of data splits from Task 5.\n",
        "        study_config: The complete study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the fitted vectorizer and the dictionary of features.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the post-vectorization validation fails.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Running Task 7: TF-IDF Vectorization Suite ---\")\n",
        "\n",
        "    # Retrieve TF-IDF parameters from the configuration.\n",
        "    tfidf_params = study_config['feature_engineering']['tfidf']\n",
        "\n",
        "    # --- Step 1 & 2: Configure, Fit, and Transform ---\n",
        "    logging.info(\"\\nStep 1 & 2: Fitting a single global vectorizer and transforming all splits...\")\n",
        "    vectorizer, tfidf_features = create_and_fit_tfidf_vectorizer(data_splits, tfidf_params)\n",
        "    logging.info(f\"TF-IDF fitting and transformation complete. Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
        "\n",
        "    # --- Step 3: Validate Feature Quality ---\n",
        "    logging.info(\"\\nStep 3: Validating TF-IDF feature quality...\")\n",
        "    is_valid, errors = validate_tfidf_features(\n",
        "        tfidf_features,\n",
        "        data_splits,\n",
        "        tfidf_params['max_features']\n",
        "    )\n",
        "\n",
        "    if not is_valid:\n",
        "        error_report = \"\\n\".join([f\"- {error}\" for error in errors])\n",
        "        raise ValueError(f\"TF-IDF feature validation failed:\\n{error_report}\")\n",
        "    else:\n",
        "        logging.info(\"TF-IDF feature validation PASSED.\")\n",
        "\n",
        "    logging.info(\"\\n>>> TF-IDF vectorization suite completed successfully. <<<\")\n",
        "    return vectorizer, tfidf_features\n"
      ],
      "metadata": {
        "id": "Z_lk3rcRxlb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Sentence Embedding Extraction\n",
        "\n",
        "def initialize_sentence_transformer(\n",
        "    model_identifier: str\n",
        ") -> SentenceTransformer:\n",
        "    \"\"\"\n",
        "    Initializes a SentenceTransformer model and moves it to the optimal device.\n",
        "\n",
        "    This function handles the loading of a pre-trained model from the\n",
        "    HuggingFace Hub, detects GPU availability, and sets the model to\n",
        "    evaluation mode for deterministic inference.\n",
        "\n",
        "    Args:\n",
        "        model_identifier: The HuggingFace identifier for the model,\n",
        "                          e.g., 'sentence-transformers/all-MiniLM-L6-v2'.\n",
        "\n",
        "    Returns:\n",
        "        The initialized SentenceTransformer model object.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If the model cannot be loaded due to network issues or\n",
        "                      an invalid identifier.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Determine Optimal Computing Device ---\n",
        "    # Check for CUDA-enabled GPU availability for accelerated processing.\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    logging.info(f\"Initializing sentence transformer on device: '{device}'\")\n",
        "\n",
        "    # --- Step 2: Load the Pre-trained Model ---\n",
        "    try:\n",
        "        # Instantiate the model. This will download it from the Hub if not cached.\n",
        "        model = SentenceTransformer(model_identifier, device=device)\n",
        "\n",
        "        # --- Step 3: Set Model to Evaluation Mode ---\n",
        "        # This is a critical step to ensure deterministic outputs by disabling\n",
        "        # layers like dropout that behave differently during training.\n",
        "        model.eval()\n",
        "\n",
        "        logging.info(f\"Successfully loaded model '{model_identifier}'.\")\n",
        "        logging.info(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch potential errors (e.g., network, invalid name) and raise a specific exception.\n",
        "        raise RuntimeError(\n",
        "            f\"Failed to initialize SentenceTransformer model '{model_identifier}'. \"\n",
        "            f\"Please check the model name and your network connection. Original error: {e}\"\n",
        "        )\n",
        "\n",
        "\n",
        "def extract_sentence_embeddings(\n",
        "    data_splits: DataSplits,\n",
        "    model: SentenceTransformer,\n",
        "    batch_size: int = 64\n",
        ") -> EmbeddingFeatures:\n",
        "    \"\"\"\n",
        "    Generates sentence embeddings for all text data in the data splits.\n",
        "\n",
        "    This function uses a pre-initialized SentenceTransformer model to encode\n",
        "    the 'aggregated_text' from all 12 data subsets into dense numerical vectors.\n",
        "    It leverages efficient batch processing for high throughput.\n",
        "\n",
        "    Args:\n",
        "        data_splits: The nested dictionary of data splits from Task 5.\n",
        "        model: The initialized SentenceTransformer model.\n",
        "        batch_size: The number of sentences to process in a single batch.\n",
        "\n",
        "    Returns:\n",
        "        A nested dictionary containing the NumPy arrays of embeddings,\n",
        "        mirroring the input structure.\n",
        "    \"\"\"\n",
        "    # Initialize the nested dictionary to store the output embedding arrays.\n",
        "    embedding_features: EmbeddingFeatures = {regime: {} for regime in data_splits}\n",
        "\n",
        "    # Iterate through every regime and every split.\n",
        "    for regime_name, splits in data_splits.items():\n",
        "        for split_name, df in splits.items():\n",
        "            logging.info(f\"Generating embeddings for {regime_name} - {split_name} split...\")\n",
        "\n",
        "            # Extract the text corpus for the current split.\n",
        "            corpus = df['aggregated_text'].tolist()\n",
        "\n",
        "            if not corpus:\n",
        "                # Handle empty splits by creating an empty array with the correct shape.\n",
        "                embedding_dim = model.get_sentence_embedding_dimension()\n",
        "                embeddings = np.empty((0, embedding_dim), dtype=np.float32)\n",
        "            else:\n",
        "                # --- Use model.encode for efficient, batched inference ---\n",
        "                embeddings = model.encode(\n",
        "                    corpus,\n",
        "                    batch_size=batch_size,\n",
        "                    show_progress_bar=True,  # Provides valuable user feedback.\n",
        "                    convert_to_numpy=True    # Directly output as a NumPy array.\n",
        "                )\n",
        "\n",
        "            # Store the resulting embedding matrix.\n",
        "            embedding_features[regime_name][split_name] = embeddings\n",
        "\n",
        "    return embedding_features\n",
        "\n",
        "\n",
        "def validate_embedding_features(\n",
        "    embedding_features: EmbeddingFeatures,\n",
        "    data_splits: DataSplits,\n",
        "    expected_dimension: int\n",
        ") -> Tuple[bool, List[str]]:\n",
        "    \"\"\"\n",
        "    Performs a quality and integrity audit on the generated sentence embeddings.\n",
        "\n",
        "    This function validates the output of the embedding process to ensure\n",
        "    that the resulting matrices are correctly shaped, numerically stable, and\n",
        "    properly normalized.\n",
        "\n",
        "    Args:\n",
        "        embedding_features: The nested dictionary of embedding matrices.\n",
        "        data_splits: The original nested dictionary of data splits.\n",
        "        expected_dimension: The expected embedding dimension of the model.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A boolean, True if all checks pass, False otherwise.\n",
        "        - A list of strings detailing any validation failures.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect detailed error messages.\n",
        "    errors = []\n",
        "\n",
        "    # Iterate through every generated embedding matrix.\n",
        "    for regime_name, splits in embedding_features.items():\n",
        "        for split_name, embeddings in splits.items():\n",
        "            original_df = data_splits[regime_name][split_name]\n",
        "\n",
        "            # --- Step 1: Validate Matrix Shape ---\n",
        "            if embeddings.shape[0] != len(original_df):\n",
        "                errors.append(\n",
        "                    f\"[{regime_name}][{split_name}]: Row count mismatch. \"\n",
        "                    f\"Expected {len(original_df)}, found {embeddings.shape[0]}.\"\n",
        "                )\n",
        "            if embeddings.shape[1] != expected_dimension and embeddings.shape[0] > 0:\n",
        "                errors.append(\n",
        "                    f\"[{regime_name}][{split_name}]: Dimension mismatch. \"\n",
        "                    f\"Expected {expected_dimension}, found {embeddings.shape[1]}.\"\n",
        "                )\n",
        "\n",
        "            # Skip further checks for empty matrices.\n",
        "            if embeddings.shape[0] == 0:\n",
        "                continue\n",
        "\n",
        "            # --- Step 2: Validate Numerical Integrity ---\n",
        "            if np.isnan(embeddings).any():\n",
        "                errors.append(f\"[{regime_name}][{split_name}]: Found NaN values in embeddings.\")\n",
        "            if np.isinf(embeddings).any():\n",
        "                errors.append(f\"[{regime_name}][{split_name}]: Found infinite values in embeddings.\")\n",
        "\n",
        "            # --- Step 3: Validate L2 Normalization ---\n",
        "            # Calculate the L2 norm for each embedding vector.\n",
        "            norms = np.linalg.norm(embeddings, axis=1)\n",
        "            # Check if all norms are approximately equal to 1.0.\n",
        "            if not np.all(np.isclose(norms, 1.0, atol=1e-6)):\n",
        "                errors.append(\n",
        "                    f\"[{regime_name}][{split_name}]: L2 normalization validation failed. \"\n",
        "                    \"Not all embedding vectors are unit vectors.\"\n",
        "                )\n",
        "\n",
        "    is_valid = not errors\n",
        "    return is_valid, errors\n",
        "\n",
        "\n",
        "def run_embedding_extraction_suite(\n",
        "    data_splits: DataSplits,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> EmbeddingFeatures:\n",
        "    \"\"\"\n",
        "    Orchestrates the full sentence embedding extraction and validation pipeline.\n",
        "\n",
        "    Args:\n",
        "        data_splits: The nested dictionary of data splits from Task 5.\n",
        "        study_config: The complete study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A nested dictionary containing the 12 validated embedding matrices.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the post-extraction validation fails.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Running Task 8: Sentence Embedding Extraction Suite ---\")\n",
        "\n",
        "    # Retrieve embedding parameters from the configuration.\n",
        "    embedding_params = study_config['feature_engineering']['sentence_embeddings']\n",
        "    model_id = embedding_params['model_identifier']\n",
        "    expected_dim = embedding_params['embedding_dimension']\n",
        "\n",
        "    # --- Step 1: Initialize the Model ---\n",
        "    logging.info(\"\\nStep 1: Initializing Sentence Transformer model...\")\n",
        "    model = initialize_sentence_transformer(model_id)\n",
        "\n",
        "    # --- Step 2: Extract Embeddings ---\n",
        "    logging.info(\"\\nStep 2: Extracting embeddings for all 12 data splits...\")\n",
        "    # A batch size of 64 is a reasonable default for models of this size.\n",
        "    embedding_features = extract_sentence_embeddings(data_splits, model, batch_size=64)\n",
        "    logging.info(\"Embedding extraction complete.\")\n",
        "\n",
        "    # --- Step 3: Validate Feature Quality ---\n",
        "    logging.info(\"\\nStep 3: Validating embedding feature quality...\")\n",
        "    is_valid, errors = validate_embedding_features(\n",
        "        embedding_features,\n",
        "        data_splits,\n",
        "        expected_dim\n",
        "    )\n",
        "\n",
        "    if not is_valid:\n",
        "        error_report = \"\\n\".join([f\"- {error}\" for error in errors])\n",
        "        raise ValueError(f\"Embedding feature validation failed:\\n{error_report}\")\n",
        "    else:\n",
        "        logging.info(\"Embedding feature validation PASSED.\")\n",
        "\n",
        "    logging.info(\"\\n>>> Sentence embedding extraction suite completed successfully. <<<\")\n",
        "    return embedding_features\n"
      ],
      "metadata": {
        "id": "cyzf0S8lywpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Feature Concatenation and Validation\n",
        "\n",
        "def concatenate_features(\n",
        "    tfidf_features: TfidfFeatures,\n",
        "    embedding_features: EmbeddingFeatures\n",
        ") -> CombinedFeatures:\n",
        "    \"\"\"\n",
        "    Concatenates sparse TF-IDF features and dense sentence embeddings.\n",
        "\n",
        "    This function synthesizes the two feature types into a single, unified\n",
        "    feature matrix for each of the 12 data subsets. It ensures mathematical\n",
        "    correctness and memory efficiency.\n",
        "\n",
        "    The concatenation follows the precise formulation:\n",
        "    x_combined = [v_tfidf || e_sentence]\n",
        "\n",
        "    Args:\n",
        "        tfidf_features: Nested dictionary of TF-IDF sparse matrices.\n",
        "        embedding_features: Nested dictionary of sentence embedding NumPy arrays.\n",
        "\n",
        "    Returns:\n",
        "        A new nested dictionary containing the concatenated dense feature\n",
        "        matrices as NumPy arrays of dtype float32.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If there is a row count mismatch between corresponding\n",
        "                    TF-IDF and embedding matrices.\n",
        "        MemoryError: If converting a sparse matrix to dense exceeds available memory.\n",
        "    \"\"\"\n",
        "    # Initialize the nested dictionary to store the output combined matrices.\n",
        "    combined_features: CombinedFeatures = {regime: {} for regime in tfidf_features}\n",
        "\n",
        "    # Iterate through every regime and split.\n",
        "    for regime_name, splits in tfidf_features.items():\n",
        "        for split_name, tfidf_matrix in splits.items():\n",
        "            # Retrieve the corresponding embedding matrix.\n",
        "            embedding_matrix = embedding_features[regime_name][split_name]\n",
        "\n",
        "            # --- Critical Safety Check: Row Alignment ---\n",
        "            # Assert that the number of samples is identical before concatenation.\n",
        "            if tfidf_matrix.shape[0] != embedding_matrix.shape[0]:\n",
        "                raise ValueError(\n",
        "                    f\"[{regime_name}][{split_name}]: Row count mismatch. \"\n",
        "                    f\"TF-IDF has {tfidf_matrix.shape[0]} rows, but embeddings \"\n",
        "                    f\"have {embedding_matrix.shape[0]} rows.\"\n",
        "                )\n",
        "\n",
        "            # Handle empty splits gracefully.\n",
        "            if tfidf_matrix.shape[0] == 0:\n",
        "                # If empty, create an empty combined matrix with the correct number of columns.\n",
        "                combined_dim = tfidf_matrix.shape[1] + embedding_matrix.shape[1]\n",
        "                combined_features[regime_name][split_name] = np.empty(\n",
        "                    (0, combined_dim), dtype=np.float32\n",
        "                )\n",
        "                continue\n",
        "\n",
        "            # --- Step 1: Convert Sparse to Dense and Concatenate ---\n",
        "            try:\n",
        "                # Convert the sparse TF-IDF matrix to a dense NumPy array.\n",
        "                # This is a potential memory bottleneck for extremely large datasets.\n",
        "                tfidf_dense = tfidf_matrix.toarray()\n",
        "\n",
        "                # Concatenate the two dense matrices horizontally.\n",
        "                # np.hstack is a clear and efficient choice for this.\n",
        "                concatenated_matrix = np.hstack([tfidf_dense, embedding_matrix])\n",
        "\n",
        "                # --- Step 2: Standardize Data Type ---\n",
        "                # Cast the final matrix to float32 for memory efficiency and GPU compatibility.\n",
        "                combined_features[regime_name][split_name] = concatenated_matrix.astype(np.float32)\n",
        "\n",
        "            except MemoryError:\n",
        "                # Provide a specific, actionable error message if densification fails.\n",
        "                raise MemoryError(\n",
        "                    f\"[{regime_name}][{split_name}]: Failed to convert sparse TF-IDF \"\n",
        "                    f\"matrix of shape {tfidf_matrix.shape} to dense. \"\n",
        "                    \"The dataset may be too large for available RAM.\"\n",
        "                )\n",
        "\n",
        "    return combined_features\n",
        "\n",
        "\n",
        "def validate_combined_features(\n",
        "    combined_features: CombinedFeatures,\n",
        "    data_splits: Dict[str, Dict[str, pd.DataFrame]],\n",
        "    expected_dimension: int\n",
        ") -> Tuple[bool, List[str]]:\n",
        "    \"\"\"\n",
        "    Performs a quality and integrity audit on the combined feature matrices.\n",
        "\n",
        "    This function validates the output of the concatenation process to ensure\n",
        "    the final matrices are correctly shaped and numerically stable.\n",
        "\n",
        "    Args:\n",
        "        combined_features: The nested dictionary of combined feature matrices.\n",
        "        data_splits: The original nested dictionary of data splits.\n",
        "        expected_dimension: The expected total number of features after\n",
        "                            concatenation (e.g., 2384).\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A boolean, True if all checks pass, False otherwise.\n",
        "        - A list of strings detailing any validation failures.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect detailed error messages.\n",
        "    errors = []\n",
        "\n",
        "    # Iterate through every generated combined feature matrix.\n",
        "    for regime_name, splits in combined_features.items():\n",
        "        for split_name, matrix in splits.items():\n",
        "            original_df = data_splits[regime_name][split_name]\n",
        "\n",
        "            # --- Step 1: Validate Matrix Shape ---\n",
        "            if matrix.shape[0] != len(original_df):\n",
        "                errors.append(\n",
        "                    f\"[{regime_name}][{split_name}]: Row count mismatch. \"\n",
        "                    f\"Expected {len(original_df)}, found {matrix.shape[0]}.\"\n",
        "                )\n",
        "            if matrix.shape[1] != expected_dimension and matrix.shape[0] > 0:\n",
        "                errors.append(\n",
        "                    f\"[{regime_name}][{split_name}]: Feature dimension mismatch. \"\n",
        "                    f\"Expected {expected_dimension}, found {matrix.shape[1]}.\"\n",
        "                )\n",
        "\n",
        "            # Skip further checks for empty matrices.\n",
        "            if matrix.shape[0] == 0:\n",
        "                continue\n",
        "\n",
        "            # --- Step 2: Validate Numerical Integrity ---\n",
        "            if np.isnan(matrix).any():\n",
        "                errors.append(f\"[{regime_name}][{split_name}]: Found NaN values in combined features.\")\n",
        "            if np.isinf(matrix).any():\n",
        "                errors.append(f\"[{regime_name}][{split_name}]: Found infinite values in combined features.\")\n",
        "\n",
        "    is_valid = not errors\n",
        "    return is_valid, errors\n",
        "\n",
        "\n",
        "def run_feature_concatenation_suite(\n",
        "    tfidf_features: TfidfFeatures,\n",
        "    embedding_features: EmbeddingFeatures,\n",
        "    data_splits: DataSplits,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> CombinedFeatures:\n",
        "    \"\"\"\n",
        "    Orchestrates the full feature concatenation and validation pipeline.\n",
        "\n",
        "    Args:\n",
        "        tfidf_features: Nested dictionary of TF-IDF sparse matrices.\n",
        "        embedding_features: Nested dictionary of sentence embedding NumPy arrays.\n",
        "        data_splits: The original nested dictionary of data splits.\n",
        "        study_config: The complete study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A nested dictionary containing the 12 validated combined feature matrices.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the post-concatenation validation fails.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Running Task 9: Feature Concatenation and Validation Suite ---\")\n",
        "\n",
        "    # --- Step 1: Concatenate Features ---\n",
        "    logging.info(\"\\nStep 1: Concatenating TF-IDF and embedding features for all 12 splits...\")\n",
        "    combined_features = concatenate_features(tfidf_features, embedding_features)\n",
        "    logging.info(\"Feature concatenation complete.\")\n",
        "\n",
        "    # --- Step 2: Validate Combined Features ---\n",
        "    logging.info(\"\\nStep 2: Validating combined feature matrices...\")\n",
        "    # Retrieve expected dimension from the configuration for validation.\n",
        "    expected_dim = study_config['model_training']['architectures']['feature_transformer']['input_size']\n",
        "    is_valid, errors = validate_combined_features(\n",
        "        combined_features,\n",
        "        data_splits,\n",
        "        expected_dim\n",
        "    )\n",
        "\n",
        "    if not is_valid:\n",
        "        error_report = \"\\n\".join([f\"- {error}\" for error in errors])\n",
        "        raise ValueError(f\"Combined feature validation failed:\\n{error_report}\")\n",
        "    else:\n",
        "        logging.info(\"Combined feature validation PASSED.\")\n",
        "\n",
        "    logging.info(\"\\n>>> Feature concatenation suite completed successfully. <<<\")\n",
        "    return combined_features\n"
      ],
      "metadata": {
        "id": "kW9mC0nh0QkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: LSTM Architecture Specification\n",
        "\n",
        "class LSTMRegressionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    An LSTM-based regression model for predicting stock returns from TF-IDF features.\n",
        "\n",
        "    This model architecture is designed as specified in the research paper. It\n",
        "    treats the static TF-IDF vector for a given day as a sequence of length 1.\n",
        "    The architecture consists of a single LSTM layer followed by a dropout layer\n",
        "    and a final linear layer for regression output.\n",
        "\n",
        "    Attributes:\n",
        "        input_size (int): The dimensionality of the input features (e.g., 2000 for TF-IDF).\n",
        "        hidden_size (int): The number of features in the LSTM hidden state.\n",
        "        dropout_rate (float): The dropout probability.\n",
        "        lstm (nn.LSTM): The core LSTM layer.\n",
        "        dropout (nn.Dropout): The dropout layer for regularization.\n",
        "        regressor (nn.Linear): The final linear layer for outputting a single regression value.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_config: Dict[str, Any]):\n",
        "        \"\"\"\n",
        "        Initializes the LSTMRegressionModel with parameters from a configuration dictionary.\n",
        "\n",
        "        Args:\n",
        "            model_config: A dictionary containing the model's hyperparameters:\n",
        "                          'input_size', 'hidden_size', and 'dropout'.\n",
        "\n",
        "        Raises:\n",
        "            KeyError: If a required key is missing from the model_config.\n",
        "        \"\"\"\n",
        "        # Call the constructor of the parent class (nn.Module).\n",
        "        super().__init__()\n",
        "\n",
        "        # --- Parameter Extraction and Validation ---\n",
        "        try:\n",
        "            # The number of features in the input TF-IDF vector.\n",
        "            self.input_size: int = model_config['input_size']\n",
        "            # The dimensionality of the LSTM's hidden state.\n",
        "            self.hidden_size: int = model_config['hidden_size']\n",
        "            # The probability for the dropout layer.\n",
        "            self.dropout_rate: float = model_config['dropout']\n",
        "        except KeyError as e:\n",
        "            raise KeyError(f\"Missing required key in model_config: {e}\")\n",
        "\n",
        "        # --- Step 1: Define LSTM Network Components ---\n",
        "        # The core LSTM layer.\n",
        "        # `input_size`: The number of expected features in the input x.\n",
        "        # `hidden_size`: The number of features in the hidden state h.\n",
        "        # `num_layers`: Number of recurrent layers.\n",
        "        # `batch_first=True`: This crucial argument means the input and output\n",
        "        # tensors are provided as (batch, seq, feature), which is the standard\n",
        "        # for PyTorch data loaders.\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.input_size,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # A dropout layer for regularization, applied to the LSTM's output.\n",
        "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
        "\n",
        "        # The final linear layer (regression head).\n",
        "        # It maps the LSTM's hidden state to a single output value (the predicted return).\n",
        "        self.regressor = nn.Linear(\n",
        "            in_features=self.hidden_size,\n",
        "            out_features=1\n",
        "        )\n",
        "\n",
        "        # --- Step 3: Configure Weight Initialization ---\n",
        "        # Apply a custom weight initialization scheme for better training stability.\n",
        "        self._init_weights()\n",
        "\n",
        "        logging.info(\"LSTMRegressionModel initialized successfully.\")\n",
        "        logging.info(f\"  - Input Size: {self.input_size}\")\n",
        "        logging.info(f\"  - Hidden Size: {self.hidden_size}\")\n",
        "        logging.info(f\"  - Dropout Rate: {self.dropout_rate}\")\n",
        "\n",
        "    def _init_weights(self) -> None:\n",
        "        \"\"\"\n",
        "        Applies a custom weight initialization scheme to the model's layers.\n",
        "        \"\"\"\n",
        "        # Iterate through all modules (layers) in the network.\n",
        "        for module in self.modules():\n",
        "            # Check if the module is a Linear layer.\n",
        "            if isinstance(module, nn.Linear):\n",
        "                # Apply Xavier uniform initialization to the weights. This is a\n",
        "                # standard practice for layers with linear or no activation.\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                # Initialize the bias to zero.\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "            # Check if the module is an LSTM layer.\n",
        "            elif isinstance(module, nn.LSTM):\n",
        "                # Iterate through all named parameters of the LSTM.\n",
        "                for name, param in module.named_parameters():\n",
        "                    # Initialize weight matrices (e.g., 'weight_ih_l0').\n",
        "                    if 'weight' in name:\n",
        "                        nn.init.xavier_uniform_(param)\n",
        "                    # Initialize bias vectors (e.g., 'bias_hh_l0').\n",
        "                    elif 'bias' in name:\n",
        "                        nn.init.constant_(param, 0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            x: The input tensor of TF-IDF features.\n",
        "               Shape: (batch_size, input_size), e.g., (64, 2000).\n",
        "\n",
        "        Returns:\n",
        "            The output tensor of predicted returns.\n",
        "               Shape: (batch_size, 1), e.g., (64, 1).\n",
        "        \"\"\"\n",
        "        # --- Input Shape Validation ---\n",
        "        # A robust check to ensure the input data has the correct dimensions.\n",
        "        if x.dim() != 2 or x.shape[1] != self.input_size:\n",
        "            raise ValueError(\n",
        "                f\"Expected input of shape (batch_size, {self.input_size}), \"\n",
        "                f\"but got {x.shape}.\"\n",
        "            )\n",
        "\n",
        "        # --- Step 2: Implement LSTM Forward Pass Logic ---\n",
        "        # 1. Reshape the input for the LSTM layer.\n",
        "        # The LSTM expects a 3D tensor: (batch, seq_len, features).\n",
        "        # Since our TF-IDF vector is a static feature set for one time step,\n",
        "        # we treat it as a sequence of length 1.\n",
        "        # Shape change: (batch_size, 2000) -> (batch_size, 1, 2000).\n",
        "        x_reshaped = x.unsqueeze(1)\n",
        "\n",
        "        # 2. Pass the reshaped tensor through the LSTM.\n",
        "        # We don't need the final hidden/cell states `(h_n, c_n)` for this architecture.\n",
        "        # `lstm_out` will contain the hidden state for each element in the sequence.\n",
        "        # Shape of lstm_out: (batch_size, 1, hidden_size), e.g., (64, 1, 256).\n",
        "        lstm_out, _ = self.lstm(x_reshaped)\n",
        "\n",
        "        # 3. Extract the final hidden state.\n",
        "        # Since our sequence length is 1, we take the output from the first (and only) time step.\n",
        "        # The slice `[:, -1, :]` is a robust way to get the last time step's output.\n",
        "        # Shape change: (batch_size, 1, 256) -> (batch_size, 256).\n",
        "        last_hidden_state = lstm_out[:, -1, :]\n",
        "\n",
        "        # 4. Apply dropout for regularization.\n",
        "        # Shape remains: (batch_size, 256).\n",
        "        regularized_hidden_state = self.dropout(last_hidden_state)\n",
        "\n",
        "        # 5. Pass the result through the final linear regressor.\n",
        "        # Shape change: (batch_size, 256) -> (batch_size, 1).\n",
        "        prediction = self.regressor(regularized_hidden_state)\n",
        "\n",
        "        return prediction\n"
      ],
      "metadata": {
        "id": "EkalxfnU1g0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Text Transformer Architecture Specification\n",
        "\n",
        "class TextTransformerRegressionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A Transformer-based regression model for predicting stock returns from raw text.\n",
        "\n",
        "    This model leverages a pre-trained DistilBERT model as its core feature\n",
        "    extractor. It fine-tunes the entire transformer and adds a custom two-layer\n",
        "    MLP regression head on top of the [CLS] token representation to predict a\n",
        "    single continuous value.\n",
        "\n",
        "    Attributes:\n",
        "        base_model_identifier (str): The HuggingFace identifier for the base model.\n",
        "        dropout_rate (float): The dropout probability for the regression head.\n",
        "        distilbert (PreTrainedModel): The pre-trained DistilBERT base model.\n",
        "        pre_classifier (nn.Linear): The first linear layer of the regression head.\n",
        "        relu (nn.ReLU): The ReLU activation function.\n",
        "        dropout (nn.Dropout): The dropout layer for regularization.\n",
        "        regressor (nn.Linear): The final output linear layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_config: Dict[str, Any]):\n",
        "        \"\"\"\n",
        "        Initializes the TextTransformerRegressionModel.\n",
        "\n",
        "        Args:\n",
        "            model_config: A dictionary containing the model's hyperparameters:\n",
        "                          'base_model_identifier', 'hidden_size', and 'dropout'.\n",
        "\n",
        "        Raises:\n",
        "            KeyError: If a required key is missing from the model_config.\n",
        "            RuntimeError: If the pre-trained model cannot be loaded.\n",
        "        \"\"\"\n",
        "        # Call the constructor of the parent class (nn.Module).\n",
        "        super().__init__()\n",
        "\n",
        "        # --- Parameter Extraction and Validation ---\n",
        "        try:\n",
        "            # The HuggingFace identifier for the pre-trained model.\n",
        "            self.base_model_identifier: str = model_config['base_model_identifier']\n",
        "            # The dropout probability for the regression head.\n",
        "            self.dropout_rate: float = model_config['dropout']\n",
        "            # The hidden size of the transformer's output.\n",
        "            self.transformer_hidden_size: int = model_config['hidden_size']\n",
        "        except KeyError as e:\n",
        "            raise KeyError(f\"Missing required key in model_config: {e}\")\n",
        "\n",
        "        # --- Step 1: Define Transformer Network Components ---\n",
        "        try:\n",
        "            # Load the pre-trained DistilBERT model from HuggingFace.\n",
        "            # This downloads and caches the model weights and configuration.\n",
        "            self.distilbert: PreTrainedModel = DistilBertModel.from_pretrained(\n",
        "                self.base_model_identifier\n",
        "            )\n",
        "        except Exception as e:\n",
        "            # Provide a clear error if the model fails to load.\n",
        "            raise RuntimeError(\n",
        "                f\"Failed to load pre-trained model '{self.base_model_identifier}'. \"\n",
        "                f\"Check model name and network connection. Original error: {e}\"\n",
        "            )\n",
        "\n",
        "        # --- Define the custom regression head ---\n",
        "        # A two-layer MLP is a robust choice for mapping the complex text\n",
        "        # representation to a single regression output.\n",
        "\n",
        "        # First linear layer: maps DistilBERT output to an intermediate dimension.\n",
        "        self.pre_classifier = nn.Linear(self.transformer_hidden_size, 256)\n",
        "\n",
        "        # ReLU activation function.\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Dropout layer for regularization, applied to the [CLS] token representation.\n",
        "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
        "\n",
        "        # Final linear layer: maps the intermediate dimension to the single output value.\n",
        "        self.regressor = nn.Linear(256, 1)\n",
        "\n",
        "        # Note: For fine-tuning, we do not apply a custom weight initialization\n",
        "        # to the pre-trained distilbert part. We only initialize the new layers\n",
        "        # (the regression head).\n",
        "        self._init_head_weights()\n",
        "\n",
        "        logging.info(\"TextTransformerRegressionModel initialized successfully.\")\n",
        "        logging.info(f\"  - Base Model: {self.base_model_identifier}\")\n",
        "        logging.info(f\"  - Transformer Hidden Size: {self.transformer_hidden_size}\")\n",
        "        logging.info(f\"  - Dropout Rate: {self.dropout_rate}\")\n",
        "\n",
        "    def _init_head_weights(self) -> None:\n",
        "        \"\"\"\n",
        "        Initializes the weights of the custom regression head layers.\n",
        "        \"\"\"\n",
        "        # Apply Xavier uniform initialization to the first linear layer.\n",
        "        nn.init.xavier_uniform_(self.pre_classifier.weight)\n",
        "        # Initialize its bias to zero.\n",
        "        nn.init.constant_(self.pre_classifier.bias, 0)\n",
        "\n",
        "        # Apply Xavier uniform initialization to the final regressor layer.\n",
        "        nn.init.xavier_uniform_(self.regressor.weight)\n",
        "        # Initialize its bias to zero.\n",
        "        nn.init.constant_(self.regressor.bias, 0)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            input_ids: A tensor of token IDs.\n",
        "                       Shape: (batch_size, sequence_length).\n",
        "            attention_mask: A tensor of attention masks (1 for real tokens,\n",
        "                            0 for padding).\n",
        "                            Shape: (batch_size, sequence_length).\n",
        "\n",
        "        Returns:\n",
        "            The output tensor of predicted returns.\n",
        "               Shape: (batch_size, 1).\n",
        "        \"\"\"\n",
        "        # --- Step 3: Implement Transformer Forward Pass ---\n",
        "        # 1. Pass inputs through the base DistilBERT model.\n",
        "        # The attention_mask ensures that the model does not perform attention\n",
        "        # on padded tokens, which is critical for correctness.\n",
        "        distilbert_output = self.distilbert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        # 2. Extract the [CLS] token representation.\n",
        "        # The `last_hidden_state` contains the output embeddings for all tokens\n",
        "        # in the sequence. For classification/regression, we use the state of\n",
        "        # the first token, the [CLS] token.\n",
        "        # Shape of last_hidden_state: (batch_size, sequence_length, hidden_size).\n",
        "        # We slice `[:, 0, :]` to get the [CLS] token for each item in the batch.\n",
        "        # Shape of cls_token_state: (batch_size, hidden_size), e.g., (64, 768).\n",
        "        cls_token_state = distilbert_output.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # 3. Apply dropout to the [CLS] token representation for regularization.\n",
        "        # Shape remains: (batch_size, 768).\n",
        "        regularized_cls_token = self.dropout(cls_token_state)\n",
        "\n",
        "        # 4. Pass the representation through the custom regression head.\n",
        "        # Shape change: (batch_size, 768) -> (batch_size, 256).\n",
        "        intermediate_output = self.pre_classifier(regularized_cls_token)\n",
        "        # Apply ReLU activation.\n",
        "        intermediate_output = self.relu(intermediate_output)\n",
        "\n",
        "        # Shape change: (batch_size, 256) -> (batch_size, 1).\n",
        "        prediction = self.regressor(intermediate_output)\n",
        "\n",
        "        return prediction\n"
      ],
      "metadata": {
        "id": "-Q56uNGo2dWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12: Feature-Enhanced Transformer Architecture Specification\n",
        "\n",
        "class FeatureEnhancedMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    An MLP model for predicting returns from combined TF-IDF and embedding features.\n",
        "\n",
        "    This model, referred to as the \"Feature-based Transformer\" in the paper,\n",
        "    is implemented as a Multi-Layer Perceptron (MLP). It processes the static,\n",
        "    concatenated vector of sparse (TF-IDF) and dense (sentence embedding)\n",
        "    features through a simple but effective neural network architecture.\n",
        "\n",
        "    The architecture consists of:\n",
        "    1. A hidden layer mapping the combined features to a smaller dimension.\n",
        "    2. A ReLU activation function.\n",
        "    3. A dropout layer for regularization.\n",
        "    4. A final linear layer for regression output.\n",
        "\n",
        "    Attributes:\n",
        "        input_size (int): The dimensionality of the concatenated input features (e.g., 2384).\n",
        "        hidden_size (int): The number of neurons in the hidden layer.\n",
        "        dropout_rate (float): The dropout probability.\n",
        "        hidden_layer (nn.Linear): The first linear layer mapping input to hidden size.\n",
        "        activation (nn.ReLU): The non-linear activation function.\n",
        "        dropout (nn.Dropout): The dropout layer for regularization.\n",
        "        regressor (nn.Linear): The final output linear layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_config: Dict[str, Any]):\n",
        "        \"\"\"\n",
        "        Initializes the FeatureEnhancedMLP with parameters from a configuration dictionary.\n",
        "\n",
        "        Args:\n",
        "            model_config: A dictionary containing the model's hyperparameters:\n",
        "                          'input_size', 'hidden_size', and 'dropout'.\n",
        "\n",
        "        Raises:\n",
        "            KeyError: If a required key is missing from the model_config.\n",
        "        \"\"\"\n",
        "        # Call the constructor of the parent class (nn.Module).\n",
        "        super().__init__()\n",
        "\n",
        "        # --- Parameter Extraction and Validation ---\n",
        "        try:\n",
        "            # The number of features in the concatenated input vector.\n",
        "            self.input_size: int = model_config['input_size']\n",
        "            # The number of neurons in the hidden layer.\n",
        "            self.hidden_size: int = model_config['hidden_size']\n",
        "            # The probability for the dropout layer.\n",
        "            self.dropout_rate: float = model_config['dropout']\n",
        "        except KeyError as e:\n",
        "            raise KeyError(f\"Missing required key in model_config: {e}\")\n",
        "\n",
        "        # --- Step 1: Define MLP Architecture Components ---\n",
        "        # A sequential container for a clean, readable architecture definition.\n",
        "        self.network = nn.Sequential(\n",
        "            # First linear layer: maps the input vector to the hidden dimension.\n",
        "            nn.Linear(self.input_size, self.hidden_size),\n",
        "            # ReLU activation function to introduce non-linearity.\n",
        "            nn.ReLU(),\n",
        "            # Dropout layer for regularization to prevent overfitting.\n",
        "            nn.Dropout(self.dropout_rate),\n",
        "            # Final linear layer: maps the hidden representation to a single output value.\n",
        "            nn.Linear(self.hidden_size, 1)\n",
        "        )\n",
        "\n",
        "        # --- Step 3: Configure Weight Initialization ---\n",
        "        # Apply a custom weight initialization scheme.\n",
        "        self._init_weights()\n",
        "\n",
        "        logging.info(\"FeatureEnhancedMLP initialized successfully.\")\n",
        "        logging.info(f\"  - Input Size: {self.input_size}\")\n",
        "        logging.info(f\"  - Hidden Size: {self.hidden_size}\")\n",
        "        logging.info(f\"  - Dropout Rate: {self.dropout_rate}\")\n",
        "\n",
        "    def _init_weights(self) -> None:\n",
        "        \"\"\"\n",
        "        Applies a custom weight initialization scheme to the model's layers.\n",
        "        \"\"\"\n",
        "        # Iterate through all modules (layers) in the network.\n",
        "        for module in self.network.modules():\n",
        "            # Check if the module is a Linear layer.\n",
        "            if isinstance(module, nn.Linear):\n",
        "                # Use He (Kaiming) initialization for the weights of layers\n",
        "                # that are followed by a ReLU activation. This is theoretically\n",
        "                # sound as it preserves variance through the activation.\n",
        "                if module.out_features == self.hidden_size:\n",
        "                    nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')\n",
        "                # Use Xavier (Glorot) initialization for the final output layer,\n",
        "                # which has a linear activation.\n",
        "                else:\n",
        "                    nn.init.xavier_uniform_(module.weight)\n",
        "\n",
        "                # Initialize all biases to zero.\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            x: The input tensor of concatenated features.\n",
        "               Shape: (batch_size, input_size), e.g., (64, 2384).\n",
        "\n",
        "        Returns:\n",
        "            The output tensor of predicted returns.\n",
        "               Shape: (batch_size, 1), e.g., (64, 1).\n",
        "        \"\"\"\n",
        "        # --- Input Shape Validation ---\n",
        "        # A robust check to ensure the input data has the correct dimensions.\n",
        "        if x.dim() != 2 or x.shape[1] != self.input_size:\n",
        "            raise ValueError(\n",
        "                f\"Expected input of shape (batch_size, {self.input_size}), \"\n",
        "                f\"but got {x.shape}.\"\n",
        "            )\n",
        "\n",
        "        # --- Step 2: Implement MLP Forward Pass ---\n",
        "        # The forward pass is a simple, sequential execution of the defined network.\n",
        "        # Shape flow: (batch, 2384) -> (batch, 256) -> (batch, 1).\n",
        "        prediction = self.network(x)\n",
        "\n",
        "        return prediction\n"
      ],
      "metadata": {
        "id": "iIA7rwtQ3hZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Training Infrastructure Setup\n",
        "\n",
        "def setup_training_environment(seed: int = 42) -> torch.device:\n",
        "    \"\"\"\n",
        "    Configures the environment for reproducible and optimized PyTorch training.\n",
        "\n",
        "    This function performs two critical setup tasks:\n",
        "    1.  Sets random seeds for all relevant libraries (torch, numpy, random) to\n",
        "        ensure run-to-run reproducibility.\n",
        "    2.  Configures PyTorch to use deterministic algorithms and detects the optimal\n",
        "        compute device (GPU if available, otherwise CPU).\n",
        "\n",
        "    Args:\n",
        "        seed: The integer seed for all random number generators.\n",
        "\n",
        "    Returns:\n",
        "        The configured torch.device object ('cuda' or 'cpu').\n",
        "    \"\"\"\n",
        "    # --- Step 1: Set Seeds for Reproducibility ---\n",
        "    # Set seed for the `random` module.\n",
        "    random.seed(seed)\n",
        "    # Set seed for NumPy.\n",
        "    np.random.seed(seed)\n",
        "    # Set seed for PyTorch on both CPU and GPU.\n",
        "    torch.manual_seed(seed)\n",
        "    # Ensure reproducibility for CUDA operations.\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # --- Step 2: Configure for Deterministic Operations ---\n",
        "    # Using deterministic algorithms can have a performance impact but is crucial\n",
        "    # for reproducibility. `benchmark=False` prevents cuDNN from choosing\n",
        "    # non-deterministic algorithms.\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # --- Step 3: Select Compute Device ---\n",
        "    # Select GPU if available, otherwise fall back to CPU.\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    logging.info(f\"Training environment configured. Seed: {seed}. Device: {device}.\")\n",
        "    logging.info(f\"cuDNN deterministic: {torch.backends.cudnn.deterministic}, benchmark: {torch.backends.cudnn.benchmark}\")\n",
        "\n",
        "    return device\n",
        "\n",
        "\n",
        "class FinancialDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for standard numerical feature matrices.\n",
        "\n",
        "    This class serves as a robust interface between NumPy-based feature arrays\n",
        "    (e.g., TF-IDF, combined features) and the PyTorch DataLoader. It is designed\n",
        "    for models that expect a single feature tensor per sample, such as the LSTM\n",
        "    and MLP architectures in this study. The class handles the conversion of\n",
        "    NumPy arrays to PyTorch tensors with the appropriate data types.\n",
        "\n",
        "    Attributes:\n",
        "        features (torch.Tensor): A tensor holding all feature data.\n",
        "        targets (torch.Tensor): A tensor holding all target data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, features: np.ndarray, targets: np.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        Initializes the FinancialDataset.\n",
        "\n",
        "        Args:\n",
        "            features (np.ndarray): A 2D NumPy array of input features, where\n",
        "                                   each row is a sample.\n",
        "                                   Shape: (num_samples, num_features).\n",
        "            targets (np.ndarray): A 1D NumPy array of target values.\n",
        "                                  Shape: (num_samples,).\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the number of samples in features and targets\n",
        "                        do not match.\n",
        "            TypeError: If inputs are not NumPy arrays.\n",
        "        \"\"\"\n",
        "        # --- Input Type and Integrity Validation ---\n",
        "        # Ensure features are a NumPy array.\n",
        "        if not isinstance(features, np.ndarray):\n",
        "            raise TypeError(f\"Features must be a NumPy array, but got {type(features)}.\")\n",
        "\n",
        "        # Ensure targets are a NumPy array.\n",
        "        if not isinstance(targets, np.ndarray):\n",
        "            raise TypeError(f\"Targets must be a NumPy array, but got {type(targets)}.\")\n",
        "\n",
        "        # Ensure the number of samples is consistent between features and targets.\n",
        "        if features.shape[0] != targets.shape[0]:\n",
        "            raise ValueError(\n",
        "                f\"Inconsistent number of samples. Features have {features.shape[0]} \"\n",
        "                f\"samples, but targets have {targets.shape[0]} samples.\"\n",
        "            )\n",
        "\n",
        "        # --- Data Conversion and Storage ---\n",
        "        # Convert the NumPy feature array to a PyTorch tensor of type float32.\n",
        "        # float32 is the standard precision for features in most deep learning tasks.\n",
        "        self.features: torch.Tensor = torch.tensor(features, dtype=torch.float32)\n",
        "\n",
        "        # Convert the NumPy target array to a PyTorch tensor of type float32.\n",
        "        self.targets: torch.Tensor = torch.tensor(targets, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns the total number of samples in the dataset.\n",
        "\n",
        "        This method is required by the PyTorch Dataset API and is used by the\n",
        "        DataLoader to determine the size of the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: The total number of samples.\n",
        "        \"\"\"\n",
        "        # Return the number of rows in the features tensor.\n",
        "        return self.features.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Retrieves a single sample (features and target) from the dataset.\n",
        "\n",
        "        This method is required by the PyTorch Dataset API. It is called by the\n",
        "        DataLoader to fetch a single data point at the specified index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): The index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n",
        "                - The feature tensor for the sample. Shape: (num_features,).\n",
        "                - The target tensor for the sample. Shape: (1,). The target is\n",
        "                  unsqueezed to ensure a consistent shape for loss calculation.\n",
        "        \"\"\"\n",
        "        # Retrieve the feature tensor at the specified index.\n",
        "        feature_sample = self.features[idx]\n",
        "\n",
        "        # Retrieve the target tensor at the specified index.\n",
        "        target_sample = self.targets[idx]\n",
        "\n",
        "        # Unsqueeze the target tensor to add a dimension, changing its shape\n",
        "        # from a scalar-like tensor to a 1D tensor of shape (1,).\n",
        "        # This is best practice for regression tasks to align with model outputs\n",
        "        # that typically have shape (batch_size, 1).\n",
        "        return feature_sample, target_sample.unsqueeze(0)\n",
        "\n",
        "\n",
        "class TextTransformerDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for tokenized text data for Transformer models.\n",
        "\n",
        "    This class is specifically designed to work with the output of a HuggingFace\n",
        "    tokenizer. It handles dictionary-based inputs (e.g., 'input_ids',\n",
        "    'attention_mask') required by Transformer models like DistilBERT.\n",
        "\n",
        "    Attributes:\n",
        "        encodings (Dict[str, torch.Tensor]): A dictionary of tensors, where keys\n",
        "            are tokenizer-generated names (e.g., 'input_ids') and values are\n",
        "            the corresponding tensors for the entire dataset.\n",
        "        targets (torch.Tensor): A tensor holding all target data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encodings: Dict[str, np.ndarray], targets: np.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        Initializes the TextTransformerDataset.\n",
        "\n",
        "        Args:\n",
        "            encodings (Dict[str, np.ndarray]): A dictionary where keys are strings\n",
        "                (e.g., 'input_ids', 'attention_mask') and values are NumPy arrays.\n",
        "                All arrays must have the same first dimension (num_samples).\n",
        "            targets (np.ndarray): A 1D NumPy array of target values.\n",
        "                                  Shape: (num_samples,).\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the number of samples in encodings and targets\n",
        "                        do not match, or if 'input_ids' is missing.\n",
        "            TypeError: If inputs are not of the expected types.\n",
        "        \"\"\"\n",
        "        # --- Input Type and Integrity Validation ---\n",
        "        # Ensure encodings is a dictionary.\n",
        "        if not isinstance(encodings, dict):\n",
        "            raise TypeError(f\"Encodings must be a dictionary, but got {type(encodings)}.\")\n",
        "\n",
        "        # Ensure 'input_ids' is present, as it's the minimum requirement.\n",
        "        if 'input_ids' not in encodings:\n",
        "            raise ValueError(\"Encodings dictionary must contain the key 'input_ids'.\")\n",
        "\n",
        "        # Ensure targets is a NumPy array.\n",
        "        if not isinstance(targets, np.ndarray):\n",
        "            raise TypeError(f\"Targets must be a NumPy array, but got {type(targets)}.\")\n",
        "\n",
        "        # Get the number of samples from the 'input_ids' array.\n",
        "        num_samples = encodings['input_ids'].shape[0]\n",
        "\n",
        "        # Ensure the number of samples is consistent across all encoding arrays.\n",
        "        for key, value in encodings.items():\n",
        "            if not isinstance(value, np.ndarray):\n",
        "                raise TypeError(f\"Value for key '{key}' in encodings must be a NumPy array.\")\n",
        "            if value.shape[0] != num_samples:\n",
        "                raise ValueError(\n",
        "                    f\"Inconsistent number of samples in encodings. 'input_ids' has \"\n",
        "                    f\"{num_samples} samples, but '{key}' has {value.shape[0]}.\"\n",
        "                )\n",
        "\n",
        "        # Ensure the number of samples in targets matches.\n",
        "        if targets.shape[0] != num_samples:\n",
        "            raise ValueError(\n",
        "                f\"Inconsistent number of samples. Encodings have {num_samples} \"\n",
        "                f\"samples, but targets have {targets.shape[0]} samples.\"\n",
        "            )\n",
        "\n",
        "        # --- Data Conversion and Storage ---\n",
        "        # Convert each NumPy array in the encodings dictionary to a PyTorch tensor.\n",
        "        # The tensor type is inferred from the NumPy array (usually int64 for IDs).\n",
        "        self.encodings: Dict[str, torch.Tensor] = {\n",
        "            key: torch.tensor(val) for key, val in encodings.items()\n",
        "        }\n",
        "\n",
        "        # Convert the NumPy target array to a PyTorch tensor of type float32.\n",
        "        self.targets: torch.Tensor = torch.tensor(targets, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns the total number of samples in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: The total number of samples.\n",
        "        \"\"\"\n",
        "        # Return the number of rows in the targets tensor.\n",
        "        return self.targets.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[Dict[str, torch.Tensor], torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Retrieves a single sample (encoding dictionary and target) from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx (int): The index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[Dict[str, torch.Tensor], torch.Tensor]: A tuple containing:\n",
        "                - A dictionary where keys are 'input_ids', 'attention_mask', etc.,\n",
        "                  and values are the corresponding tensors for the single sample.\n",
        "                - The target tensor for the sample. Shape: (1,).\n",
        "        \"\"\"\n",
        "        # Create a dictionary for the single sample by slicing each tensor\n",
        "        # in the main encodings dictionary at the specified index.\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "\n",
        "        # Retrieve the target tensor at the specified index.\n",
        "        target = self.targets[idx]\n",
        "\n",
        "        # Unsqueeze the target tensor to shape (1,) for consistency.\n",
        "        return item, target.unsqueeze(0)\n",
        "\n",
        "\n",
        "def create_dataloaders(\n",
        "    features: Union[np.ndarray, Dict[str, np.ndarray]],\n",
        "    targets: np.ndarray,\n",
        "    model_type: str,\n",
        "    batch_size: int,\n",
        "    shuffle: bool\n",
        ") -> DataLoader:\n",
        "    \"\"\"\n",
        "    Factory function to create a PyTorch DataLoader for a given dataset.\n",
        "\n",
        "    This function selects the appropriate Dataset class based on the model type\n",
        "    and configures a DataLoader with specified parameters.\n",
        "\n",
        "    Args:\n",
        "        features: The input features (either a NumPy array or a dictionary of tokenized data).\n",
        "        targets: The corresponding target values.\n",
        "        model_type: The type of model ('lstm', 'feature_transformer', or 'text_transformer').\n",
        "        batch_size: The size of each batch.\n",
        "        shuffle: Whether to shuffle the data (True for training, False for val/test).\n",
        "\n",
        "    Returns:\n",
        "        A configured PyTorch DataLoader instance.\n",
        "    \"\"\"\n",
        "    # --- Select the appropriate Dataset class based on model type ---\n",
        "    if model_type in ['lstm', 'feature_transformer']:\n",
        "        # For LSTM and MLP, use the standard numerical dataset.\n",
        "        dataset = FinancialDataset(features, targets)\n",
        "    elif model_type == 'text_transformer':\n",
        "        # For the text transformer, use the specialized dictionary-based dataset.\n",
        "        dataset = TextTransformerDataset(features, targets)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
        "\n",
        "    # --- Configure and return the DataLoader ---\n",
        "    # `num_workers` > 0 enables multi-process data loading.\n",
        "    # `pin_memory=True` speeds up CPU-to-GPU data transfer.\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "\n",
        "def setup_optimization_components(\n",
        "    model: nn.Module,\n",
        "    train_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Configures the loss function, optimizer, and learning rate scheduler.\n",
        "\n",
        "    Args:\n",
        "        model: The PyTorch model whose parameters will be optimized.\n",
        "        train_config: The 'global_params' section of the study configuration.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the instantiated 'criterion' (loss function),\n",
        "        'optimizer', and 'scheduler'.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Configure Loss Function ---\n",
        "    # The paper specifies Mean Squared Error for this regression task.\n",
        "    # Equation: L_MSE = 1/N * sum((y_pred - y_true)^2)\n",
        "    if train_config['loss_function'] == 'MeanSquaredError':\n",
        "        criterion = nn.MSELoss()\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported loss function: {train_config['loss_function']}\")\n",
        "\n",
        "    # --- Step 2: Configure Optimizer ---\n",
        "    # The paper specifies the Adam optimizer.\n",
        "    if train_config['optimizer'] == 'Adam':\n",
        "        optimizer = optim.Adam(\n",
        "            model.parameters(),\n",
        "            lr=train_config['learning_rate']\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported optimizer: {train_config['optimizer']}\")\n",
        "\n",
        "    # --- Step 3: Configure Learning Rate Scheduler ---\n",
        "    # ReduceLROnPlateau adaptively reduces the learning rate when the validation\n",
        "    # loss stops improving, which is a robust strategy.\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',      # Reduce LR when the monitored quantity has stopped decreasing.\n",
        "        factor=0.5,      # Factor by which the learning rate will be reduced.\n",
        "        patience=3,      # Number of epochs with no improvement after which LR is reduced.\n",
        "        verbose=True     # Print a message when the learning rate is updated.\n",
        "    )\n",
        "\n",
        "    logging.info(\"Optimization components configured (MSE Loss, Adam Optimizer, ReduceLROnPlateau Scheduler).\")\n",
        "\n",
        "    return {'criterion': criterion, 'optimizer': optimizer, 'scheduler': scheduler}\n"
      ],
      "metadata": {
        "id": "SopuRZKw8jpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14: Model Training Loop Implementation\n",
        "\n",
        "def _handle_batch_input(\n",
        "    batch: Tuple[Union[torch.Tensor, Dict[str, torch.Tensor]], torch.Tensor],\n",
        "    device: torch.device\n",
        ") -> Tuple[Union[torch.Tensor, Dict[str, torch.Tensor]], torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Internal helper to move a batch of data to the correct device.\n",
        "    Handles both standard tensor and dictionary-based (for transformers) inputs.\n",
        "    \"\"\"\n",
        "    # Unpack the batch into features and targets.\n",
        "    features, targets = batch\n",
        "\n",
        "    # Move the targets tensor to the specified device.\n",
        "    targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "    # Check if the features are a dictionary (for transformer models).\n",
        "    if isinstance(features, dict):\n",
        "        # If so, move each tensor within the dictionary to the device.\n",
        "        features = {key: val.to(device, non_blocking=True) for key, val in features.items()}\n",
        "    else:\n",
        "        # Otherwise, move the single feature tensor to the device.\n",
        "        features = features.to(device, non_blocking=True)\n",
        "\n",
        "    return features, targets\n",
        "\n",
        "\n",
        "def train_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device,\n",
        "    scaler: GradScaler,\n",
        "    max_grad_norm: float = 1.0\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Executes a single training epoch for the given model.\n",
        "\n",
        "    This function iterates over the training dataloader, performs the forward\n",
        "    and backward passes, and updates the model weights. It incorporates best\n",
        "    practices such as gradient clipping and automatic mixed precision (AMP) for\n",
        "    stable and efficient training.\n",
        "\n",
        "    Args:\n",
        "        model: The PyTorch model to be trained.\n",
        "        dataloader: The DataLoader for the training set.\n",
        "        optimizer: The optimizer for updating model weights.\n",
        "        criterion: The loss function.\n",
        "        device: The device to perform computations on ('cuda' or 'cpu').\n",
        "        scaler: The GradScaler for mixed-precision training.\n",
        "        max_grad_norm: The maximum norm for gradient clipping.\n",
        "\n",
        "    Returns:\n",
        "        The average training loss for the epoch.\n",
        "    \"\"\"\n",
        "    # --- Set the model to training mode ---\n",
        "    # This enables layers like Dropout and BatchNorm to function correctly.\n",
        "    model.train()\n",
        "\n",
        "    # Initialize running loss for the epoch.\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # --- Iterate over the training data ---\n",
        "    # Use tqdm for a descriptive progress bar.\n",
        "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "        # Move the batch of data to the configured device.\n",
        "        features, targets = _handle_batch_input(batch, device)\n",
        "\n",
        "        # --- Core Training Steps ---\n",
        "        # 1. Zero the gradients from the previous iteration.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 2. Forward pass with Automatic Mixed Precision (AMP).\n",
        "        # `autocast` automatically casts operations to lower-precision dtypes (like float16)\n",
        "        # to speed up computation and reduce memory usage on compatible GPUs.\n",
        "        with autocast(enabled=(device.type == 'cuda')):\n",
        "            # Get model predictions.\n",
        "            if isinstance(features, dict):\n",
        "                predictions = model(**features)\n",
        "            else:\n",
        "                predictions = model(features)\n",
        "            # Calculate the loss.\n",
        "            loss = criterion(predictions, targets)\n",
        "\n",
        "        # 3. Backward pass with the GradScaler.\n",
        "        # `scaler.scale(loss)` multiplies the loss by a scaling factor to prevent\n",
        "        # gradients from underflowing (becoming zero) in mixed precision.\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # 4. Gradient Clipping.\n",
        "        # Unscales the gradients before clipping to view their true values.\n",
        "        scaler.unscale_(optimizer)\n",
        "        # Clips the norm of the gradients to prevent them from exploding, which\n",
        "        # can destabilize training.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        # 5. Optimizer step.\n",
        "        # `scaler.step` unscales the gradients and calls `optimizer.step()`.\n",
        "        # It skips the step if the gradients contain NaNs or Infs.\n",
        "        scaler.step(optimizer)\n",
        "\n",
        "        # 6. Update the GradScaler for the next iteration.\n",
        "        scaler.update()\n",
        "\n",
        "        # Accumulate the loss for the epoch.\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Calculate the average loss for the epoch.\n",
        "    avg_epoch_loss = running_loss / len(dataloader)\n",
        "    return avg_epoch_loss\n",
        "\n",
        "\n",
        "def validate_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Executes a single validation epoch for the given model.\n",
        "\n",
        "    This function iterates over the validation dataloader to compute the\n",
        "    model's loss on unseen data. It runs in inference mode, disabling gradients\n",
        "    for efficiency and correctness.\n",
        "\n",
        "    Args:\n",
        "        model: The PyTorch model to be validated.\n",
        "        dataloader: The DataLoader for the validation set.\n",
        "        criterion: The loss function.\n",
        "        device: The device to perform computations on ('cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        The average validation loss for the epoch.\n",
        "    \"\"\"\n",
        "    # --- Set the model to evaluation mode ---\n",
        "    # This is critical: it disables Dropout and sets other layers to inference mode.\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize running loss for the epoch.\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # --- Disable gradient computation ---\n",
        "    # This context manager reduces memory usage and speeds up computations, as\n",
        "    # no gradients need to be stored for the backward pass.\n",
        "    with torch.no_grad():\n",
        "        # --- Iterate over the validation data ---\n",
        "        for batch in tqdm(dataloader, desc=\"Validating\"):\n",
        "            # Move the batch of data to the configured device.\n",
        "            features, targets = _handle_batch_input(batch, device)\n",
        "\n",
        "            # Perform the forward pass.\n",
        "            if isinstance(features, dict):\n",
        "                predictions = model(**features)\n",
        "            else:\n",
        "                predictions = model(features)\n",
        "\n",
        "            # Calculate the loss.\n",
        "            loss = criterion(predictions, targets)\n",
        "\n",
        "            # Accumulate the loss.\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    # Calculate the average loss for the epoch.\n",
        "    avg_epoch_loss = running_loss / len(dataloader)\n",
        "    return avg_epoch_loss\n",
        "\n",
        "\n",
        "def run_training_orchestrator(\n",
        "    model: nn.Module,\n",
        "    optimization_components: Dict[str, Any],\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    device: torch.device,\n",
        "    num_epochs: int,\n",
        "    patience: int,\n",
        "    checkpoint_path: Union[str, Path]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete model training loop.\n",
        "\n",
        "    This function manages the entire training process over multiple epochs,\n",
        "    integrating training, validation, learning rate scheduling, early stopping,\n",
        "    and model checkpointing.\n",
        "\n",
        "    Args:\n",
        "        model: The PyTorch model to train.\n",
        "        optimization_components: A dictionary containing the optimizer, criterion, and scheduler.\n",
        "        train_loader: The DataLoader for the training set.\n",
        "        val_loader: The DataLoader for the validation set.\n",
        "        device: The compute device.\n",
        "        num_epochs: The maximum number of epochs to train for.\n",
        "        patience: The number of epochs to wait for validation loss improvement\n",
        "                  before stopping early.\n",
        "        checkpoint_path: The file path to save the best model weights.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the training history and the path to the best model.\n",
        "    \"\"\"\n",
        "    # --- Unpack optimization components ---\n",
        "    optimizer = optimization_components['optimizer']\n",
        "    criterion = optimization_components['criterion']\n",
        "    scheduler = optimization_components['scheduler']\n",
        "\n",
        "    # --- Initialize training state variables ---\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    # Initialize the GradScaler for mixed-precision training.\n",
        "    scaler = GradScaler(enabled=(device.type == 'cuda'))\n",
        "\n",
        "    # Ensure the directory for the checkpoint exists.\n",
        "    Path(checkpoint_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # --- Main Training Loop ---\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # --- Run Training and Validation for one epoch ---\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device, scaler)\n",
        "        val_loss = validate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "        end_time = time.time()\n",
        "        epoch_duration = end_time - start_time\n",
        "\n",
        "        logging.info(\n",
        "            f\"Epoch {epoch}/{num_epochs} | \"\n",
        "            f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "            f\"Duration: {epoch_duration:.2f}s\"\n",
        "        )\n",
        "\n",
        "        # --- Record history ---\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "        # --- Learning Rate Scheduling ---\n",
        "        # The scheduler adjusts the LR based on the validation loss.\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # --- Checkpointing and Early Stopping ---\n",
        "        if val_loss < best_val_loss:\n",
        "            # If validation loss has improved, save the model and reset counter.\n",
        "            logging.info(f\"Validation loss improved ({best_val_loss:.4f} -> {val_loss:.4f}). Saving model...\")\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            # If no improvement, increment the patience counter.\n",
        "            epochs_no_improve += 1\n",
        "            logging.info(f\"No improvement in validation loss for {epochs_no_improve} epoch(s).\")\n",
        "\n",
        "        # Check for early stopping.\n",
        "        if epochs_no_improve >= patience:\n",
        "            logging.info(f\"Early stopping triggered after {patience} epochs with no improvement.\")\n",
        "            break\n",
        "\n",
        "    logging.info(f\"Training finished. Best validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'history': history,\n",
        "        'best_model_path': checkpoint_path\n",
        "    }\n"
      ],
      "metadata": {
        "id": "7_X_21-M_CL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15: Regime-Specific Training Orchestration\n",
        "\n",
        "def _get_model_and_features_for_run(\n",
        "    model_type: str,\n",
        "    regime_name: str,\n",
        "    split_name: str,\n",
        "    study_config: Dict[str, Any],\n",
        "    data_splits: DataSplits,\n",
        "    tfidf_features: TfidfFeatures,\n",
        "    embedding_features: EmbeddingFeatures,\n",
        "    combined_features: CombinedFeatures,\n",
        "    tokenizer: PreTrainedTokenizerBase\n",
        ") -> Tuple[nn.Module, Union[np.ndarray, Dict[str, np.ndarray]], np.ndarray]:\n",
        "    \"\"\"\n",
        "    Internal helper to select the correct model class and feature set for a training run.\n",
        "    This acts as a factory for models and their corresponding data.\n",
        "    \"\"\"\n",
        "    # --- Select Model Class ---\n",
        "    model_classes = {\n",
        "        'lstm': LSTMRegressionModel,\n",
        "        'text_transformer': TextTransformerRegressionModel,\n",
        "        'feature_transformer': FeatureEnhancedMLP\n",
        "    }\n",
        "    if model_type not in model_classes:\n",
        "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
        "\n",
        "    # Instantiate the correct model with its specific configuration.\n",
        "    model_config = study_config['model_training']['architectures'][model_type]\n",
        "    model = model_classes[model_type](model_config)\n",
        "\n",
        "    # --- Select Corresponding Features and Targets ---\n",
        "    # Get the target values for the current split.\n",
        "    targets = data_splits[regime_name][split_name]['target_return'].values\n",
        "\n",
        "    # Select the feature set based on the model type.\n",
        "    if model_type == 'lstm':\n",
        "        # LSTM model uses TF-IDF features.\n",
        "        features = tfidf_features[regime_name][split_name].toarray()\n",
        "    elif model_type == 'feature_transformer':\n",
        "        # Feature-enhanced model uses the combined features.\n",
        "        features = combined_features[regime_name][split_name]\n",
        "    elif model_type == 'text_transformer':\n",
        "        # Text transformer requires tokenized text.\n",
        "        text_corpus = data_splits[regime_name][split_name]['aggregated_text'].tolist()\n",
        "        features = tokenizer(\n",
        "            text_corpus,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='np' # Return NumPy arrays for the Dataset class\n",
        "        )\n",
        "    else:\n",
        "        # This case is already handled, but included for logical completeness.\n",
        "        raise ValueError(f\"Feature selection logic not implemented for model_type: {model_type}\")\n",
        "\n",
        "    return model, features, targets\n",
        "\n",
        "\n",
        "def run_regime_specific_training_pipeline(\n",
        "    data_splits: DataSplits,\n",
        "    tfidf_features: TfidfFeatures,\n",
        "    embedding_features: EmbeddingFeatures,\n",
        "    combined_features: CombinedFeatures,\n",
        "    study_config: Dict[str, Any],\n",
        "    base_checkpoint_dir: str = \"checkpoints\",\n",
        "    force_retrain: bool = False\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire regime-specific training pipeline for all models.\n",
        "\n",
        "    This master function iterates through each regime and each model architecture\n",
        "    defined in the study configuration. For each of the 12 combinations, it:\n",
        "    1. Sets up a reproducible training environment.\n",
        "    2. Selects the correct model, features, and data.\n",
        "    3. Configures DataLoaders, optimizer, and loss function.\n",
        "    4. Executes the full training loop with validation, checkpointing, and early stopping.\n",
        "    5. Stores the results (path to best model and training history).\n",
        "\n",
        "    Args:\n",
        "        data_splits: The nested dictionary of data splits.\n",
        "        tfidf_features: The nested dictionary of TF-IDF features.\n",
        "        embedding_features: The nested dictionary of sentence embedding features.\n",
        "        combined_features: The nested dictionary of combined features.\n",
        "        study_config: The complete study configuration dictionary.\n",
        "        base_checkpoint_dir: The root directory to save model checkpoints.\n",
        "        force_retrain: If True, retrains models even if a checkpoint exists.\n",
        "\n",
        "    Returns:\n",
        "        A nested dictionary containing the results of all training runs.\n",
        "        Structure: {regime: {model_type: {'best_model_path': ..., 'history': ...}}}\n",
        "    \"\"\"\n",
        "    logging.info(\"====== Starting Full Regime-Specific Training Pipeline ======\")\n",
        "\n",
        "    # --- Initialize environment and results dictionary ---\n",
        "    # Setup a reproducible environment. The same seed is used for all runs for consistency.\n",
        "    device = setup_training_environment(seed=42)\n",
        "    # This dictionary will store the final results of all 12 training runs.\n",
        "    all_training_results = {regime: {} for regime in data_splits.keys()}\n",
        "\n",
        "    # --- Pre-load the tokenizer for the text transformer model ---\n",
        "    # This is done once to avoid repeated loading inside the loop.\n",
        "    tokenizer_id = study_config['model_training']['architectures']['text_transformer']['base_model_identifier']\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
        "\n",
        "    # --- Main Experimental Loop: Iterate through Regimes and Models ---\n",
        "    for regime_name in data_splits.keys():\n",
        "        for model_type in study_config['model_training']['architectures'].keys():\n",
        "\n",
        "            logging.info(f\"\\n--- Starting Training Run: [Regime: {regime_name}] - [Model: {model_type}] ---\")\n",
        "\n",
        "            # --- Step 1: Setup Checkpoint Path and Check for Existing Model ---\n",
        "            # Define a systematic path for saving the model checkpoint.\n",
        "            checkpoint_path = Path(base_checkpoint_dir) / regime_name / f\"{model_type}_best.pth\"\n",
        "\n",
        "            # Idempotency: Skip training if the model already exists and not forcing retrain.\n",
        "            if checkpoint_path.exists() and not force_retrain:\n",
        "                logging.info(f\"Checkpoint found at '{checkpoint_path}'. Skipping training.\")\n",
        "                # Store the existing path and a placeholder for history.\n",
        "                all_training_results[regime_name][model_type] = {\n",
        "                    'best_model_path': str(checkpoint_path),\n",
        "                    'history': 'Skipped; loaded from checkpoint.'\n",
        "                }\n",
        "                continue\n",
        "\n",
        "            # --- Step 2: Instantiate Model and Prepare Data ---\n",
        "            # Use the factory helper to get the correct model and features.\n",
        "            model, train_features, train_targets = _get_model_and_features_for_run(\n",
        "                model_type, regime_name, 'training', study_config, data_splits,\n",
        "                tfidf_features, embedding_features, combined_features, tokenizer\n",
        "            )\n",
        "            _, val_features, val_targets = _get_model_and_features_for_run(\n",
        "                model_type, regime_name, 'validation', study_config, data_splits,\n",
        "                tfidf_features, embedding_features, combined_features, tokenizer\n",
        "            )\n",
        "\n",
        "            # Move the model to the configured compute device.\n",
        "            model.to(device)\n",
        "\n",
        "            # --- Step 3: Create DataLoaders ---\n",
        "            # Configure DataLoaders for training and validation sets.\n",
        "            train_loader = create_dataloaders(\n",
        "                train_features, train_targets, model_type,\n",
        "                batch_size=study_config['model_training']['global_params']['batch_size'],\n",
        "                shuffle=True\n",
        "            )\n",
        "            val_loader = create_dataloaders(\n",
        "                val_features, val_targets, model_type,\n",
        "                batch_size=study_config['model_training']['global_params']['batch_size'],\n",
        "                shuffle=False\n",
        "            )\n",
        "\n",
        "            # --- Step 4: Setup Optimization Components ---\n",
        "            # Configure the loss function, optimizer, and scheduler for this model.\n",
        "            optimization_components = setup_optimization_components(\n",
        "                model, study_config['model_training']['global_params']\n",
        "            )\n",
        "\n",
        "            # --- Step 5: Execute the Training Orchestrator ---\n",
        "            # Delegate the actual training process to the function from Task 14.\n",
        "            training_result = run_training_orchestrator(\n",
        "                model=model,\n",
        "                optimization_components=optimization_components,\n",
        "                train_loader=train_loader,\n",
        "                val_loader=val_loader,\n",
        "                device=device,\n",
        "                num_epochs=50,  # A reasonable maximum number of epochs.\n",
        "                patience=5,     # Standard patience for early stopping.\n",
        "                checkpoint_path=checkpoint_path\n",
        "            )\n",
        "\n",
        "            # --- Step 6: Store Results ---\n",
        "            # Save the training history and the path to the best model.\n",
        "            all_training_results[regime_name][model_type] = training_result\n",
        "\n",
        "            logging.info(f\"--- Finished Training Run: [Regime: {regime_name}] - [Model: {model_type}] ---\")\n",
        "\n",
        "    logging.info(\"\\n====== Full Regime-Specific Training Pipeline Finished. ======\")\n",
        "    return all_training_results\n"
      ],
      "metadata": {
        "id": "iZr-30CVAuOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16: Model Loading and Inference Setup\n",
        "\n",
        "def generate_predictions_for_split(\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    device: torch.device\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generates predictions for a given dataset using a trained model.\n",
        "\n",
        "    This function runs a model in inference mode over a dataloader, collects\n",
        "    all batch predictions, and returns them as a single NumPy array.\n",
        "\n",
        "    Args:\n",
        "        model: The trained PyTorch model (already in eval mode).\n",
        "        dataloader: The DataLoader for the dataset to be predicted.\n",
        "        device: The compute device ('cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        A 1D NumPy array containing the model's predictions.\n",
        "    \"\"\"\n",
        "    # --- Set model to evaluation mode and disable gradients ---\n",
        "    # This is a redundant safety check; the orchestrator should already do this.\n",
        "    model.eval()\n",
        "\n",
        "    # List to store predictions from each batch.\n",
        "    all_predictions = []\n",
        "\n",
        "    # The `torch.no_grad()` context manager is critical for inference.\n",
        "    with torch.no_grad():\n",
        "        # Iterate over the data with a progress bar.\n",
        "        for batch in tqdm(dataloader, desc=\"Generating Predictions\"):\n",
        "            # Move the batch of data to the configured device.\n",
        "            features, _ = _handle_batch_input(batch, device)\n",
        "\n",
        "            # --- Forward Pass ---\n",
        "            # Get model predictions for the batch.\n",
        "            if isinstance(features, dict):\n",
        "                predictions = model(**features)\n",
        "            else:\n",
        "                predictions = model(features)\n",
        "\n",
        "            # --- Collect Predictions ---\n",
        "            # Move predictions to the CPU and convert to a NumPy array before appending.\n",
        "            # This frees up GPU memory.\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "\n",
        "    # Concatenate the list of batch predictions into a single NumPy array.\n",
        "    predictions_array = np.vstack(all_predictions)\n",
        "\n",
        "    # Flatten the array from (num_samples, 1) to (num_samples,) for easier use.\n",
        "    return predictions_array.flatten()\n",
        "\n",
        "\n",
        "def run_inference_pipeline(\n",
        "    training_results: Dict[str, Any],\n",
        "    data_splits: DataSplits,\n",
        "    tfidf_features: TfidfFeatures,\n",
        "    embedding_features: EmbeddingFeatures,\n",
        "    combined_features: CombinedFeatures,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire inference pipeline for all trained models.\n",
        "\n",
        "    This master function iterates through each trained model from the training\n",
        "    results, loads its checkpoint, generates predictions on its corresponding\n",
        "    test set, and collates all predictions into a single, comprehensive DataFrame.\n",
        "\n",
        "    Args:\n",
        "        training_results: The results dictionary from the training pipeline.\n",
        "        data_splits: The nested dictionary of data splits.\n",
        "        tfidf_features: The nested dictionary of TF-IDF features.\n",
        "        embedding_features: The nested dictionary of sentence embedding features.\n",
        "        combined_features: The nested dictionary of combined features.\n",
        "        study_config: The complete study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A single, tidy pandas DataFrame containing predictions and ground truth\n",
        "        for all model-regime test sets.\n",
        "    \"\"\"\n",
        "    logging.info(\"====== Starting Full Inference Pipeline ======\")\n",
        "\n",
        "    # --- Setup environment ---\n",
        "    # Device selection should be consistent with the training environment.\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Pre-load the tokenizer once.\n",
        "    tokenizer_id = study_config['model_training']['architectures']['text_transformer']['base_model_identifier']\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
        "\n",
        "    # List to store DataFrame chunks for each test set.\n",
        "    all_results_dfs = []\n",
        "\n",
        "    # --- Main Inference Loop: Iterate through Regimes and Models ---\n",
        "    for regime_name, model_runs in training_results.items():\n",
        "        for model_type, run_result in model_runs.items():\n",
        "\n",
        "            logging.info(f\"\\n--- Generating Predictions for: [Regime: {regime_name}] - [Model: {model_type}] ---\")\n",
        "\n",
        "            # --- Step 1: Load the Best Model from Checkpoint ---\n",
        "            checkpoint_path = run_result['best_model_path']\n",
        "            if not Path(checkpoint_path).exists():\n",
        "                logging.warning(f\"Checkpoint not found for {regime_name}-{model_type} at {checkpoint_path}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Instantiate a new model of the correct architecture.\n",
        "            model_config = study_config['model_training']['architectures'][model_type]\n",
        "            model_classes = {\n",
        "                'lstm': LSTMRegressionModel,\n",
        "                'text_transformer': TextTransformerRegressionModel,\n",
        "                'feature_transformer': FeatureEnhancedMLP\n",
        "            }\n",
        "            model = model_classes[model_type](model_config)\n",
        "\n",
        "            # Load the saved weights from the checkpoint file.\n",
        "            model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "\n",
        "            # Move the model to the compute device and set to evaluation mode.\n",
        "            model.to(device)\n",
        "            model.eval()\n",
        "\n",
        "            # --- Step 2: Prepare the Test Data ---\n",
        "            # Get the specific test DataFrame for this run.\n",
        "            test_df = data_splits[regime_name]['test']\n",
        "            if test_df.empty:\n",
        "                logging.warning(f\"Test set for {regime_name}-{model_type} is empty. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Use the factory helper to get the correct features and targets for the test set.\n",
        "            _, test_features, test_targets = _get_model_and_features_for_run(\n",
        "                model_type, regime_name, 'test', study_config, data_splits,\n",
        "                tfidf_features, embedding_features, combined_features, tokenizer\n",
        "            )\n",
        "\n",
        "            # Create the DataLoader for the test set. `shuffle=False` is critical.\n",
        "            test_loader = create_dataloaders(\n",
        "                test_features, test_targets, model_type,\n",
        "                batch_size=study_config['model_training']['global_params']['batch_size'] * 2, # Use larger batch for inference\n",
        "                shuffle=False\n",
        "            )\n",
        "\n",
        "            # --- Step 3: Generate and Store Predictions ---\n",
        "            # Generate predictions for the entire test set.\n",
        "            predictions = generate_predictions_for_split(model, test_loader, device)\n",
        "\n",
        "            # Create a temporary DataFrame from the test set to store results.\n",
        "            result_df = test_df.copy()\n",
        "            result_df['model_type'] = model_type\n",
        "            result_df['prediction'] = predictions\n",
        "            # The ground truth is already in the 'target_return' column.\n",
        "            result_df = result_df.rename(columns={'target_return': 'ground_truth'})\n",
        "\n",
        "            # Append this chunk to the master list.\n",
        "            all_results_dfs.append(result_df)\n",
        "\n",
        "            logging.info(f\"Generated {len(predictions)} predictions.\")\n",
        "\n",
        "    # --- Final Assembly ---\n",
        "    # Concatenate all the individual result DataFrames into one large DataFrame.\n",
        "    if not all_results_dfs:\n",
        "        logging.warning(\"No predictions were generated. Returning an empty DataFrame.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    final_predictions_df = pd.concat(all_results_dfs)\n",
        "\n",
        "    logging.info(\"\\n====== Full Inference Pipeline Finished. ======\")\n",
        "    logging.info(f\"Total predictions generated: {len(final_predictions_df)}\")\n",
        "\n",
        "    return final_predictions_df\n"
      ],
      "metadata": {
        "id": "DTMcexxDQh7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17: Mean Squared Error (MSE) Computation\n",
        "\n",
        "def _bootstrap_mse_ci(\n",
        "    group: pd.DataFrame,\n",
        "    n_bootstraps: int = 1000,\n",
        "    alpha: float = 0.05\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Internal helper to calculate bootstrap confidence intervals for MSE.\n",
        "    \"\"\"\n",
        "    # Initialize an array to store the MSE from each bootstrap sample.\n",
        "    bootstrap_mses = np.zeros(n_bootstraps)\n",
        "    # Get the number of samples in the group.\n",
        "    n_samples = len(group)\n",
        "\n",
        "    # Perform the bootstrapping.\n",
        "    for i in range(n_bootstraps):\n",
        "        # Create a bootstrap sample by sampling with replacement.\n",
        "        resample = group.sample(n=n_samples, replace=True)\n",
        "        # Calculate the squared error for the resampled data.\n",
        "        squared_errors = (resample['prediction'] - resample['ground_truth']) ** 2\n",
        "        # Calculate and store the MSE for this bootstrap sample.\n",
        "        bootstrap_mses[i] = squared_errors.mean()\n",
        "\n",
        "    # Calculate the lower and upper bounds of the confidence interval.\n",
        "    lower_bound = np.percentile(bootstrap_mses, (alpha / 2) * 100)\n",
        "    upper_bound = np.percentile(bootstrap_mses, (1 - alpha / 2) * 100)\n",
        "\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "\n",
        "def compute_and_validate_mse(\n",
        "    predictions_df: pd.DataFrame,\n",
        "    n_bootstraps: int = 1000\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Computes Mean Squared Error (MSE) and its confidence intervals.\n",
        "\n",
        "    This function calculates the MSE for each model-regime pair from the\n",
        "    provided predictions DataFrame. It also performs bootstrap resampling to\n",
        "    estimate a 95% confidence interval for each MSE value, providing a measure\n",
        "    of statistical uncertainty.\n",
        "\n",
        "    Args:\n",
        "        predictions_df: A tidy DataFrame containing predictions, ground truth,\n",
        "                        regime, and model_type.\n",
        "        n_bootstraps: The number of bootstrap samples to generate for\n",
        "                      confidence intervals.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A DataFrame of MSE values, pivoted for presentation.\n",
        "        - A DataFrame of confidence intervals (lower and upper bounds).\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    required_cols = {'prediction', 'ground_truth', 'regime', 'model_type'}\n",
        "    if not required_cols.issubset(predictions_df.columns):\n",
        "        raise ValueError(f\"Input DataFrame is missing required columns: {required_cols - set(predictions_df.columns)}\")\n",
        "\n",
        "    # --- Step 1: Calculate Squared Error ---\n",
        "    # This is the core calculation, performed once in a vectorized manner.\n",
        "    # Equation: SE_i = (prediction_i - ground_truth_i)^2\n",
        "    df = predictions_df.copy()\n",
        "    df['squared_error'] = (df['prediction'] - df['ground_truth']) ** 2\n",
        "\n",
        "    # --- Step 2: Calculate Mean Squared Error per Group ---\n",
        "    # Group by regime and model type and calculate the mean of the squared errors.\n",
        "    # Equation: MSE = 1/N * sum(SE_i) for each group.\n",
        "    mse_series = df.groupby(['regime', 'model_type'])['squared_error'].mean()\n",
        "\n",
        "    # Pivot the resulting series into the desired wide-format DataFrame.\n",
        "    mse_table = mse_series.unstack(level='model_type')\n",
        "\n",
        "    # --- Step 3: Calculate Bootstrap Confidence Intervals ---\n",
        "    logging.info(f\"Calculating bootstrap CIs with {n_bootstraps} samples...\")\n",
        "    # Group by regime and model type and apply the bootstrap helper function.\n",
        "    ci_series = df.groupby(['regime', 'model_type']).apply(\n",
        "        _bootstrap_mse_ci, n_bootstraps=n_bootstraps\n",
        "    )\n",
        "\n",
        "    # Format the confidence intervals into a readable DataFrame.\n",
        "    ci_df = ci_series.unstack(level='model_type')\n",
        "    for col in ci_df.columns:\n",
        "        ci_df[col] = ci_df[col].apply(lambda x: f\"[{x[0]:.2f}, {x[1]:.2f}]\")\n",
        "\n",
        "    return mse_table, ci_df\n",
        "\n",
        "\n",
        "def generate_mse_summary_table(\n",
        "    mse_table: pd.DataFrame\n",
        ") -> pd.io.formats.style.Styler:\n",
        "    \"\"\"\n",
        "    Formats the MSE table for publication-quality presentation.\n",
        "\n",
        "    This function takes the raw MSE table and applies styling to match the\n",
        "    format of Table 3 in the paper, including number formatting and\n",
        "    highlighting the best-performing model in each regime.\n",
        "\n",
        "    Args:\n",
        "        mse_table: The pivoted DataFrame of MSE values.\n",
        "\n",
        "    Returns:\n",
        "        A pandas Styler object, which can be rendered in environments like\n",
        "        Jupyter notebooks or exported to HTML/LaTeX.\n",
        "    \"\"\"\n",
        "    # --- Apply Styling using the pandas Style API ---\n",
        "    styled_table = mse_table.style \\\n",
        "        .set_caption(\"Table 3 (Reproduced): Mean Squared Error (MSE) of models across economic regimes\") \\\n",
        "        .format(\"{:.2f}\") \\\n",
        "        .highlight_min(axis=1, props='font-weight: bold;')\n",
        "\n",
        "    return styled_table\n",
        "\n",
        "\n",
        "def run_mse_evaluation_suite(\n",
        "    predictions_df: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the full MSE evaluation and reporting pipeline.\n",
        "\n",
        "    Args:\n",
        "        predictions_df: The tidy DataFrame of all model predictions.\n",
        "        study_config: The complete study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        The raw, unstyled DataFrame of MSE values for programmatic use.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Running Task 17: Mean Squared Error (MSE) Evaluation Suite ---\")\n",
        "\n",
        "    # --- Step 1 & 2: Compute MSE and Confidence Intervals ---\n",
        "    logging.info(\"\\nStep 1 & 2: Computing MSE and validating with bootstrap confidence intervals...\")\n",
        "    mse_table, ci_table = compute_and_validate_mse(predictions_df)\n",
        "\n",
        "    logging.info(\"\\nMean Squared Error (MSE) Table (raw values):\\n\" + mse_table.to_string(float_format='{:.4f}'.format))\n",
        "    logging.info(\"\\n95% Confidence Intervals for MSE:\\n\" + ci_table.to_string())\n",
        "\n",
        "    # --- Step 3: Generate Formatted Summary Table ---\n",
        "    logging.info(\"\\nStep 3: Generating publication-quality summary table...\")\n",
        "    styled_mse_table = generate_mse_summary_table(mse_table)\n",
        "\n",
        "    # In a script, you might save this to HTML. In a notebook, `display` is used.\n",
        "    # styled_mse_table.to_html(\"mse_summary_table.html\")\n",
        "    logging.info(\"Displaying formatted MSE summary table (equivalent to Table 3):\")\n",
        "    display(styled_mse_table)\n",
        "\n",
        "    logging.info(\"\\n>>> MSE evaluation suite completed successfully. <<<\")\n",
        "\n",
        "    # Return the raw data table for further programmatic use.\n",
        "    return mse_table\n"
      ],
      "metadata": {
        "id": "M99yUBTaSGLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18: Prediction Storage and Metadata Management\n",
        "\n",
        "def enrich_and_store_predictions(\n",
        "    predictions_df: pd.DataFrame,\n",
        "    export_path: Union[str, Path],\n",
        "    metadata: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Enriches a predictions DataFrame with diagnostic metrics and metadata,\n",
        "    and provides a robust method for storage and validation.\n",
        "\n",
        "    This function serves as the final step in the prediction generation pipeline.\n",
        "    It takes the raw predictions and transforms them into a comprehensive,\n",
        "    auditable artifact by:\n",
        "    1.  Calculating per-sample error metrics (Squared Error, Absolute Error).\n",
        "    2.  Calculating directional accuracy.\n",
        "    3.  Embedding user-provided metadata (e.g., experiment ID, timestamp)\n",
        "        into the DataFrame.\n",
        "    4.  Saving the enriched DataFrame to a file using a secure protocol.\n",
        "    5.  Creating a checksum file to validate the integrity of the stored artifact.\n",
        "\n",
        "    Args:\n",
        "        predictions_df: The tidy DataFrame of all model predictions from the\n",
        "                        inference pipeline.\n",
        "        export_path: The file path (e.g., 'results/predictions.pkl') where the\n",
        "                     enriched DataFrame will be saved.\n",
        "        metadata: A dictionary containing metadata to be added to the DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        The enriched pandas DataFrame with added diagnostic and metadata columns.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the input DataFrame is missing required columns.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Running Task 18: Prediction Storage and Metadata Management ---\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    required_cols = {'prediction', 'ground_truth'}\n",
        "    if not required_cols.issubset(predictions_df.columns):\n",
        "        raise ValueError(f\"Input DataFrame is missing required columns: {required_cols - set(predictions_df.columns)}\")\n",
        "\n",
        "    # --- Operate on a copy to ensure purity ---\n",
        "    enriched_df = predictions_df.copy()\n",
        "\n",
        "    # --- Step 1: Generate Prediction Quality Diagnostics ---\n",
        "    logging.info(\"Step 1: Generating per-sample prediction quality diagnostics...\")\n",
        "\n",
        "    # Calculate Squared Error (SE), the contribution of each sample to MSE.\n",
        "    # Equation: SE_i = (prediction_i - ground_truth_i)^2\n",
        "    enriched_df['squared_error'] = (enriched_df['prediction'] - enriched_df['ground_truth']) ** 2\n",
        "\n",
        "    # Calculate Absolute Error (AE), the contribution to MAE.\n",
        "    # Equation: AE_i = |prediction_i - ground_truth_i|\n",
        "    enriched_df['absolute_error'] = (enriched_df['prediction'] - enriched_df['ground_truth']).abs()\n",
        "\n",
        "    # Calculate Directional Accuracy.\n",
        "    # This is True if the signs of the prediction and ground truth match.\n",
        "    # np.sign correctly handles positive, negative, and zero values.\n",
        "    enriched_df['directional_accuracy'] = np.sign(enriched_df['prediction']) == np.sign(enriched_df['ground_truth'])\n",
        "\n",
        "    logging.info(\"Added 'squared_error', 'absolute_error', and 'directional_accuracy' columns.\")\n",
        "\n",
        "    # --- Step 2: Embed Metadata ---\n",
        "    logging.info(\"Step 2: Embedding metadata into the DataFrame...\")\n",
        "    # Add a timestamp for when the artifact was created.\n",
        "    enriched_df['artifact_timestamp_utc'] = datetime.utcnow()\n",
        "\n",
        "    # Add all user-provided metadata as new columns.\n",
        "    # This makes every prediction row self-describing and fully auditable.\n",
        "    for key, value in metadata.items():\n",
        "        enriched_df[key] = value\n",
        "\n",
        "    logging.info(f\"Added metadata columns: {list(metadata.keys()) + ['artifact_timestamp_utc']}\")\n",
        "\n",
        "    # --- Step 3: Create Prediction Export and Validation System ---\n",
        "    logging.info(f\"Step 3: Exporting enriched DataFrame to '{export_path}'...\")\n",
        "\n",
        "    # Ensure the parent directory exists.\n",
        "    export_path = Path(export_path)\n",
        "    export_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Save the DataFrame using pickle with a modern, efficient protocol.\n",
        "    # Pickle is chosen as it perfectly preserves dtypes and index structure.\n",
        "    with open(export_path, \"wb\") as f:\n",
        "        pickle.dump(enriched_df, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    # --- Create a checksum for data integrity validation ---\n",
        "    # Read the file back in binary mode to compute the hash.\n",
        "    with open(export_path, \"rb\") as f:\n",
        "        file_bytes = f.read()\n",
        "        # Compute the SHA256 hash of the file content.\n",
        "        checksum = hashlib.sha256(file_bytes).hexdigest()\n",
        "\n",
        "    # Define the path for the checksum file.\n",
        "    checksum_path = export_path.with_suffix(export_path.suffix + '.sha256')\n",
        "\n",
        "    # Write the checksum to its corresponding file.\n",
        "    with open(checksum_path, \"w\") as f:\n",
        "        f.write(checksum)\n",
        "\n",
        "    logging.info(f\"Successfully saved DataFrame. Integrity checksum saved to '{checksum_path}'.\")\n",
        "\n",
        "    logging.info(\"\\n>>> Prediction enrichment and storage suite completed successfully. <<<\")\n",
        "\n",
        "    return enriched_df\n",
        "\n",
        "\n",
        "def load_and_validate_predictions(\n",
        "    import_path: Union[str, Path]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Loads a prediction artifact and validates its integrity using a checksum.\n",
        "\n",
        "    Args:\n",
        "        import_path: The file path of the prediction artifact to load.\n",
        "\n",
        "    Returns:\n",
        "        The loaded and validated pandas DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the artifact or its checksum file is not found.\n",
        "        ValueError: If the checksum validation fails, indicating data corruption.\n",
        "    \"\"\"\n",
        "    import_path = Path(import_path)\n",
        "    checksum_path = import_path.with_suffix(import_path.suffix + '.sha256')\n",
        "\n",
        "    # --- Check for file existence ---\n",
        "    if not import_path.exists():\n",
        "        raise FileNotFoundError(f\"Prediction artifact not found at '{import_path}'\")\n",
        "    if not checksum_path.exists():\n",
        "        raise FileNotFoundError(f\"Checksum file not found at '{checksum_path}'\")\n",
        "\n",
        "    # --- Validate Integrity ---\n",
        "    # Read the expected checksum.\n",
        "    with open(checksum_path, \"r\") as f:\n",
        "        expected_checksum = f.read()\n",
        "\n",
        "    # Read the artifact file and compute its actual checksum.\n",
        "    with open(import_path, \"rb\") as f:\n",
        "        file_bytes = f.read()\n",
        "        actual_checksum = hashlib.sha256(file_bytes).hexdigest()\n",
        "\n",
        "    # Compare the checksums.\n",
        "    if expected_checksum != actual_checksum:\n",
        "        raise ValueError(\n",
        "            f\"Data integrity check failed for '{import_path}'. \"\n",
        "            \"The file may be corrupted.\"\n",
        "        )\n",
        "\n",
        "    logging.info(f\"Checksum validation passed for '{import_path}'.\")\n",
        "\n",
        "    # --- Load the DataFrame ---\n",
        "    # If validation passes, load the object from the pickle file.\n",
        "    with open(import_path, \"rb\") as f:\n",
        "        predictions_df = pickle.load(f)\n",
        "\n",
        "    logging.info(f\"Successfully loaded prediction artifact with shape {predictions_df.shape}.\")\n",
        "\n",
        "    return predictions_df\n"
      ],
      "metadata": {
        "id": "1GZM-9YFTRCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19: Diagnostic Metric Orchestrator Function Creation, Task 20: Financial Causal Attribution Score (FCAS) Implementation, Task 21: Patent Cliff Sensitivity (PCS) Implementation,\n",
        "# Task 22: Temporal Semantic Volatility (TSV) Computation, Task 23: NLI-based Logical Consistency Score (NLICS) Implementation\n",
        "\n",
        "def _extract_causal_polarity(\n",
        "    text: str,\n",
        "    positive_keywords: List[str],\n",
        "    negative_keywords: List[str]\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Internal helper to extract causal polarity from text via keyword counting.\n",
        "\n",
        "    This function performs a simple keyword match to determine the overall\n",
        "    sentiment polarity of a given text. It is case-insensitive and matches\n",
        "    whole words only.\n",
        "\n",
        "    Args:\n",
        "        text: The input string to analyze.\n",
        "        positive_keywords: A list of keywords indicating positive sentiment.\n",
        "        negative_keywords: A list of keywords indicating negative sentiment.\n",
        "\n",
        "    Returns:\n",
        "        An integer representing the polarity: 1 for positive, -1 for negative,\n",
        "        and 0 for neutral or ambiguous.\n",
        "    \"\"\"\n",
        "    # Ensure the input is a string.\n",
        "    if not isinstance(text, str):\n",
        "        return 0\n",
        "\n",
        "    # Count occurrences of positive keywords using a case-insensitive, whole-word regex search.\n",
        "    pos_count = sum(len(re.findall(r'\\b' + kw + r'\\b', text, re.IGNORECASE)) for kw in positive_keywords)\n",
        "\n",
        "    # Count occurrences of negative keywords.\n",
        "    neg_count = sum(len(re.findall(r'\\b' + kw + r'\\b', text, re.IGNORECASE)) for kw in negative_keywords)\n",
        "\n",
        "    # Determine the final polarity based on the counts.\n",
        "    if pos_count > neg_count:\n",
        "        return 1\n",
        "    elif neg_count > pos_count:\n",
        "        return -1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def compute_fcas(predictions_df: pd.DataFrame, **kwargs) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the Financial Causal Attribution Score (FCAS).\n",
        "\n",
        "    FCAS measures the alignment between a model's prediction direction (positive\n",
        "    or negative return) and the causal sentiment polarity implied by the input\n",
        "    text. A higher score indicates that the model's predictions are more\n",
        "    consistent with the simple causal cues in the news.\n",
        "\n",
        "    The calculation follows the equation from the paper:\n",
        "    FCAS = E[I(sign(prediction) == sign(causal_cue))]\n",
        "    where the expectation is taken over all samples with non-zero causal cues.\n",
        "\n",
        "    Args:\n",
        "        predictions_df: The enriched DataFrame containing predictions, ground\n",
        "                        truth, and the 'aggregated_text' for each sample.\n",
        "        **kwargs: Catches unused arguments from the orchestrator.\n",
        "\n",
        "    Returns:\n",
        "        A pandas Series containing the FCAS for each (regime, model_type) pair,\n",
        "        indexed by a MultiIndex.\n",
        "    \"\"\"\n",
        "    # Log the start of the computation.\n",
        "    logging.info(\"Computing Financial Causal Attribution Score (FCAS)...\")\n",
        "\n",
        "    # Work on a copy to avoid modifying the original DataFrame.\n",
        "    df = predictions_df.copy()\n",
        "\n",
        "    # Define comprehensive, financially relevant keywords for sentiment polarity.\n",
        "    positive_keywords = [\"growth\", \"surge\", \"increase\", \"rise\", \"gain\", \"profit\", \"beat\", \"strong\", \"positive\", \"expansion\", \"boost\", \"outperform\", \"record\", \"upbeat\"]\n",
        "    negative_keywords = [\"decline\", \"fall\", \"decrease\", \"drop\", \"loss\", \"miss\", \"weak\", \"negative\", \"contraction\", \"plunge\", \"crash\", \"underperform\", \"risk\", \"concern\"]\n",
        "\n",
        "    # Apply the helper function to each row to determine the causal polarity of the text.\n",
        "    df['causal_polarity'] = df['aggregated_text'].apply(\n",
        "        _extract_causal_polarity, args=(positive_keywords, negative_keywords)\n",
        "    )\n",
        "\n",
        "    # Filter out samples where the text was neutral or ambiguous (polarity = 0).\n",
        "    df_filtered = df[df['causal_polarity'] != 0].copy()\n",
        "\n",
        "    # Handle the case where no samples have a clear causal direction.\n",
        "    if df_filtered.empty:\n",
        "        logging.warning(\"No samples with clear causal polarity found for FCAS calculation.\")\n",
        "        return pd.Series(name='FCAS', dtype=float)\n",
        "\n",
        "    # Determine the alignment: True if the sign of the prediction matches the sign of the polarity.\n",
        "    df_filtered['alignment'] = (np.sign(df_filtered['prediction']) == np.sign(df_filtered['causal_polarity']))\n",
        "\n",
        "    # Calculate the final FCAS by taking the mean of the boolean 'alignment' series for each group.\n",
        "    fcas_series = df_filtered.groupby(['regime', 'model_type'])['alignment'].mean()\n",
        "\n",
        "    # Return the resulting Series.\n",
        "    return fcas_series\n",
        "\n",
        "\n",
        "def compute_tsv(embedding_features: Dict[str, Dict[str, np.ndarray]], data_splits: Dict[str, Dict[str, pd.DataFrame]], **kwargs) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the Temporal Semantic Volatility (TSV).\n",
        "\n",
        "    TSV quantifies the average semantic drift between consecutive news articles\n",
        "    within the test set of each regime. It is calculated as the mean Euclidean\n",
        "    distance between the sentence embeddings of chronologically adjacent texts.\n",
        "    A higher TSV indicates greater narrative change over time.\n",
        "\n",
        "    The calculation follows the equation from the paper:\n",
        "    TSV = (1/(N-1)) * sum(||embedding_{i+1} - embedding_i||_2)\n",
        "\n",
        "    Args:\n",
        "        embedding_features: A nested dictionary containing the sentence embedding\n",
        "                            NumPy arrays for all data splits.\n",
        "        data_splits: The nested dictionary of data splits, used to access the\n",
        "                     date index for chronological sorting.\n",
        "        **kwargs: Catches unused arguments from the orchestrator.\n",
        "\n",
        "    Returns:\n",
        "        A pandas Series containing the TSV for each regime, indexed by regime name.\n",
        "    \"\"\"\n",
        "    # Log the start of the computation.\n",
        "    logging.info(\"Computing Temporal Semantic Volatility (TSV)...\")\n",
        "\n",
        "    # Dictionary to store the final TSV score for each regime.\n",
        "    tsv_scores = {}\n",
        "\n",
        "    # Iterate through each regime defined in the data splits.\n",
        "    for regime_name in data_splits.keys():\n",
        "        # Get the test set DataFrame for the current regime.\n",
        "        test_df = data_splits[regime_name]['test']\n",
        "\n",
        "        # A minimum of two data points are required to compute a difference.\n",
        "        if len(test_df) < 2:\n",
        "            tsv_scores[regime_name] = np.nan\n",
        "            continue\n",
        "\n",
        "        # Retrieve the corresponding sentence embeddings for the test set.\n",
        "        test_embeddings = embedding_features[regime_name]['test']\n",
        "\n",
        "        # Create a temporary DataFrame to align embeddings with their dates.\n",
        "        temp_df = pd.DataFrame(test_embeddings, index=test_df.index)\n",
        "\n",
        "        # Sort this DataFrame chronologically by the 'date' level of the index.\n",
        "        temp_df = temp_df.sort_index(level='date')\n",
        "\n",
        "        # Extract the sorted embeddings as a NumPy array.\n",
        "        sorted_embeddings = temp_df.values\n",
        "\n",
        "        # Calculate the vector difference between each consecutive embedding.\n",
        "        # `np.diff` computes the difference between adjacent elements.\n",
        "        differences = np.diff(sorted_embeddings, axis=0)\n",
        "\n",
        "        # Calculate the Euclidean (L2) norm of each difference vector.\n",
        "        distances = np.linalg.norm(differences, axis=1)\n",
        "\n",
        "        # The TSV is the mean of these distances.\n",
        "        tsv_scores[regime_name] = distances.mean()\n",
        "\n",
        "    # Convert the dictionary of scores to a pandas Series and return it.\n",
        "    return pd.Series(tsv_scores, name='TSV')\n",
        "\n",
        "\n",
        "def _perturb_text(text: str, perturbation_map: Dict[str, str]) -> Union[str, None]:\n",
        "    \"\"\"\n",
        "    Internal helper to apply a single, first-occurrence keyword perturbation.\n",
        "\n",
        "    This function searches for the first keyword from the perturbation map in\n",
        "    the text and replaces it with its counterpart.\n",
        "\n",
        "    Args:\n",
        "        text: The input string to perturb.\n",
        "        perturbation_map: A dictionary mapping keywords to their replacements.\n",
        "\n",
        "    Returns:\n",
        "        The perturbed string, or None if no keywords were found to replace.\n",
        "    \"\"\"\n",
        "    # Ensure the input is a string.\n",
        "    if not isinstance(text, str):\n",
        "        return None\n",
        "\n",
        "    # Iterate through the keyword-replacement pairs.\n",
        "    for word, replacement in perturbation_map.items():\n",
        "        # Use a case-insensitive, whole-word regex pattern.\n",
        "        pattern = r'\\b' + re.escape(word) + r'\\b'\n",
        "        # Check if the keyword exists in the text.\n",
        "        if re.search(pattern, text, re.IGNORECASE):\n",
        "            # If found, perform a single replacement and return immediately.\n",
        "            return re.sub(pattern, replacement, text, count=1, flags=re.IGNORECASE)\n",
        "\n",
        "    # If the loop completes without finding any keywords, return None.\n",
        "    return None\n",
        "\n",
        "\n",
        "def compute_pcs(\n",
        "    predictions_df: pd.DataFrame,\n",
        "    training_results: Dict[str, Any],\n",
        "    data_splits: Dict[str, Dict[str, pd.DataFrame]],\n",
        "    study_config: Dict[str, Any],\n",
        "    vectorizer: TfidfVectorizer,\n",
        "    st_model: SentenceTransformer,\n",
        "    tokenizer: PreTrainedTokenizerBase,\n",
        "    **kwargs\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the Patent Cliff Sensitivity (PCS).\n",
        "\n",
        "    PCS measures a model's output sensitivity to small, controlled semantic\n",
        "    changes in the input text. It is calculated as the average absolute\n",
        "    difference between a model's original prediction and its prediction on a\n",
        "    semantically perturbed version of the input text.\n",
        "\n",
        "    The calculation follows the equation from the paper:\n",
        "    PCS = E[|f_theta(x) - f_theta(x_perturbed)|]\n",
        "\n",
        "    This is a computationally intensive process as it requires re-running\n",
        "    feature engineering and inference for many samples.\n",
        "\n",
        "    Args:\n",
        "        predictions_df: DataFrame with original predictions.\n",
        "        training_results: Dictionary with paths to trained model checkpoints.\n",
        "        data_splits: The original data splits to get the raw text.\n",
        "        study_config: The main study configuration dictionary.\n",
        "        vectorizer: The globally fitted TfidfVectorizer.\n",
        "        st_model: The initialized SentenceTransformer model.\n",
        "        tokenizer: The initialized HuggingFace tokenizer.\n",
        "        **kwargs: Catches unused arguments.\n",
        "\n",
        "    Returns:\n",
        "        A pandas Series of PCS scores for each (regime, model_type) pair.\n",
        "    \"\"\"\n",
        "    # Log the start of this intensive computation.\n",
        "    logging.info(\"Computing Patent Cliff Sensitivity (PCS)...\")\n",
        "\n",
        "    # Determine the compute device.\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Define the bidirectional map for semantic perturbations.\n",
        "    perturbation_map = {\n",
        "        \"growth\": \"decline\", \"increase\": \"decrease\", \"rise\": \"fall\", \"gain\": \"loss\",\n",
        "        \"profit\": \"loss\", \"beat\": \"miss\", \"strong\": \"weak\", \"positive\": \"negative\",\n",
        "        \"expansion\": \"contraction\", \"boost\": \"drop\", \"outperform\": \"underperform\",\n",
        "        \"decline\": \"growth\", \"decrease\": \"increase\", \"fall\": \"rise\",\n",
        "        \"miss\": \"beat\", \"weak\": \"strong\", \"negative\": \"positive\",\n",
        "        \"contraction\": \"expansion\", \"drop\": \"boost\", \"underperform\": \"outperform\"\n",
        "    }\n",
        "\n",
        "    # List to store the results (absolute differences) from all runs.\n",
        "    all_diffs = []\n",
        "\n",
        "    # --- Main Loop: Iterate through each of the 12 experimental cells ---\n",
        "    for regime_name, model_runs in tqdm(training_results.items(), desc=\"PCS Regimes\"):\n",
        "        for model_type, run_result in model_runs.items():\n",
        "\n",
        "            # --- 1. Load the specific model for this cell ---\n",
        "            checkpoint_path = run_result['best_model_path']\n",
        "            model_config = study_config['model_training']['architectures'][model_type]\n",
        "            model_classes = {'lstm': LSTMRegressionModel, 'text_transformer': TextTransformerRegressionModel, 'feature_transformer': FeatureEnhancedMLP}\n",
        "            model = model_classes[model_type](model_config)\n",
        "            model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "            model.to(device).eval()\n",
        "\n",
        "            # --- 2. Prepare data for this cell ---\n",
        "            test_df = data_splits[regime_name]['test']\n",
        "            if test_df.empty: continue\n",
        "\n",
        "            # Get the original predictions for this cell's test set.\n",
        "            original_preds = predictions_df[\n",
        "                (predictions_df['regime'] == regime_name) & (predictions_df['model_type'] == model_type)\n",
        "            ]\n",
        "\n",
        "            # --- 3. Perturb texts and identify valid samples ---\n",
        "            perturbed_texts = test_df['aggregated_text'].apply(lambda x: _perturb_text(x, perturbation_map))\n",
        "            valid_indices = perturbed_texts.notna()\n",
        "\n",
        "            if not valid_indices.any(): continue\n",
        "\n",
        "            perturbed_texts_list = perturbed_texts[valid_indices].tolist()\n",
        "\n",
        "            # --- 4. Re-run feature engineering and inference on perturbed data ---\n",
        "            with torch.no_grad():\n",
        "                # This block dynamically creates the correct feature set for the perturbed text.\n",
        "                if model_type == 'text_transformer':\n",
        "                    encodings = tokenizer(perturbed_texts_list, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
        "                    # Use a dummy target array for the Dataset.\n",
        "                    dataloader = DataLoader(TextTransformerDataset(encodings, np.zeros(len(perturbed_texts_list))), batch_size=128)\n",
        "                else:\n",
        "                    perturbed_tfidf = vectorizer.transform(perturbed_texts_list).toarray()\n",
        "                    perturbed_emb = st_model.encode(perturbed_texts_list, batch_size=128, show_progress_bar=False)\n",
        "                    if model_type == 'lstm':\n",
        "                        features = perturbed_tfidf\n",
        "                    else: # feature_transformer\n",
        "                        features = np.hstack([perturbed_tfidf, perturbed_emb])\n",
        "                    dataloader = DataLoader(FinancialDataset(features, np.zeros(len(features))), batch_size=128)\n",
        "\n",
        "                # --- 5. Generate predictions in batches ---\n",
        "                perturbed_predictions = []\n",
        "                for batch in dataloader:\n",
        "                    features_batch, _ = _handle_batch_input(batch, device)\n",
        "                    preds = model(**features_batch) if isinstance(features_batch, dict) else model(features_batch)\n",
        "                    perturbed_predictions.append(preds.cpu().numpy())\n",
        "\n",
        "            perturbed_predictions = np.vstack(perturbed_predictions).flatten()\n",
        "\n",
        "            # --- 6. Calculate and store absolute differences ---\n",
        "            diffs_df = original_preds.loc[valid_indices].copy()\n",
        "            diffs_df['perturbed_prediction'] = perturbed_predictions\n",
        "            diffs_df['abs_diff'] = (diffs_df['prediction'] - diffs_df['perturbed_prediction']).abs()\n",
        "            all_diffs.append(diffs_df[['regime', 'model_type', 'abs_diff']])\n",
        "\n",
        "    # --- 7. Aggregate final PCS scores ---\n",
        "    if not all_diffs:\n",
        "        return pd.Series(name='PCS', dtype=float)\n",
        "\n",
        "    final_diffs_df = pd.concat(all_diffs)\n",
        "    pcs_series = final_diffs_df.groupby(['regime', 'model_type'])['abs_diff'].mean()\n",
        "    return pcs_series\n",
        "\n",
        "\n",
        "async def _get_nlics_score_for_row_cached(\n",
        "    session: AsyncOpenAI,\n",
        "    row: pd.Series,\n",
        "    config: Dict[str, Any],\n",
        "    cache: Dict[str, float],\n",
        "    cache_path: Path\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Async helper to get NLICS score for a single row with retry and caching.\n",
        "\n",
        "    This function generates a unique key for the input, checks a cache,\n",
        "    and only makes an API call if the result is not already cached. It includes\n",
        "    robust error handling with exponential backoff for API calls.\n",
        "\n",
        "    Args:\n",
        "        session: The active AsyncOpenAI client session.\n",
        "        row: A row from the predictions DataFrame.\n",
        "        config: The NLICS configuration dictionary.\n",
        "        cache: The in-memory cache dictionary.\n",
        "        cache_path: The path to the persistent cache file.\n",
        "\n",
        "    Returns:\n",
        "        The calculated NLICS score (1.0, 0.5, or 0.0), or np.nan on failure.\n",
        "    \"\"\"\n",
        "    # Generate the prediction hypothesis string.\n",
        "    hypothesis = \"The stock price will increase\" if row['prediction'] > 0 else \"The stock price will decrease\"\n",
        "    # Truncate text to a reasonable length to manage token costs and context windows.\n",
        "    text_excerpt = row['aggregated_text'][:2000]\n",
        "\n",
        "    # Create a unique, deterministic hash key for this text-hypothesis pair.\n",
        "    cache_key = hashlib.sha256((text_excerpt + hypothesis).encode('utf-8')).hexdigest()\n",
        "\n",
        "    # First, check the in-memory cache for the result.\n",
        "    if cache_key in cache:\n",
        "        return cache[cache_key]\n",
        "\n",
        "    # If not in cache, format the prompt for the API call.\n",
        "    user_prompt = config['prompt_template']['user'].format(news_excerpt=text_excerpt, prediction_hypothesis=hypothesis)\n",
        "\n",
        "    # Implement a retry loop with exponential backoff for API call robustness.\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            # Make the asynchronous API call.\n",
        "            response = await session.chat.completions.create(\n",
        "                model=config['llm_model_identifier'],\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": config['prompt_template']['system']},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ],\n",
        "                **config['llm_settings']\n",
        "            )\n",
        "\n",
        "            # Rigorously parse the response to get the top token and its probability.\n",
        "            top_logprob = response.choices[0].logprobs.content[0]\n",
        "            token = top_logprob.token.strip()\n",
        "            confidence = np.exp(top_logprob.logprob)\n",
        "\n",
        "            # Apply the precise scoring rule from the paper.\n",
        "            score = 0.5  # Default to 'Uncertain'\n",
        "            if token == \"Yes\" and confidence > config['scoring_rules']['confidence_threshold']:\n",
        "                score = 1.0\n",
        "            elif token == \"No\":\n",
        "                score = 0.0\n",
        "\n",
        "            # Update the in-memory cache.\n",
        "            cache[cache_key] = score\n",
        "            # Append the new result to the persistent cache file for future runs.\n",
        "            with open(cache_path, 'a') as f:\n",
        "                f.write(json.dumps({'key': cache_key, 'score': score}) + '\\n')\n",
        "            return score\n",
        "        except RateLimitError as e:\n",
        "            # Handle rate limit errors by waiting for the suggested duration.\n",
        "            logging.warning(f\"Rate limit hit. Waiting for {e.retry_after or 2 ** attempt}s...\")\n",
        "            await asyncio.sleep(e.retry_after or 2 ** attempt)\n",
        "        except Exception as e:\n",
        "            # Handle other transient errors.\n",
        "            logging.warning(f\"API call failed on attempt {attempt + 1}: {e}\")\n",
        "            if attempt < 2: await asyncio.sleep(2 ** attempt)\n",
        "\n",
        "    # If all retries fail, return NaN.\n",
        "    return np.nan\n",
        "\n",
        "\n",
        "async def _compute_nlics_async_cached(\n",
        "    df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    cache_path: Path\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Asynchronous orchestrator for NLICS computation with persistent caching.\n",
        "\n",
        "    This function manages the high-throughput, asynchronous execution of NLI\n",
        "    evaluations using an external API (e.g., OpenAI). It is designed for\n",
        "    efficiency and robustness by:\n",
        "    1.  Loading a persistent, file-based cache to avoid re-processing and\n",
        "        re-incurring costs for previously seen text-hypothesis pairs.\n",
        "    2.  Creating an asynchronous API client.\n",
        "    3.  Generating a list of concurrent tasks, one for each row in the input\n",
        "        DataFrame.\n",
        "    4.  Executing these tasks in parallel using `asyncio.gather`, with a\n",
        "        progress bar for user feedback.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing the samples to be evaluated.\n",
        "                           Must include 'aggregated_text' and 'prediction' columns.\n",
        "        config (Dict[str, Any]): The NLICS configuration dictionary, containing\n",
        "                                 API keys, model identifiers, and prompt templates.\n",
        "        cache_path (Path): The file path to the JSONL cache file for reading\n",
        "                           and writing results.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A pandas Series containing the calculated NLICS score for each\n",
        "                   sample, indexed identically to the input DataFrame. Failed\n",
        "                   evaluations will be represented by `np.nan`.\n",
        "    \"\"\"\n",
        "    # --- 1. Load Existing Cache from Disk ---\n",
        "    # Initialize an in-memory dictionary to hold the cache for fast lookups.\n",
        "    cache: Dict[str, float] = {}\n",
        "\n",
        "    # Check if the cache file already exists.\n",
        "    if cache_path.exists():\n",
        "        # Open the cache file for reading.\n",
        "        with open(cache_path, 'r') as f:\n",
        "            # Iterate through each line in the JSONL file.\n",
        "            for line in f:\n",
        "                try:\n",
        "                    # Attempt to load the JSON object from the line.\n",
        "                    entry = json.loads(line)\n",
        "                    # Populate the in-memory cache with the key and score.\n",
        "                    cache[entry['key']] = entry['score']\n",
        "                except (json.JSONDecodeError, KeyError):\n",
        "                    # If a line is corrupted or malformed, log a warning and skip it.\n",
        "                    logging.warning(f\"Skipping corrupted or malformed line in cache file: {line.strip()}\")\n",
        "                    continue\n",
        "\n",
        "    # Log the number of entries successfully loaded from the cache.\n",
        "    logging.info(f\"Loaded {len(cache)} entries from NLICS cache at '{cache_path}'.\")\n",
        "\n",
        "    # --- 2. Initialize Asynchronous API Client ---\n",
        "    # Create an instance of the AsyncOpenAI client using the API key from the config.\n",
        "    client = AsyncOpenAI(api_key=config['api_key'])\n",
        "\n",
        "    # --- 3. Create Concurrent Tasks ---\n",
        "    # Create a list of coroutine tasks. Each task is a call to the helper\n",
        "    # function that will process one row of the DataFrame.\n",
        "    tasks = [\n",
        "        _get_nlics_score_for_row_cached(client, row, config, cache, cache_path)\n",
        "        for _, row in df.iterrows()\n",
        "    ]\n",
        "\n",
        "    # --- 4. Execute Tasks Concurrently ---\n",
        "    # `async_tqdm.gather` is a wrapper around `asyncio.gather` that provides a\n",
        "    # real-time progress bar, which is essential for monitoring long-running\n",
        "    # asynchronous operations. It runs all tasks in the `tasks` list concurrently.\n",
        "    scores = await async_tqdm.gather(*tasks, desc=\"Computing NLICS Scores\")\n",
        "\n",
        "    # --- 5. Return Results ---\n",
        "    # Convert the list of returned scores into a pandas Series.\n",
        "    # The index of the Series is explicitly set to the index of the input\n",
        "    # DataFrame to ensure perfect alignment.\n",
        "    return pd.Series(scores, index=df.index)\n",
        "\n",
        "def compute_nlics(predictions_df: pd.DataFrame, study_config: Dict[str, Any], **kwargs) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the NLI-based Logical Consistency Score (NLICS).\n",
        "\n",
        "    NLICS uses a Large Language Model (GPT-4) to assess if a model's prediction\n",
        "    is logically supported by the input news text. This implementation is highly\n",
        "    performant due to asynchronous API calls and robust due to a persistent,\n",
        "    file-based caching mechanism that prevents re-computation and re-incurring\n",
        "    API costs on subsequent runs.\n",
        "\n",
        "    Args:\n",
        "        predictions_df: The enriched DataFrame of predictions.\n",
        "        study_config: The main study configuration dictionary.\n",
        "        **kwargs: Catches unused arguments.\n",
        "\n",
        "    Returns:\n",
        "        A pandas Series of NLICS scores for each (regime, model_type) pair.\n",
        "    \"\"\"\n",
        "    # Log the start of the computation.\n",
        "    logging.info(\"Computing NLICS (with async and caching)...\")\n",
        "\n",
        "    # Get the specific configuration for the NLICS metric.\n",
        "    nlics_config = study_config['diagnostics']['nlics_metric']\n",
        "\n",
        "    # Define the path for the persistent cache and ensure its directory exists.\n",
        "    cache_path = Path(\"results/nlics_cache.jsonl\")\n",
        "    cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Run the main asynchronous logic.\n",
        "    scores = asyncio.run(_compute_nlics_async_cached(predictions_df, nlics_config, cache_path))\n",
        "\n",
        "    # Add the calculated scores to a copy of the DataFrame.\n",
        "    df = predictions_df.copy()\n",
        "    df['nlics_score'] = scores\n",
        "\n",
        "    # Calculate the final NLICS by taking the mean score for each group.\n",
        "    nlics_series = df.groupby(['regime', 'model_type'])['nlics_score'].mean()\n",
        "    return nlics_series\n",
        "\n",
        "\n",
        "def run_diagnostic_metrics_orchestrator(\n",
        "    predictions_df: pd.DataFrame,\n",
        "    study_config: Dict[str, Any],\n",
        "    data_splits: Dict[str, Dict[str, pd.DataFrame]],\n",
        "    embedding_features: Dict[str, Dict[str, pd.ndarray]],\n",
        "    training_results: Dict[str, Any],\n",
        "    vectorizer: TfidfVectorizer,\n",
        "    st_model: SentenceTransformer,\n",
        "    tokenizer: PreTrainedTokenizerBase,\n",
        "    max_workers: int = 4\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of all diagnostic metrics.\n",
        "\n",
        "    This master function manages the execution of the entire diagnostic suite,\n",
        "    including the computationally intensive PCS and the I/O-bound NLICS. It is\n",
        "    designed for robustness and performance, handling failures in individual\n",
        "    metric calculations gracefully and structuring the workflow logically.\n",
        "\n",
        "    The workflow is as follows:\n",
        "    1.  MSE is calculated directly and synchronously as it is fast.\n",
        "    2.  Simpler, CPU-bound metrics (FCAS, TSV) are run in parallel using a\n",
        "        ProcessPoolExecutor.\n",
        "    3.  The complex, computationally intensive PCS metric is run sequentially\n",
        "        as it is a major pipeline in itself.\n",
        "    4.  The I/O-bound, asynchronous NLICS metric is run sequentially to manage\n",
        "        its async event loop and caching mechanism.\n",
        "    5.  All results are aggregated into a final, comprehensive \"Robustness Profile\"\n",
        "        DataFrame.\n",
        "\n",
        "    Args:\n",
        "        predictions_df: The enriched DataFrame containing predictions, ground\n",
        "                        truth, and all necessary metadata.\n",
        "        study_config: The complete study configuration dictionary.\n",
        "        data_splits: The original nested dictionary of data splits.\n",
        "        embedding_features: The nested dictionary of sentence embedding features.\n",
        "        training_results: The dictionary containing paths to trained model checkpoints.\n",
        "        vectorizer: The globally fitted TfidfVectorizer.\n",
        "        st_model: The initialized SentenceTransformer model.\n",
        "        tokenizer: The initialized HuggingFace tokenizer.\n",
        "        max_workers: The maximum number of parallel processes to use for\n",
        "                     the simpler metric computations.\n",
        "\n",
        "    Returns:\n",
        "        A single DataFrame containing the complete Robustness Profile for all\n",
        "        model-regime pairs, with metrics as columns.\n",
        "    \"\"\"\n",
        "    # Log the start of the master orchestration.\n",
        "    logging.info(\"====== Starting Definitive Diagnostic Metrics Orchestrator ======\")\n",
        "\n",
        "    # --- 1. Synchronous MSE Calculation ---\n",
        "    # MSE is fast to compute directly from the 'squared_error' column.\n",
        "    logging.info(\"Computing MSE...\")\n",
        "    # The calculation is a simple groupby-mean operation.\n",
        "    mse_series = predictions_df.groupby(['regime', 'model_type'])['squared_error'].mean()\n",
        "    # Name the resulting series for later joining.\n",
        "    mse_series.name = 'MSE'\n",
        "\n",
        "    # --- 2. Parallel Computation of Simpler Metrics (FCAS, TSV) ---\n",
        "    # These metrics are CPU-bound and independent, making them ideal for parallelization.\n",
        "    simple_metrics_to_run: Dict[str, Callable] = {\n",
        "        'FCAS': compute_fcas,\n",
        "        'TSV': compute_tsv\n",
        "    }\n",
        "    # This dictionary bundles all necessary artifacts to pass to each function.\n",
        "    shared_args = {\n",
        "        'predictions_df': predictions_df,\n",
        "        'data_splits': data_splits,\n",
        "        'embedding_features': embedding_features\n",
        "    }\n",
        "\n",
        "    # List to hold the resulting pandas Series from the parallel jobs.\n",
        "    parallel_metric_results = []\n",
        "    # Use a ProcessPoolExecutor for true parallelism on CPU-bound tasks.\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Submit each function call to the executor pool.\n",
        "        future_to_metric = {\n",
        "            executor.submit(func, **shared_args): name\n",
        "            for name, func in simple_metrics_to_run.items()\n",
        "        }\n",
        "\n",
        "        # Process results as they complete, with a progress bar.\n",
        "        for future in tqdm(as_completed(future_to_metric), total=len(future_to_metric), desc=\"Computing FCAS & TSV\"):\n",
        "            metric_name = future_to_metric[future]\n",
        "            try:\n",
        "                # Retrieve the result from the completed job.\n",
        "                result_series = future.result()\n",
        "                # Assign the correct name to the series.\n",
        "                result_series.name = metric_name\n",
        "                parallel_metric_results.append(result_series)\n",
        "                logging.info(f\"Successfully computed metric: {metric_name}\")\n",
        "            except Exception as e:\n",
        "                # Log any exceptions gracefully without crashing the orchestrator.\n",
        "                logging.error(f\"Metric calculation for '{metric_name}' FAILED: {e}\", exc_info=True)\n",
        "\n",
        "    # --- 3. Sequential Computation of Complex Metrics (PCS, NLICS) ---\n",
        "    # These are run sequentially because they are either extremely complex pipelines\n",
        "    # (PCS) or manage their own concurrency (NLICS).\n",
        "\n",
        "    # Compute PCS.\n",
        "    try:\n",
        "        pcs_series = compute_pcs(\n",
        "            predictions_df=predictions_df,\n",
        "            training_results=training_results,\n",
        "            data_splits=data_splits,\n",
        "            study_config=study_config,\n",
        "            vectorizer=vectorizer,\n",
        "            st_model=st_model,\n",
        "            tokenizer=tokenizer\n",
        "        )\n",
        "        pcs_series.name = 'PCS'\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Metric 'PCS' failed entirely: {e}\", exc_info=True)\n",
        "        pcs_series = None\n",
        "\n",
        "    # Compute NLICS.\n",
        "    try:\n",
        "        nlics_series = compute_nlics(\n",
        "            predictions_df=predictions_df,\n",
        "            study_config=study_config\n",
        "        )\n",
        "        nlics_series.name = 'NLICS'\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Metric 'NLICS' failed entirely: {e}\", exc_info=True)\n",
        "        nlics_series = None\n",
        "\n",
        "    # --- 4. Aggregate All Results into a Single DataFrame ---\n",
        "    logging.info(\"Aggregating all metric results into a final Robustness Profile table...\")\n",
        "\n",
        "    # Start with the MSE results as the base DataFrame.\n",
        "    robustness_profile = mse_series.to_frame()\n",
        "\n",
        "    # Join the results from the parallel computations.\n",
        "    for result_series in parallel_metric_results:\n",
        "        # Special handling for TSV, which is indexed only by 'regime'.\n",
        "        if result_series.name == 'TSV':\n",
        "            robustness_profile = robustness_profile.join(result_series, on='regime')\n",
        "        else:\n",
        "            # Join other metrics on the full (regime, model_type) index.\n",
        "            robustness_profile = robustness_profile.join(result_series, how='outer')\n",
        "\n",
        "    # Join the results from the sequential computations if they were successful.\n",
        "    if pcs_series is not None:\n",
        "        robustness_profile = robustness_profile.join(pcs_series)\n",
        "    if nlics_series is not None:\n",
        "        robustness_profile = robustness_profile.join(nlics_series)\n",
        "\n",
        "    # Sort the index for a clean, canonical ordering.\n",
        "    robustness_profile = robustness_profile.sort_index()\n",
        "\n",
        "    # Log the final, completed table.\n",
        "    logging.info(\"\\n====== Diagnostic Metrics Orchestration Finished. ======\")\n",
        "    logging.info(\"Final Robustness Profile:\\n\" + robustness_profile.to_string(float_format='{:.4f}'.format))\n",
        "\n",
        "    # Return the final DataFrame.\n",
        "    return robustness_profile\n"
      ],
      "metadata": {
        "id": "NpnV1leLUpTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 24: Diagnostic Metric Validation and Quality Assurance\n",
        "\n",
        "def run_diagnostic_validation_suite(\n",
        "    robustness_profile: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.io.formats.style.Styler]:\n",
        "    \"\"\"\n",
        "    Performs a final quality assurance audit on the computed diagnostic metrics.\n",
        "\n",
        "    This function takes the aggregated robustness profile, validates its\n",
        "    completeness and the integrity of its values, computes a diagnostic\n",
        "    correlation matrix, and generates a publication-quality summary table.\n",
        "\n",
        "    Args:\n",
        "        robustness_profile: The DataFrame containing the complete set of\n",
        "                            computed diagnostic metrics.\n",
        "        study_config: The complete study configuration dictionary, used to\n",
        "                      verify all expected regimes and models are present.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - The validated (and potentially cleaned) robustness profile DataFrame.\n",
        "        - A pandas Styler object representing the formatted, publication-quality\n",
        "          summary report.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If a critical validation check (e.g., a metric value\n",
        "                    being outside its theoretical range) fails.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Running Task 24: Diagnostic Metric Validation and Quality Assurance ---\")\n",
        "\n",
        "    # --- Operate on a copy ---\n",
        "    profile_df = robustness_profile.copy()\n",
        "\n",
        "    # Initialize a list to collect critical validation errors.\n",
        "    errors = []\n",
        "\n",
        "    # --- Step 1: Cross-Metric Consistency and Completeness Validation ---\n",
        "    logging.info(\"Step 1: Validating completeness and numerical integrity...\")\n",
        "\n",
        "    # Define expected rows (index) and columns.\n",
        "    expected_regimes = [r['regime_name'] for r in study_config['experimental_design']['regime_definitions']]\n",
        "    expected_models = list(study_config['model_training']['architectures'].keys())\n",
        "    expected_index = pd.MultiIndex.from_product([expected_regimes, expected_models], names=['regime', 'model_type'])\n",
        "    expected_columns = ['MSE', 'FCAS', 'PCS', 'TSV', 'NLICS']\n",
        "\n",
        "    # Check for missing rows.\n",
        "    missing_rows = expected_index.difference(profile_df.index)\n",
        "    if not missing_rows.empty:\n",
        "        logging.warning(f\"Robustness profile is missing results for the following (regime, model) pairs: {missing_rows.tolist()}\")\n",
        "\n",
        "    # Check for missing columns.\n",
        "    for col in expected_columns:\n",
        "        if col not in profile_df.columns:\n",
        "            errors.append(f\"Robustness profile is missing the required metric column: '{col}'\")\n",
        "            # Add a placeholder column with NaNs to prevent downstream errors.\n",
        "            profile_df[col] = np.nan\n",
        "\n",
        "    # Check for unexpected NaN values, which indicate a failed computation.\n",
        "    if profile_df.isnull().values.any():\n",
        "        nan_report = profile_df.isnull().sum()\n",
        "        nan_report = nan_report[nan_report > 0]\n",
        "        logging.warning(f\"Found NaN values, indicating failed metric computations:\\n{nan_report}\")\n",
        "\n",
        "    # --- Define theoretical ranges for each metric ---\n",
        "    metric_ranges = {\n",
        "        'MSE': (0, float('inf')),\n",
        "        'FCAS': (0, 1),\n",
        "        'PCS': (0, float('inf')),\n",
        "        'TSV': (0, 2),  # Max L2 distance between two unit vectors is 2.\n",
        "        'NLICS': (0, 1),\n",
        "    }\n",
        "\n",
        "    # Validate that all computed values fall within their theoretical ranges.\n",
        "    for metric, (min_val, max_val) in metric_ranges.items():\n",
        "        if metric in profile_df.columns:\n",
        "            # Drop NaNs for validation, as they were already reported.\n",
        "            values = profile_df[metric].dropna()\n",
        "            if not values.empty:\n",
        "                if not (values >= min_val).all() or not (values <= max_val).all():\n",
        "                    errors.append(\n",
        "                        f\"Metric '{metric}' has values outside its theoretical range \"\n",
        "                        f\"of [{min_val}, {max_val}].\"\n",
        "                    )\n",
        "\n",
        "    # If any critical errors were found, raise an exception.\n",
        "    if errors:\n",
        "        error_report = \"\\n\".join([f\"- {error}\" for error in errors])\n",
        "        raise ValueError(f\"Diagnostic metric validation failed:\\n{error_report}\")\n",
        "\n",
        "    logging.info(\"Completeness and numerical integrity validation PASSED.\")\n",
        "\n",
        "    # --- Step 2: Assemble Robustness Profile and Compute Diagnostics ---\n",
        "    # The profile is already assembled, but we can compute a correlation matrix as a diagnostic.\n",
        "    logging.info(\"\\nStep 2: Computing diagnostic correlation matrix of metrics...\")\n",
        "    metric_correlations = profile_df.corr()\n",
        "    logging.info(\"Metric Correlation Matrix:\\n\" + metric_correlations.to_string(float_format='{:.3f}'.format))\n",
        "\n",
        "    # --- Step 3: Generate Diagnostic Metric Summary Report ---\n",
        "    logging.info(\"\\nStep 3: Generating publication-quality summary report...\")\n",
        "\n",
        "    # Use the pandas Styler API for professional formatting.\n",
        "    styled_profile = profile_df.style \\\n",
        "        .set_caption(\"Robustness Profile: Diagnostic Metrics Across Regimes and Models\") \\\n",
        "        .format(\"{:.3f}\", na_rep=\"N/A\") \\\n",
        "        .set_table_styles([\n",
        "            {'selector': 'th', 'props': [('text-align', 'center')]},\n",
        "            {'selector': 'td', 'props': [('text-align', 'center')]},\n",
        "        ]) \\\n",
        "        .background_gradient(cmap='viridis', subset=['MSE', 'PCS', 'TSV']) \\\n",
        "        .background_gradient(cmap='viridis_r', subset=['FCAS', 'NLICS'])\n",
        "\n",
        "    # Display the styled table in a notebook environment.\n",
        "    display(styled_profile)\n",
        "\n",
        "    logging.info(\"\\n>>> Diagnostic metric validation and reporting suite completed successfully. <<<\")\n",
        "\n",
        "    # Return the validated DataFrame and the Styler object.\n",
        "    return profile_df, styled_profile\n"
      ],
      "metadata": {
        "id": "yp5GaXw-imr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 25: Jensen-Shannon Divergence Computation\n",
        "\n",
        "def compute_js_divergence_matrix(\n",
        "    data_splits: Dict[str, Dict[str, pd.DataFrame]],\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the pairwise Jensen-Shannon (J-S) Divergence between all regimes.\n",
        "\n",
        "    This function quantifies the semantic shift between different macroeconomic\n",
        "    regimes by treating their entire text corpora as documents and calculating\n",
        "    the J-S Divergence between their vocabulary distributions.\n",
        "\n",
        "    The process is as follows:\n",
        "    1.  A unified vocabulary is created from the text of all regimes combined.\n",
        "    2.  For each regime, its text corpus is transformed into a term frequency\n",
        "        vector based on the unified vocabulary.\n",
        "    3.  These vectors are averaged and normalized to create a probability\n",
        "        distribution for each regime.\n",
        "    4.  The pairwise J-S Divergence is calculated for all regime pairs.\n",
        "\n",
        "    Args:\n",
        "        data_splits: The nested dictionary of data splits, used to access the\n",
        "                     raw text for all samples in each regime.\n",
        "        study_config: The complete study configuration dictionary, used to get\n",
        "                      TF-IDF parameters.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame representing the symmetric J-S Divergence matrix,\n",
        "        where both the index and columns are the regime names.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Running Task 25: Jensen-Shannon Divergence Computation ---\")\n",
        "\n",
        "    # --- Step 1: Extract Corpora and Create Unified Vocabulary ---\n",
        "    logging.info(\"Step 1: Creating a unified vocabulary across all regimes...\")\n",
        "\n",
        "    # Get TF-IDF parameters from the config.\n",
        "    tfidf_params = study_config['feature_engineering']['tfidf']\n",
        "\n",
        "    # Assemble the full text corpus from all splits and all regimes.\n",
        "    all_texts = []\n",
        "    regime_corpora = {}\n",
        "    regime_names = sorted(data_splits.keys()) # Sort for deterministic order.\n",
        "\n",
        "    for regime_name in regime_names:\n",
        "        # Concatenate text from train, val, and test splits for the current regime.\n",
        "        regime_corpus_series = pd.concat([\n",
        "            splits_df['aggregated_text']\n",
        "            for split_name, splits_df in data_splits[regime_name].items()\n",
        "        ])\n",
        "        all_texts.append(regime_corpus_series)\n",
        "        regime_corpora[regime_name] = regime_corpus_series\n",
        "\n",
        "    global_corpus = pd.concat(all_texts)\n",
        "\n",
        "    # Create a vectorizer to define the common feature space (vocabulary).\n",
        "    # We use CountVectorizer as we only need term frequencies for this task.\n",
        "    vocab_vectorizer = TfidfVectorizer(\n",
        "        max_features=tfidf_params.get('max_features', 2000),\n",
        "        ngram_range=tfidf_params.get('ngram_range', (1, 2)),\n",
        "        min_df=tfidf_params.get('min_df', 2),\n",
        "        max_df=tfidf_params.get('max_df', 0.95),\n",
        "        stop_words='english',\n",
        "        use_idf=False # We only need term frequencies.\n",
        "    )\n",
        "\n",
        "    # Fit on the global corpus to establish the unified vocabulary.\n",
        "    vocab_vectorizer.fit(global_corpus)\n",
        "    unified_vocabulary = vocab_vectorizer.vocabulary_\n",
        "\n",
        "    # --- Step 2: Create Probability Distributions for Each Regime ---\n",
        "    logging.info(\"Step 2: Generating probability distributions for each regime...\")\n",
        "\n",
        "    # Create a new vectorizer that will use the fixed, unified vocabulary.\n",
        "    dist_vectorizer = TfidfVectorizer(\n",
        "        ngram_range=tfidf_params.get('ngram_range', (1, 2)),\n",
        "        stop_words='english',\n",
        "        vocabulary=unified_vocabulary,\n",
        "        use_idf=False\n",
        "    )\n",
        "\n",
        "    regime_distributions = {}\n",
        "    epsilon = 1e-10  # Smoothing constant to avoid log(0)\n",
        "\n",
        "    for regime_name, corpus in regime_corpora.items():\n",
        "        # Transform the regime's corpus into term frequency vectors.\n",
        "        tf_vectors = dist_vectorizer.fit_transform(corpus)\n",
        "\n",
        "        # Compute the average term frequency vector for the \"average document\".\n",
        "        avg_tf_vector = np.array(tf_vectors.mean(axis=0)).flatten()\n",
        "\n",
        "        # Add smoothing constant.\n",
        "        avg_tf_vector += epsilon\n",
        "\n",
        "        # Normalize the vector to create a valid probability distribution.\n",
        "        distribution = avg_tf_vector / avg_tf_vector.sum()\n",
        "        regime_distributions[regime_name] = distribution\n",
        "\n",
        "    # --- Step 3: Calculate Pairwise Jensen-Shannon Divergence ---\n",
        "    logging.info(\"Step 3: Calculating the pairwise J-S Divergence matrix...\")\n",
        "\n",
        "    # Initialize a DataFrame to store the results.\n",
        "    js_matrix = pd.DataFrame(\n",
        "        np.zeros((len(regime_names), len(regime_names))),\n",
        "        index=regime_names,\n",
        "        columns=regime_names\n",
        "    )\n",
        "\n",
        "    # Iterate over all unique pairs of regimes.\n",
        "    for r1, r2 in combinations(regime_names, 2):\n",
        "        # Get the corresponding probability distributions.\n",
        "        p = regime_distributions[r1]\n",
        "        q = regime_distributions[r2]\n",
        "\n",
        "        # --- J-S Divergence Calculation ---\n",
        "        # 1. Calculate the mixture distribution M.\n",
        "        m = 0.5 * (p + q)\n",
        "\n",
        "        # 2. Calculate the two KL divergences using scipy's stable entropy function.\n",
        "        # D_KL(P || M) = sum(p_i * log2(p_i / m_i))\n",
        "        kl_p_m = entropy(pk=p, qk=m, base=2)\n",
        "        kl_q_m = entropy(pk=q, qk=m, base=2)\n",
        "\n",
        "        # 3. Calculate the J-S Divergence.\n",
        "        # D_JS(P, Q) = 0.5 * D_KL(P || M) + 0.5 * D_KL(Q || M)\n",
        "        js_divergence = 0.5 * kl_p_m + 0.5 * kl_q_m\n",
        "\n",
        "        # Store the symmetric result in the matrix.\n",
        "        js_matrix.loc[r1, r2] = js_divergence\n",
        "        js_matrix.loc[r2, r1] = js_divergence\n",
        "\n",
        "    # --- Step 4: Validate and Visualize Results ---\n",
        "    logging.info(\"\\nJensen-Shannon Divergence Matrix (raw values):\\n\" + js_matrix.to_string(float_format='{:.4f}'.format))\n",
        "\n",
        "    # Create a heatmap for visualization, as shown in Figure 2 of the paper.\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(\n",
        "        js_matrix,\n",
        "        annot=True,\n",
        "        fmt=\".2f\",\n",
        "        cmap=\"viridis\",\n",
        "        linewidths=.5\n",
        "    )\n",
        "    plt.title(\"Vocabulary Shift (Jensen-Shannon Divergence) Across Regimes\")\n",
        "    plt.show()\n",
        "\n",
        "    logging.info(\"\\n>>> Jensen-Shannon Divergence computation completed successfully. <<<\")\n",
        "\n",
        "    return js_matrix\n"
      ],
      "metadata": {
        "id": "wCXNnULqrYTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 26: Dimensionality Reduction and Visualization\n",
        "\n",
        "def _prepare_data_for_tsne(\n",
        "    data_splits: Dict[str, Dict[str, pd.DataFrame]],\n",
        "    embedding_features: Dict[str, Dict[str, np.ndarray]],\n",
        "    max_samples: int = 5000\n",
        ") -> Tuple[np.ndarray, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Internal helper to aggregate test set embeddings and metadata for t-SNE.\n",
        "\n",
        "    This function extracts all embeddings from the 'test' splits, aligns them\n",
        "    with their regime and sector labels, and performs stratified subsampling\n",
        "    if the total number of samples exceeds a given threshold.\n",
        "\n",
        "    Args:\n",
        "        data_splits: The nested dictionary of data splits.\n",
        "        embedding_features: The nested dictionary of sentence embedding features.\n",
        "        max_samples: The maximum number of samples to use for t-SNE. If the\n",
        "                     total exceeds this, stratified sampling is performed.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A NumPy array of the selected embeddings.\n",
        "        - A pandas DataFrame containing the corresponding metadata (labels).\n",
        "    \"\"\"\n",
        "    # --- 1. Aggregate all test set embeddings and metadata ---\n",
        "    all_embeddings = []\n",
        "    all_metadata = []\n",
        "\n",
        "    # Iterate through each regime in a sorted order for determinism.\n",
        "    for regime_name in sorted(data_splits.keys()):\n",
        "        # Get the test split DataFrame and its corresponding embeddings.\n",
        "        test_df = data_splits[regime_name]['test']\n",
        "        test_embeddings = embedding_features[regime_name]['test']\n",
        "\n",
        "        # Ensure consistency.\n",
        "        if len(test_df) != test_embeddings.shape[0] or test_df.empty:\n",
        "            continue\n",
        "\n",
        "        # Append the embeddings to the master list.\n",
        "        all_embeddings.append(test_embeddings)\n",
        "\n",
        "        # Create and append metadata for each sample.\n",
        "        metadata_df = pd.DataFrame({\n",
        "            'regime': test_df['regime'],\n",
        "            'sector': test_df.index.get_level_values('sector')\n",
        "        })\n",
        "        all_metadata.append(metadata_df)\n",
        "\n",
        "    # Concatenate all embeddings and metadata into single structures.\n",
        "    if not all_embeddings:\n",
        "        raise ValueError(\"No test set embeddings found to process for t-SNE.\")\n",
        "\n",
        "    full_embeddings_array = np.vstack(all_embeddings)\n",
        "    full_metadata_df = pd.concat(all_metadata)\n",
        "\n",
        "    # --- 2. Perform Stratified Subsampling if Necessary ---\n",
        "    n_total_samples = full_embeddings_array.shape[0]\n",
        "    if n_total_samples > max_samples:\n",
        "        logging.warning(\n",
        "            f\"Total samples ({n_total_samples}) exceed max_samples ({max_samples}). \"\n",
        "            \"Performing stratified subsampling for t-SNE efficiency.\"\n",
        "        )\n",
        "\n",
        "        # Use train_test_split as a convenient way to perform stratified sampling.\n",
        "        # We create a combined stratification key to preserve joint distribution.\n",
        "        stratify_key = full_metadata_df['regime'].astype(str) + \"_\" + full_metadata_df['sector'].astype(str)\n",
        "\n",
        "        # We only need the 'train' part of the split, which will be our subsample.\n",
        "        sampled_embeddings, _, sampled_metadata, _ = train_test_split(\n",
        "            full_embeddings_array,\n",
        "            full_metadata_df,\n",
        "            train_size=max_samples,\n",
        "            stratify=stratify_key,\n",
        "            random_state=42\n",
        "        )\n",
        "        return sampled_embeddings, sampled_metadata\n",
        "    else:\n",
        "        # If below the threshold, use all data.\n",
        "        return full_embeddings_array, full_metadata_df\n",
        "\n",
        "\n",
        "def _create_tsne_plot(\n",
        "    tsne_df: pd.DataFrame,\n",
        "    hue_column: str,\n",
        "    title: str,\n",
        "    palette: str = \"viridis\"\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Internal helper to generate and display a single t-SNE scatter plot.\n",
        "    \"\"\"\n",
        "    # Create a new figure for the plot.\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Generate the scatter plot using seaborn for aesthetics.\n",
        "    ax = sns.scatterplot(\n",
        "        data=tsne_df,\n",
        "        x='tsne_dim_1',\n",
        "        y='tsne_dim_2',\n",
        "        hue=hue_column,\n",
        "        palette=palette,\n",
        "        alpha=0.7,\n",
        "        s=50, # Marker size\n",
        "        legend='full'\n",
        "    )\n",
        "\n",
        "    # Set plot titles and labels.\n",
        "    ax.set_title(title, fontsize=16, fontweight='bold')\n",
        "    ax.set_xlabel(\"t-SNE Component 1\", fontsize=12)\n",
        "    ax.set_ylabel(\"t-SNE Component 2\", fontsize=12)\n",
        "\n",
        "    # Improve legend placement.\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "    plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to make space for legend\n",
        "\n",
        "    # Display the plot.\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def run_tsne_visualization_suite(\n",
        "    data_splits: Dict[str, Dict[str, pd.DataFrame]],\n",
        "    embedding_features: Dict[str, Dict[str, np.ndarray]],\n",
        "    tsne_params: Dict[str, Any] = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the t-SNE dimensionality reduction and visualization pipeline.\n",
        "\n",
        "    This function reproduces the analysis for Figures 3 and 4 from the paper.\n",
        "    It aggregates all test set embeddings, performs t-SNE to project them into\n",
        "    2D space, and generates two scatter plots: one colored by macroeconomic\n",
        "    regime and another colored by industry sector.\n",
        "\n",
        "    Args:\n",
        "        data_splits: The nested dictionary of data splits.\n",
        "        embedding_features: The nested dictionary of sentence embedding features.\n",
        "        tsne_params: Optional dictionary of parameters to pass to the\n",
        "                     sklearn.manifold.TSNE constructor.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame containing the 2D t-SNE coordinates and the\n",
        "        corresponding metadata for each sample, which can be used for further\n",
        "        interactive analysis.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Running Task 26: Dimensionality Reduction and Visualization Suite ---\")\n",
        "\n",
        "    # --- Step 1: Prepare Embeddings and Metadata for t-SNE ---\n",
        "    logging.info(\"Step 1: Aggregating test set embeddings and metadata...\")\n",
        "    try:\n",
        "        # This helper function handles aggregation and stratified subsampling.\n",
        "        embeddings_to_process, metadata_df = _prepare_data_for_tsne(\n",
        "            data_splits, embedding_features\n",
        "        )\n",
        "        logging.info(f\"Prepared {embeddings_to_process.shape[0]} samples for t-SNE.\")\n",
        "    except ValueError as e:\n",
        "        logging.error(f\"Failed to prepare data for t-SNE: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # --- Step 2: Execute t-SNE Dimensionality Reduction ---\n",
        "    logging.info(\"Step 2: Executing t-SNE algorithm... (This may take several minutes)\")\n",
        "\n",
        "    # Define default t-SNE parameters, which can be overridden.\n",
        "    default_params = {\n",
        "        'n_components': 2,\n",
        "        'perplexity': 30,\n",
        "        'learning_rate': 'auto',\n",
        "        'n_iter': 1000,\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1 # Use all available CPU cores\n",
        "    }\n",
        "    if tsne_params:\n",
        "        default_params.update(tsne_params)\n",
        "\n",
        "    # Initialize and run the t-SNE algorithm.\n",
        "    tsne = TSNE(**default_params)\n",
        "    tsne_results = tsne.fit_transform(embeddings_to_process)\n",
        "\n",
        "    # Create a DataFrame to hold the results and metadata.\n",
        "    tsne_df = pd.DataFrame(\n",
        "        tsne_results,\n",
        "        columns=['tsne_dim_1', 'tsne_dim_2'],\n",
        "        index=metadata_df.index\n",
        "    )\n",
        "    tsne_df = pd.concat([tsne_df, metadata_df], axis=1)\n",
        "\n",
        "    logging.info(\"t-SNE computation complete.\")\n",
        "\n",
        "    # --- Step 3: Generate Visualization Plots ---\n",
        "    logging.info(\"Step 3: Generating visualization plots...\")\n",
        "\n",
        "    # Plot 1: Colored by Economic Regime (reproducing Figure 3)\n",
        "    _create_tsne_plot(\n",
        "        tsne_df=tsne_df,\n",
        "        hue_column='regime',\n",
        "        title='t-SNE Visualization of Financial News Embeddings by Economic Regime',\n",
        "        palette='viridis'\n",
        "    )\n",
        "\n",
        "    # Plot 2: Colored by Industry Sector (reproducing Figure 4)\n",
        "    _create_tsne_plot(\n",
        "        tsne_df=tsne_df,\n",
        "        hue_column='sector',\n",
        "        title='t-SNE Visualization of Financial News Embeddings by Industry Sector',\n",
        "        palette='tab20' # A palette suitable for many categories\n",
        "    )\n",
        "\n",
        "    logging.info(\"\\n>>> t-SNE visualization suite completed successfully. <<<\")\n",
        "\n",
        "    return tsne_df\n"
      ],
      "metadata": {
        "id": "JRQBIHExtAuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 27: Cross-Regime Analysis Integration\n",
        "\n",
        "def run_cross_regime_analysis_suite(\n",
        "    js_divergence_matrix: pd.DataFrame,\n",
        "    robustness_profile: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Integrates and analyzes the relationship between semantic drift and model performance.\n",
        "\n",
        "    This function tests the paper's central hypothesis by correlating the\n",
        "    Jensen-Shannon (J-S) Divergence between regime pairs with the corresponding\n",
        "    degradation in model performance (measured by the change in MSE).\n",
        "\n",
        "    The process is as follows:\n",
        "    1.  Create a tidy DataFrame aligning each unique regime pair with its\n",
        "        J-S divergence.\n",
        "    2.  For each model, calculate the absolute difference in MSE between the\n",
        "        regimes in each pair.\n",
        "    3.  For each model, compute the Spearman rank correlation between the\n",
        "        J-S divergences and the MSE differences.\n",
        "    4.  Generate scatter plots to visualize this relationship for each model.\n",
        "\n",
        "    Args:\n",
        "        js_divergence_matrix: The symmetric DataFrame of pairwise J-S divergences.\n",
        "        robustness_profile: The DataFrame containing the complete set of\n",
        "                            diagnostic metrics, including MSE.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame summarizing the Spearman correlation coefficient (rho)\n",
        "        and p-value for each model type, quantifying the strength and\n",
        "        significance of the relationship between semantic drift and error.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Running Task 27: Cross-Regime Analysis Integration Suite ---\")\n",
        "\n",
        "    # --- Step 1: Align Semantic Drift and Performance Degradation Data ---\n",
        "    logging.info(\"Step 1: Aligning J-S Divergence with MSE degradation...\")\n",
        "\n",
        "    # Get the names of all regimes.\n",
        "    regime_names = js_divergence_matrix.index.tolist()\n",
        "    # Get the names of all model types.\n",
        "    model_types = robustness_profile.index.get_level_values('model_type').unique().tolist()\n",
        "\n",
        "    # Create a list of all unique, unordered regime pairs.\n",
        "    regime_pairs = list(combinations(regime_names, 2))\n",
        "\n",
        "    # This list will hold the structured data for our analysis.\n",
        "    analysis_data = []\n",
        "\n",
        "    # Iterate through each model type to create a separate analysis.\n",
        "    for model_type in model_types:\n",
        "        # Filter the robustness profile for the current model.\n",
        "        model_mse = robustness_profile.xs(model_type, level='model_type')['MSE']\n",
        "\n",
        "        # Iterate through each unique regime pair.\n",
        "        for r1, r2 in regime_pairs:\n",
        "            # Look up the J-S divergence for the pair.\n",
        "            js_div = js_divergence_matrix.loc[r1, r2]\n",
        "\n",
        "            # Calculate the absolute difference in MSE for the model between the two regimes.\n",
        "            mse_degradation = abs(model_mse.loc[r1] - model_mse.loc[r2])\n",
        "\n",
        "            # Append the aligned data point.\n",
        "            analysis_data.append({\n",
        "                'model_type': model_type,\n",
        "                'regime_pair': f\"{r1}-{r2}\",\n",
        "                'js_divergence': js_div,\n",
        "                'mse_degradation': mse_degradation\n",
        "            })\n",
        "\n",
        "    # Create a single, tidy DataFrame from the collected data.\n",
        "    analysis_df = pd.DataFrame(analysis_data)\n",
        "\n",
        "    logging.info(\"Successfully created aligned analysis DataFrame.\")\n",
        "\n",
        "    # --- Step 2: Correlate Semantic Drift with Model Performance ---\n",
        "    logging.info(\"\\nStep 2: Computing Spearman correlation for each model...\")\n",
        "\n",
        "    correlation_results = []\n",
        "\n",
        "    # Iterate through each model type to compute its correlation.\n",
        "    for model_type in model_types:\n",
        "        # Filter the DataFrame for the current model.\n",
        "        model_df = analysis_df[analysis_df['model_type'] == model_type]\n",
        "\n",
        "        # --- Spearman Rank Correlation ---\n",
        "        # This is a non-parametric test that assesses how well the relationship\n",
        "        # between two variables can be described using a monotonic function.\n",
        "        # It is robust to outliers and non-linearities.\n",
        "        rho, p_value = spearmanr(model_df['js_divergence'], model_df['mse_degradation'])\n",
        "\n",
        "        # Store the results.\n",
        "        correlation_results.append({\n",
        "            'model_type': model_type,\n",
        "            'spearman_rho': rho,\n",
        "            'p_value': p_value\n",
        "        })\n",
        "\n",
        "    # Create a DataFrame to summarize the correlation results.\n",
        "    correlation_df = pd.DataFrame(correlation_results).set_index('model_type')\n",
        "\n",
        "    logging.info(\"Correlation Analysis Results:\\n\" + correlation_df.to_string(float_format='{:.4f}'.format))\n",
        "    logging.warning(\n",
        "        \"Note: p-values should be interpreted with caution due to the small \"\n",
        "        f\"sample size (N={len(regime_pairs)} regime pairs).\"\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Generate Visualization Plots ---\n",
        "    logging.info(\"\\nStep 3: Generating visualization plots...\")\n",
        "\n",
        "    # Use seaborn's `lmplot` to create a scatter plot with a regression line\n",
        "    # for each model type, faceted by model.\n",
        "    g = sns.lmplot(\n",
        "        data=analysis_df,\n",
        "        x='js_divergence',\n",
        "        y='mse_degradation',\n",
        "        col='model_type',\n",
        "        hue='model_type',\n",
        "        height=5,\n",
        "        aspect=1.2,\n",
        "        scatter_kws={'s': 100, 'alpha': 0.8},\n",
        "        ci=95 # Show 95% confidence interval for the regression line.\n",
        "    )\n",
        "\n",
        "    # Set a comprehensive title for the entire figure.\n",
        "    g.fig.suptitle(\"Relationship Between Semantic Drift (J-S Divergence) and Model Error (MSE Degradation)\", y=1.03, fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Improve axis labels.\n",
        "    g.set_axis_labels(\"Jensen-Shannon Divergence\", \"Absolute Difference in MSE\")\n",
        "\n",
        "    # Display the plot.\n",
        "    plt.show()\n",
        "\n",
        "    logging.info(\"\\n>>> Cross-regime analysis suite completed successfully. <<<\")\n",
        "\n",
        "    return correlation_df\n"
      ],
      "metadata": {
        "id": "2mQJhxsrvKii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 28: Stock-Specific Case Study Implementation\n",
        "\n",
        "def _filter_artifacts_for_tickers(\n",
        "    target_tickers: List[str],\n",
        "    predictions_df: pd.DataFrame,\n",
        "    data_splits: Dict[str, Dict[str, pd.DataFrame]],\n",
        "    embedding_features: Dict[str, Dict[str, np.ndarray]]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Dict[str, pd.DataFrame]], Dict[str, Dict[str, np.ndarray]]]:\n",
        "    \"\"\"\n",
        "    Internal helper to meticulously filter all data artifacts for specific tickers.\n",
        "\n",
        "    This function creates a self-consistent, miniature set of all data artifacts\n",
        "    (predictions, data splits, and embedding features) that pertains only to the\n",
        "    specified list of target tickers. This is the rigorous foundation for a\n",
        "    stock-specific case study.\n",
        "\n",
        "    Args:\n",
        "        target_tickers: A list of stock tickers to isolate.\n",
        "        predictions_df: The full DataFrame of predictions.\n",
        "        data_splits: The full dictionary of data splits.\n",
        "        embedding_features: The full dictionary of embedding features.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the filtered versions of:\n",
        "        - ticker_predictions_df\n",
        "        - ticker_data_splits\n",
        "        - ticker_embedding_features\n",
        "    \"\"\"\n",
        "    # --- 1. Filter the main predictions DataFrame ---\n",
        "    # This is a straightforward boolean indexing operation on the 'ticker' level of the index.\n",
        "    ticker_mask = predictions_df.index.get_level_values('ticker').isin(target_tickers)\n",
        "    ticker_predictions_df = predictions_df[ticker_mask].copy()\n",
        "\n",
        "    # --- 2. Filter the nested dictionaries of DataFrames and NumPy arrays ---\n",
        "    ticker_data_splits = {}\n",
        "    ticker_embedding_features = {}\n",
        "\n",
        "    # Iterate through all 12 splits.\n",
        "    for regime, splits in data_splits.items():\n",
        "        ticker_data_splits[regime] = {}\n",
        "        ticker_embedding_features[regime] = {}\n",
        "        for split_name, df in splits.items():\n",
        "            if df.empty:\n",
        "                # If the original split is empty, the filtered split is also empty.\n",
        "                ticker_data_splits[regime][split_name] = pd.DataFrame(columns=df.columns, index=df.index)\n",
        "                # Create an empty embedding array with the correct number of columns.\n",
        "                emb_dim = embedding_features[regime][split_name].shape[1]\n",
        "                ticker_embedding_features[regime][split_name] = np.empty((0, emb_dim), dtype=np.float32)\n",
        "                continue\n",
        "\n",
        "            # Filter the DataFrame for the current ticker.\n",
        "            split_ticker_mask = df.index.get_level_values('ticker').isin(target_tickers)\n",
        "            filtered_df = df[split_ticker_mask].copy()\n",
        "            ticker_data_splits[regime][split_name] = filtered_df\n",
        "\n",
        "            # --- 3. Rigorously filter the parallel embedding array ---\n",
        "            # To filter the NumPy array, we must find the integer positions of the\n",
        "            # rows we want to keep. `get_indexer_for` is the canonical way to do this.\n",
        "            positions_to_keep = df.index.get_indexer_for(filtered_df.index)\n",
        "            # Use these integer positions to slice the embedding array.\n",
        "            ticker_embedding_features[regime][split_name] = embedding_features[regime][split_name][positions_to_keep]\n",
        "\n",
        "    return ticker_predictions_df, ticker_data_splits, ticker_embedding_features\n",
        "\n",
        "\n",
        "def run_stock_specific_case_study(\n",
        "    target_tickers: List[str],\n",
        "    predictions_df: pd.DataFrame,\n",
        "    data_splits: Dict[str, Dict[str, pd.DataFrame]],\n",
        "    embedding_features: Dict[str, Dict[str, np.ndarray]],\n",
        "    training_results: Dict[str, Any],\n",
        "    study_config: Dict[str, Any],\n",
        "    vectorizer: TfidfVectorizer,\n",
        "    st_model: SentenceTransformer,\n",
        "    tokenizer: PreTrainedTokenizerBase\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs a deep-dive case study on a specific list of stocks.\n",
        "\n",
        "    This function isolates all data related to the target tickers and re-computes\n",
        "    the full suite of diagnostic metrics (FCAS, PCS, TSV, NLICS) on these\n",
        "    subsets. This provides a granular view of model performance and robustness\n",
        "    at the individual company level.\n",
        "\n",
        "    Args:\n",
        "        target_tickers: A list of stock tickers for the case study (e.g., ['JPM', 'AAPL']).\n",
        "        predictions_df: The full DataFrame of all predictions.\n",
        "        data_splits: The full dictionary of all data splits.\n",
        "        embedding_features: The full dictionary of all embedding features.\n",
        "        training_results: The dictionary with paths to trained model checkpoints.\n",
        "        study_config: The complete study configuration dictionary.\n",
        "        vectorizer: The globally fitted TfidfVectorizer.\n",
        "        st_model: The initialized SentenceTransformer model.\n",
        "        tokenizer: The initialized HuggingFace tokenizer.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame formatted to be comparable to Table 6 in the paper,\n",
        "        showing the diagnostic metric values for each target stock across all regimes.\n",
        "    \"\"\"\n",
        "    logging.info(f\"--- Running Definitive Stock-Specific Case Study for {target_tickers} ---\")\n",
        "\n",
        "    # --- Step 1: Create Filtered, Stock-Specific Data Artifacts ---\n",
        "    # This helper function robustly filters all complex data structures.\n",
        "    logging.info(f\"Filtering all data artifacts for tickers: {target_tickers}...\")\n",
        "    ticker_predictions_df, ticker_data_splits, ticker_embedding_features = _filter_artifacts_for_tickers(\n",
        "        target_tickers, predictions_df, data_splits, embedding_features\n",
        "    )\n",
        "\n",
        "    if ticker_predictions_df.empty:\n",
        "        logging.warning(f\"No data found for tickers {target_tickers}. Cannot run case study.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # --- Step 2: Re-compute All Diagnostic Metrics on the Filtered Data ---\n",
        "    # Re-running the metrics ensures they are calculated only on the data\n",
        "    # relevant to the case study stocks.\n",
        "\n",
        "    logging.info(\"Re-computing diagnostic metrics on filtered data...\")\n",
        "\n",
        "    # FCAS computation on the filtered prediction set.\n",
        "    fcas_series = compute_fcas(ticker_predictions_df)\n",
        "    fcas_series.name = 'FCAS'\n",
        "\n",
        "    # TSV computation on the filtered embeddings and splits.\n",
        "    tsv_series = compute_tsv(ticker_embedding_features, ticker_data_splits)\n",
        "    tsv_series.name = 'TSV'\n",
        "\n",
        "    # PCS computation on the filtered artifacts.\n",
        "    pcs_series = compute_pcs(\n",
        "        ticker_predictions_df, training_results, ticker_data_splits, study_config,\n",
        "        vectorizer, st_model, tokenizer\n",
        "    )\n",
        "    pcs_series.name = 'PCS'\n",
        "\n",
        "    # NLICS computation on the filtered prediction set (will leverage caching).\n",
        "    nlics_series = compute_nlics(ticker_predictions_df, study_config)\n",
        "    nlics_series.name = 'NLICS'\n",
        "\n",
        "    # --- Step 3: Assemble and Format the Final Case Study Table ---\n",
        "    logging.info(\"Assembling final case study table...\")\n",
        "\n",
        "    # Join all the re-computed metric series into a single DataFrame.\n",
        "    # The index will be (regime, model_type).\n",
        "    case_study_profile = fcas_series.to_frame()\n",
        "    case_study_profile = case_study_profile.join(pcs_series)\n",
        "    case_study_profile = case_study_profile.join(nlics_series)\n",
        "\n",
        "    # The index of the main profile is (regime, model_type). We need to add the ticker.\n",
        "    # Run the logic inside a loop for each ticker.\n",
        "    all_results = []\n",
        "    for ticker in target_tickers:\n",
        "        # This re-runs the filtering for each ticker.\n",
        "        t_preds, t_splits, t_embeds = _filter_artifacts_for_tickers([ticker], predictions_df, data_splits, embedding_features)\n",
        "\n",
        "        if t_preds.empty: continue\n",
        "\n",
        "        # Re-compute metrics for this single ticker\n",
        "        fcas = compute_fcas(t_preds).rename('FCAS')\n",
        "        tsv = compute_tsv(t_embeds, t_splits).rename('TSV')\n",
        "        pcs = compute_pcs(t_preds, training_results, t_splits, study_config, vectorizer, st_model, tokenizer).rename('PCS')\n",
        "        nlics = compute_nlics(t_preds, study_config).rename('NLICS')\n",
        "\n",
        "        # Join all metrics for this ticker\n",
        "        ticker_res = pd.concat([fcas, pcs, nlics], axis=1)\n",
        "        ticker_res = ticker_res.join(tsv, on='regime')\n",
        "        ticker_res['Stock'] = ticker\n",
        "        all_results.append(ticker_res.reset_index())\n",
        "\n",
        "    if not all_results:\n",
        "        logging.warning(\"No case study results were generated.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Concatenate results from all tickers into a single tidy DataFrame.\n",
        "    final_tidy_table = pd.concat(all_results)\n",
        "\n",
        "    # --- Format the table to resemble Table 6 from the paper ---\n",
        "    # Table 6 shows metrics for one model at a time. Let's assume we want to see all.\n",
        "    # The structure is (Stock, Regime) as index and metrics as columns.\n",
        "    final_table = final_tidy_table.set_index(['Stock', 'regime', 'model_type']).sort_index()\n",
        "\n",
        "    logging.info(\"\\n--- Case Study Results (Tidy Format) ---\")\n",
        "    # Display the full, tidy table with all models.\n",
        "    logging.info(\"\\n\" + final_table.to_string(float_format='{:.4f}'.format))\n",
        "\n",
        "    # To exactly reproduce a table like Table 6, you would filter this final table, e.g.:\n",
        "    # table_6_like = final_table.xs('feature_transformer', level='model_type')\n",
        "\n",
        "    logging.info(\"\\n>>> Stock-specific case study completed successfully. <<<\")\n",
        "\n",
        "    return final_table\n"
      ],
      "metadata": {
        "id": "h_u0mNnzxa_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 29: Ablation Study Implementation\n",
        "\n",
        "def perform_metric_ablation_analysis(\n",
        "    robustness_profile: pd.DataFrame,\n",
        "    target_regime: str = \"Rate-Hike\",\n",
        "    target_model: str = \"text_transformer\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs and presents a metric ablation analysis based on computed results.\n",
        "\n",
        "    This function reproduces the analysis shown in Table 7 of the paper. It\n",
        "    isolates the results for a specific model and regime (e.g., Text Transformer\n",
        "    during the Rate-Hike period) and presents a table showing how the\n",
        "    robustness profile appears when each metric is individually excluded.\n",
        "\n",
        "    Note on Interpretation: In our modular pipeline, metrics are computed\n",
        "    independently. Therefore, \"excluding\" a metric does not change the values\n",
        "    of the other metrics. This analysis serves as a reporting tool to visualize\n",
        "    the complete profile and compare it to profiles missing one component.\n",
        "\n",
        "    Args:\n",
        "        robustness_profile: The complete DataFrame of diagnostic metrics from\n",
        "                            the evaluation orchestrator.\n",
        "        target_regime: The specific regime to focus on for the ablation study.\n",
        "        target_model: The specific model to focus on for the ablation study.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame representing the metric ablation table, which can be\n",
        "        styled or displayed.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If the specified target_regime or target_model is not found\n",
        "                  in the robustness_profile index.\n",
        "    \"\"\"\n",
        "    logging.info(f\"--- Running Task 29, Step 1: Metric Ablation Analysis ---\")\n",
        "    logging.info(f\"Focusing on Regime: '{target_regime}', Model: '{target_model}'\")\n",
        "\n",
        "    # --- Step 1: Isolate the Data for the Target Configuration ---\n",
        "    try:\n",
        "        # Use .xs() to select the specific row from the MultiIndex.\n",
        "        # This returns a pandas Series containing all metric values for the target.\n",
        "        source_metrics = robustness_profile.xs(\n",
        "            (target_regime, target_model),\n",
        "            level=('regime', 'model_type')\n",
        "        ).iloc[0] # Use iloc[0] in case of duplicate indices, though there shouldn't be.\n",
        "\n",
        "    except KeyError:\n",
        "        # Provide a specific error if the requested data slice does not exist.\n",
        "        raise KeyError(\n",
        "            f\"The combination of regime='{target_regime}' and model_type='{target_model}' \"\n",
        "            \"was not found in the robustness_profile DataFrame.\"\n",
        "        )\n",
        "\n",
        "    # --- Step 2: Define Ablation Configurations and Build the Table ---\n",
        "    # The metrics to be ablated, in the desired order.\n",
        "    # Note: MSE is not ablated in the paper's table, so we exclude it from this list.\n",
        "    metrics_to_ablate = ['FCAS', 'PCS', 'TSV', 'NLICS']\n",
        "\n",
        "    # The full set of columns for the final table.\n",
        "    table_columns = ['FCAS', 'PCS', 'TSV', 'NLICS']\n",
        "\n",
        "    # This list will hold the data for each row of the final table.\n",
        "    ablation_data = []\n",
        "\n",
        "    # --- Create the \"Full Evaluation\" row ---\n",
        "    # This is simply the original set of metrics.\n",
        "    full_eval_row = source_metrics[table_columns].to_dict()\n",
        "    full_eval_row['Model Variant'] = 'Full Evaluation'\n",
        "    ablation_data.append(full_eval_row)\n",
        "\n",
        "    # --- Create a row for each ablated metric ---\n",
        "    for metric in metrics_to_ablate:\n",
        "        # Create a copy of the original metric values.\n",
        "        ablated_row_data = source_metrics[table_columns].to_dict()\n",
        "        # Set the value of the ablated metric to NaN.\n",
        "        ablated_row_data[metric] = np.nan\n",
        "        # Define the name for this configuration.\n",
        "        ablated_row_data['Model Variant'] = f'No {metric}'\n",
        "        # Append the new row to our data list.\n",
        "        ablation_data.append(ablated_row_data)\n",
        "\n",
        "    # --- Step 3: Assemble and Format the Final DataFrame ---\n",
        "    # Create the DataFrame from the list of dictionaries.\n",
        "    ablation_df = pd.DataFrame(ablation_data)\n",
        "\n",
        "    # Set 'Model Variant' as the index.\n",
        "    ablation_df = ablation_df.set_index('Model Variant')\n",
        "\n",
        "    # Reorder the columns to match the desired output.\n",
        "    ablation_df = ablation_df[table_columns]\n",
        "\n",
        "    logging.info(\"\\nMetric Ablation Analysis Table (Reproducing Table 7):\\n\" + ablation_df.to_string(float_format='{:.2f}'.format, na_rep='N/A'))\n",
        "\n",
        "    logging.info(\"\\n>>> Metric ablation analysis completed successfully. <<<\")\n",
        "\n",
        "    return ablation_df\n",
        "\n",
        "\n",
        "def execute_single_training_run(\n",
        "    model_type: str,\n",
        "    train_df: pd.DataFrame,\n",
        "    val_df: pd.DataFrame,\n",
        "    study_config: Dict[str, Any],\n",
        "    checkpoint_path: Path,\n",
        "    vectorizer: TfidfVectorizer,\n",
        "    st_model: SentenceTransformer,\n",
        "    tokenizer: PreTrainedTokenizerBase,\n",
        "    device: torch.device\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes a single, isolated training run for a given model and dataset.\n",
        "\n",
        "    This refactored, granular function is the core atomic unit of training. It\n",
        "    takes a specific training and validation set and handles the entire training\n",
        "    process for one model, including data prep, optimization setup, and execution.\n",
        "\n",
        "    Args:\n",
        "        model_type: The type of model to train ('lstm', 'text_transformer', etc.).\n",
        "        train_df: The DataFrame for training.\n",
        "        val_df: The DataFrame for validation.\n",
        "        study_config: The main study configuration.\n",
        "        checkpoint_path: The path to save the best model.\n",
        "        vectorizer: The fitted TfidfVectorizer.\n",
        "        st_model: The initialized SentenceTransformer.\n",
        "        tokenizer: The initialized AutoTokenizer.\n",
        "        device: The torch.device to train on.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the training history and the path to the best model.\n",
        "    \"\"\"\n",
        "    # --- 1. Instantiate Model ---\n",
        "    model_config = study_config['model_training']['architectures'][model_type]\n",
        "    model_classes = {'lstm': LSTMRegressionModel, 'text_transformer': TextTransformerRegressionModel, 'feature_transformer': FeatureEnhancedMLP}\n",
        "    model = model_classes[model_type](model_config)\n",
        "    model.to(device)\n",
        "\n",
        "    # --- 2. Prepare Features and DataLoaders ---\n",
        "    # This block dynamically prepares the correct feature format for the given model.\n",
        "    if model_type == 'text_transformer':\n",
        "        train_features = tokenizer(train_df['aggregated_text'].tolist(), max_length=512, padding='max_length', truncation=True, return_tensors='np')\n",
        "        val_features = tokenizer(val_df['aggregated_text'].tolist(), max_length=512, padding='max_length', truncation=True, return_tensors='np')\n",
        "    else:\n",
        "        train_tfidf = vectorizer.transform(train_df['aggregated_text']).toarray()\n",
        "        val_tfidf = vectorizer.transform(val_df['aggregated_text']).toarray()\n",
        "        if model_type == 'lstm':\n",
        "            train_features, val_features = train_tfidf, val_tfidf\n",
        "        else: # feature_transformer\n",
        "            train_emb = st_model.encode(train_df['aggregated_text'].tolist(), batch_size=64)\n",
        "            val_emb = st_model.encode(val_df['aggregated_text'].tolist(), batch_size=64)\n",
        "            train_features = np.hstack([train_tfidf, train_emb])\n",
        "            val_features = np.hstack([val_tfidf, val_emb])\n",
        "\n",
        "    train_loader = create_dataloaders(train_features, train_df['target_return'].values, model_type, study_config['model_training']['global_params']['batch_size'], shuffle=True)\n",
        "    val_loader = create_dataloaders(val_features, val_df['target_return'].values, model_type, study_config['model_training']['global_params']['batch_size'], shuffle=False)\n",
        "\n",
        "    # --- 3. Setup Optimization and Run Training ---\n",
        "    optimization_components = setup_optimization_components(model, study_config['model_training']['global_params'])\n",
        "\n",
        "    training_result = run_training_orchestrator(\n",
        "        model=model,\n",
        "        optimization_components=optimization_components,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        device=device,\n",
        "        num_epochs=50,\n",
        "        patience=5,\n",
        "        checkpoint_path=checkpoint_path\n",
        "    )\n",
        "    return training_result\n",
        "\n",
        "\n",
        "def execute_single_inference_run(\n",
        "    model_type: str,\n",
        "    checkpoint_path: Path,\n",
        "    test_df: pd.DataFrame,\n",
        "    study_config: Dict[str, Any],\n",
        "    vectorizer: TfidfVectorizer,\n",
        "    st_model: SentenceTransformer,\n",
        "    tokenizer: PreTrainedTokenizerBase,\n",
        "    device: torch.device\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Executes a single, isolated inference run and returns the MSE.\n",
        "\n",
        "    Args:\n",
        "        model_type: The type of model being evaluated.\n",
        "        checkpoint_path: Path to the trained model weights.\n",
        "        test_df: The DataFrame for testing.\n",
        "        study_config: The main study configuration.\n",
        "        ... (feature engineering artifacts) ...\n",
        "        device: The torch.device to run inference on.\n",
        "\n",
        "    Returns:\n",
        "        The calculated Mean Squared Error on the test set.\n",
        "    \"\"\"\n",
        "    # --- 1. Load Model ---\n",
        "    model_config = study_config['model_training']['architectures'][model_type]\n",
        "    model_classes = {'lstm': LSTMRegressionModel, 'text_transformer': TextTransformerRegressionModel, 'feature_transformer': FeatureEnhancedMLP}\n",
        "    model = model_classes[model_type](model_config)\n",
        "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "    model.to(device).eval()\n",
        "\n",
        "    # --- 2. Prepare Test Data ---\n",
        "    if model_type == 'text_transformer':\n",
        "        test_features = tokenizer(test_df['aggregated_text'].tolist(), max_length=512, padding='max_length', truncation=True, return_tensors='np')\n",
        "    else:\n",
        "        test_tfidf = vectorizer.transform(test_df['aggregated_text']).toarray()\n",
        "        if model_type == 'lstm':\n",
        "            test_features = test_tfidf\n",
        "        else:\n",
        "            test_emb = st_model.encode(test_df['aggregated_text'].tolist(), batch_size=64)\n",
        "            test_features = np.hstack([test_tfidf, test_emb])\n",
        "\n",
        "    test_loader = create_dataloaders(test_features, test_df['target_return'].values, model_type, study_config['model_training']['global_params']['batch_size'] * 2, shuffle=False)\n",
        "\n",
        "    # --- 3. Generate Predictions and Calculate MSE ---\n",
        "    predictions = generate_predictions_for_split(model, test_loader, device)\n",
        "    squared_errors = (predictions - test_df['target_return'].values) ** 2\n",
        "    mse = np.mean(squared_errors)\n",
        "\n",
        "    return mse\n",
        "\n",
        "\n",
        "def perform_feature_augmentation_ablation(\n",
        "    robustness_profile: pd.DataFrame,\n",
        "    data_splits: Dict[str, Dict[str, pd.DataFrame]],\n",
        "    study_config: Dict[str, Any],\n",
        "    vectorizer: TfidfVectorizer,\n",
        "    st_model: SentenceTransformer,\n",
        "    force_retrain_cross_sector: bool = False\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs a definitive feature augmentation ablation study.\n",
        "\n",
        "    This analysis compares a \"Text Only\" model with a \"Feature Enhanced\" model\n",
        "    across three dimensions: TSV, NLICS, and Cross-Sector MSE. This version\n",
        "    includes a full, non-placeholder implementation of the cross-sector\n",
        "    training and evaluation experiment.\n",
        "\n",
        "    Args:\n",
        "        robustness_profile: The complete DataFrame of diagnostic metrics.\n",
        "        data_splits: The nested dictionary of data splits.\n",
        "        study_config: The main study configuration.\n",
        "        vectorizer: The globally fitted TfidfVectorizer.\n",
        "        st_model: The initialized SentenceTransformer model.\n",
        "        force_retrain_cross_sector: If True, retrains cross-sector models\n",
        "                                    even if checkpoints exist.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame summarizing the feature augmentation ablation results.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Running Definitive Feature Augmentation Ablation Analysis ---\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(study_config['model_training']['architectures']['text_transformer']['base_model_identifier'])\n",
        "\n",
        "    # --- 1. Calculate Average TSV and NLICS from existing results ---\n",
        "    logging.info(\"Step 1: Calculating average TSV and NLICS from main results...\")\n",
        "    avg_tsv = robustness_profile['TSV'].mean()\n",
        "    avg_nlics = robustness_profile.groupby(level='model_type')['NLICS'].mean()\n",
        "    nlics_text_only = avg_nlics.get('text_transformer', np.nan)\n",
        "    nlics_feature_enhanced = avg_nlics.get('feature_transformer', np.nan)\n",
        "\n",
        "    # --- 2. Run Full Cross-Sector MSE Experiment ---\n",
        "    logging.info(\"Step 2: Running full Cross-Sector MSE experiment (Financials -> Health Care)...\")\n",
        "\n",
        "    # --- a. Prepare sector-specific data slices ---\n",
        "    source_sector, target_sector = 'Financials', 'Health Care'\n",
        "\n",
        "    source_train_dfs = [df[df.index.get_level_values('sector') == source_sector] for r, s in data_splits.items() for sn, df in s.items() if sn == 'training']\n",
        "    source_val_dfs = [df[df.index.get_level_values('sector') == source_sector] for r, s in data_splits.items() for sn, df in s.items() if sn == 'validation']\n",
        "    target_test_dfs = [df[df.index.get_level_values('sector') == target_sector] for r, s in data_splits.items() for sn, df in s.items() if sn == 'testing']\n",
        "\n",
        "    source_train_df = pd.concat(source_train_dfs)\n",
        "    source_val_df = pd.concat(source_val_dfs)\n",
        "    target_test_df = pd.concat(target_test_dfs)\n",
        "\n",
        "    cross_sector_results = {}\n",
        "\n",
        "    for model_type in ['text_transformer', 'feature_transformer']:\n",
        "        logging.info(f\"  - Running cross-sector experiment for model: {model_type}\")\n",
        "\n",
        "        # --- b. Train model on source sector (if needed) ---\n",
        "        checkpoint_dir = Path(\"checkpoints/cross_sector\")\n",
        "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "        checkpoint_path = checkpoint_dir / f\"{model_type}_trained_on_{source_sector}.pth\"\n",
        "\n",
        "        if not checkpoint_path.exists() or force_retrain_cross_sector:\n",
        "            logging.info(f\"    - Training model on '{source_sector}' data...\")\n",
        "            execute_single_training_run(\n",
        "                model_type=model_type,\n",
        "                train_df=source_train_df,\n",
        "                val_df=source_val_df,\n",
        "                study_config=study_config,\n",
        "                checkpoint_path=checkpoint_path,\n",
        "                vectorizer=vectorizer,\n",
        "                st_model=st_model,\n",
        "                tokenizer=tokenizer,\n",
        "                device=device\n",
        "            )\n",
        "        else:\n",
        "            logging.info(f\"    - Found existing checkpoint: {checkpoint_path}\")\n",
        "\n",
        "        # --- c. Evaluate model on target sector ---\n",
        "        logging.info(f\"    - Evaluating on '{target_sector}' data...\")\n",
        "        mse = execute_single_inference_run(\n",
        "            model_type=model_type,\n",
        "            checkpoint_path=checkpoint_path,\n",
        "            test_df=target_test_df,\n",
        "            study_config=study_config,\n",
        "            vectorizer=vectorizer,\n",
        "            st_model=st_model,\n",
        "            tokenizer=tokenizer,\n",
        "            device=device\n",
        "        )\n",
        "        cross_sector_results[model_type] = mse\n",
        "        logging.info(f\"    - Cross-Sector MSE: {mse:.4f}\")\n",
        "\n",
        "    # --- 3. Assemble the Final Table ---\n",
        "    logging.info(\"Step 3: Assembling final ablation table...\")\n",
        "\n",
        "    ablation_results = pd.DataFrame([\n",
        "        {\n",
        "            'Model Type': 'Text Only',\n",
        "            'TSV': avg_tsv,\n",
        "            'NLICS': nlics_text_only,\n",
        "            'Cross-Sector MSE': cross_sector_results.get('text_transformer', np.nan)\n",
        "        },\n",
        "        {\n",
        "            'Model Type': 'Feature Enhanced',\n",
        "            'TSV': avg_tsv,\n",
        "            'NLICS': nlics_feature_enhanced,\n",
        "            'Cross-Sector MSE': cross_sector_results.get('feature_transformer', np.nan)\n",
        "        }\n",
        "    ]).set_index('Model Type')\n",
        "\n",
        "    logging.info(\"\\nDefinitive Feature Augmentation Ablation Analysis:\\n\" + ablation_results.to_string(float_format='{:.3f}'.format))\n",
        "\n",
        "    logging.info(\"\\n>>> Feature augmentation ablation analysis completed successfully. <<<\")\n",
        "\n",
        "    return ablation_results\n",
        "\n",
        "\n",
        "def create_nli_benchmark_file(\n",
        "    predictions_df: pd.DataFrame,\n",
        "    output_path: Path,\n",
        "    n_samples: int = 100,\n",
        "    random_state: int = 42\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Creates and saves a reproducible benchmark dataset for human annotation.\n",
        "\n",
        "    This function generates a random, stratified sample of text-hypothesis pairs\n",
        "    and saves it to a CSV file. This file is intended to be given to a human\n",
        "    expert for annotation. It includes a 'human_label' column pre-filled with\n",
        "    placeholders that must be manually replaced.\n",
        "\n",
        "    Args:\n",
        "        predictions_df: The full DataFrame of all predictions.\n",
        "        output_path: The path to save the CSV file for annotation.\n",
        "        n_samples: The number of samples to include in the benchmark.\n",
        "        random_state: The random seed for reproducibility.\n",
        "    \"\"\"\n",
        "    logging.info(f\"--- Creating NLI Benchmark File for Annotation ---\")\n",
        "\n",
        "    # --- 1. Create a reproducible random sample ---\n",
        "    # Stratifying by regime ensures the sample is representative.\n",
        "    benchmark_df = predictions_df.groupby('regime').sample(\n",
        "        frac=min(1.0, n_samples / len(predictions_df)), # Adjust frac for safety\n",
        "        random_state=random_state\n",
        "    ).sample(n=n_samples, random_state=random_state) # Final sample to get exact n_samples\n",
        "\n",
        "    # --- 2. Generate hypothesis strings ---\n",
        "    benchmark_df['hypothesis'] = benchmark_df['prediction'].apply(\n",
        "        lambda p: \"The stock price will increase\" if p > 0 else \"The stock price will decrease\"\n",
        "    )\n",
        "\n",
        "    # --- 3. Add placeholder for human labels ---\n",
        "    # The annotator should replace this with 1.0 (Entailment), 0.5 (Neutral), or 0.0 (Contradiction).\n",
        "    benchmark_df['human_label'] = \"ANNOTATE_HERE\"\n",
        "\n",
        "    # --- 4. Select relevant columns and save ---\n",
        "    # We only need the text, hypothesis, and the column to be annotated.\n",
        "    # The index is preserved for later merging.\n",
        "    columns_to_save = ['aggregated_text', 'hypothesis', 'human_label']\n",
        "    benchmark_df[columns_to_save].to_csv(output_path)\n",
        "\n",
        "    logging.info(f\"Successfully saved benchmark file for annotation to '{output_path}'.\")\n",
        "    logging.warning(\n",
        "        \"ACTION REQUIRED: Please manually edit this CSV file and replace \"\n",
        "        \"'ANNOTATE_HERE' with numerical labels (1.0, 0.5, 0.0) before \"\n",
        "        \"running the entailment model ablation.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def perform_entailment_model_ablation(\n",
        "    predictions_df: pd.DataFrame,\n",
        "    annotated_benchmark_path: Path,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs a definitive ablation study comparing GPT-4 and BART-NLI.\n",
        "\n",
        "    This analysis loads a human-annotated benchmark dataset and evaluates both\n",
        "    GPT-4 and a local BART-NLI model against it. It compares the models on:\n",
        "    1.  NLICS: The average score produced by the model on the benchmark set.\n",
        "    2.  Human Agreement %: The percentage of exact matches between the model's\n",
        "        score and the expert human's score.\n",
        "\n",
        "    Args:\n",
        "        predictions_df: The full DataFrame of all predictions (used for context).\n",
        "        annotated_benchmark_path: The file path to the CSV file that has been\n",
        "                                  manually annotated by a human expert.\n",
        "        study_config: The complete study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame summarizing the comparison results.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the annotated benchmark file does not exist.\n",
        "        ValueError: If the annotated file is invalid (e.g., missing columns,\n",
        "                    invalid labels).\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Running Definitive Entailment Model Comparison Ablation ---\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # --- 1. Load and Validate the Annotated Benchmark Dataset ---\n",
        "    logging.info(f\"Step 1: Loading and validating annotated benchmark data from '{annotated_benchmark_path}'...\")\n",
        "    if not annotated_benchmark_path.exists():\n",
        "        raise FileNotFoundError(f\"Annotated benchmark file not found. Please create and annotate it first using `create_nli_benchmark_file`.\")\n",
        "\n",
        "    # Load the annotated data.\n",
        "    benchmark_df = pd.read_csv(annotated_benchmark_path, index_col=[0, 1, 2]) # Assuming original multi-index\n",
        "\n",
        "    # --- Validation of the annotated file ---\n",
        "    if 'human_label' not in benchmark_df.columns:\n",
        "        raise ValueError(\"Annotated file is missing the 'human_label' column.\")\n",
        "    if benchmark_df['human_label'].isnull().any() or (benchmark_df['human_label'] == \"ANNOTATE_HERE\").any():\n",
        "        raise ValueError(\"Annotated file contains missing or un-annotated 'ANNOTATE_HERE' values.\")\n",
        "    try:\n",
        "        benchmark_df['human_label'] = pd.to_numeric(benchmark_df['human_label'])\n",
        "    except ValueError:\n",
        "        raise ValueError(\"The 'human_label' column must contain only numeric values (1.0, 0.5, 0.0).\")\n",
        "    valid_labels = {1.0, 0.5, 0.0}\n",
        "    if not set(benchmark_df['human_label'].unique()).issubset(valid_labels):\n",
        "        raise ValueError(f\"Invalid values found in 'human_label' column. Only {valid_labels} are allowed.\")\n",
        "\n",
        "    # Join with original predictions_df to get all necessary columns like 'prediction'.\n",
        "    benchmark_df = predictions_df.join(benchmark_df[['human_label', 'hypothesis']], how='inner')\n",
        "    logging.info(f\"Successfully loaded and validated {len(benchmark_df)} annotated samples.\")\n",
        "\n",
        "    # --- 2. Evaluate with BART-NLI ---\n",
        "    logging.info(\"\\nStep 2: Evaluating benchmark data with BART-NLI...\")\n",
        "    bart_scores = _evaluate_with_bart_nli(benchmark_df, device)\n",
        "    bart_nlics = bart_scores.mean()\n",
        "    # Use np.isclose for robust floating-point comparison.\n",
        "    bart_agreement = np.isclose(bart_scores, benchmark_df['human_label']).mean() * 100\n",
        "\n",
        "    # --- 3. Evaluate with GPT-4 ---\n",
        "    logging.info(\"\\nStep 3: Evaluating benchmark data with GPT-4...\")\n",
        "    nlics_config = study_config['diagnostics']['nlics_metric']\n",
        "    cache_path = Path(\"results/nlics_cache.jsonl\")\n",
        "    gpt4_scores = asyncio.run(_compute_nlics_async_cached(benchmark_df, nlics_config, cache_path))\n",
        "    gpt4_nlics = gpt4_scores.mean()\n",
        "    # Use np.isclose for robust floating-point comparison.\n",
        "    gpt4_agreement = np.isclose(gpt4_scores, benchmark_df['human_label']).mean() * 100\n",
        "\n",
        "    # --- 4. Assemble the Final Table ---\n",
        "    logging.info(\"\\nStep 4: Assembling final comparison table...\")\n",
        "\n",
        "    comparison_results = pd.DataFrame([\n",
        "        {\n",
        "            'Entailment Model': 'BART-NLI',\n",
        "            'NLICS': bart_nlics,\n",
        "            'Human Agreement (%)': bart_agreement\n",
        "        },\n",
        "        {\n",
        "            'Entailment Model': 'GPT-4',\n",
        "            'NLICS': gpt4_nlics,\n",
        "            'Human Agreement (%)': gpt4_agreement\n",
        "        }\n",
        "    ]).set_index('Entailment Model')\n",
        "\n",
        "    logging.info(\"\\nDefinitive Entailment Model Comparison (Reproducing Table 9):\\n\" + comparison_results.to_string(float_format='{:.2f}'.format))\n",
        "\n",
        "    logging.info(\"\\n>>> Entailment model comparison ablation completed successfully. <<<\")\n",
        "\n",
        "    return comparison_results\n"
      ],
      "metadata": {
        "id": "MJV0Q9JH2qH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 30: Control Experiment Implementation\n",
        "\n",
        "def _identify_earnings_events(\n",
        "    predictions_df: pd.DataFrame,\n",
        "    keywords: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filters the predictions DataFrame to identify rows related to earnings events.\n",
        "    \"\"\"\n",
        "    # Create a case-insensitive regex pattern that matches any of the whole-word keywords.\n",
        "    pattern = r'\\b(' + '|'.join(keywords) + r')\\b'\n",
        "\n",
        "    # Apply the filter to the 'aggregated_text' column.\n",
        "    earnings_mask = predictions_df['aggregated_text'].str.contains(pattern, case=False, na=False, regex=True)\n",
        "\n",
        "    return predictions_df[earnings_mask]\n",
        "\n",
        "\n",
        "def run_control_experiment(\n",
        "    predictions_df: pd.DataFrame,\n",
        "    data_splits: Dict[str, Dict[str, pd.DataFrame]],\n",
        "    embedding_features: Dict[str, Dict[str, np.ndarray]],\n",
        "    training_results: Dict[str, Any],\n",
        "    study_config: Dict[str, Any],\n",
        "    vectorizer: TfidfVectorizer,\n",
        "    st_model: SentenceTransformer,\n",
        "    tokenizer: PreTrainedTokenizerBase\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs the control experiment to disentangle situational vs. linguistic drift.\n",
        "\n",
        "    This function reproduces the analysis from Table 5 of the paper. It isolates\n",
        "    a specific, consistent event type (quarterly earnings reports) and computes\n",
        "    PCS and TSV metrics both within single regimes and across different regimes\n",
        "    to measure the impact of narrative framing.\n",
        "\n",
        "    Args:\n",
        "        predictions_df: The full DataFrame of all predictions.\n",
        "        ... (all other data artifacts required for metric computation) ...\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame summarizing the control experiment results, formatted\n",
        "        to match Table 5.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Running Task 30: Control Experiment Implementation ---\")\n",
        "\n",
        "    # --- 1. Identify all earnings-related events in the test sets ---\n",
        "    logging.info(\"Step 1: Identifying all earnings-related events...\")\n",
        "\n",
        "    # Define a comprehensive list of keywords to identify earnings reports.\n",
        "    earnings_keywords = ['earnings', 'quarterly', 'q1', 'q2', 'q3', 'q4', 'revenue', 'profit', 'guidance', 'forecast']\n",
        "\n",
        "    # We need to work with the full test set data.\n",
        "    test_dfs = [splits['test'] for regime, splits in data_splits.items()]\n",
        "    full_test_df = pd.concat(test_dfs)\n",
        "\n",
        "    # Filter the full test set to get only earnings-related rows.\n",
        "    earnings_df = _identify_earnings_events(full_test_df, earnings_keywords)\n",
        "\n",
        "    # Filter the main predictions DataFrame to align with these events.\n",
        "    earnings_predictions_df = predictions_df.loc[earnings_df.index]\n",
        "\n",
        "    if earnings_predictions_df.empty:\n",
        "        logging.error(\"No earnings events found in any test set. Cannot run control experiment.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    logging.info(f\"Identified {len(earnings_predictions_df)} total earnings-related samples in test sets.\")\n",
        "\n",
        "    # --- 2. Define the experimental conditions ---\n",
        "    # These are the specific comparisons we need to make for Table 5.\n",
        "    experiments = {\n",
        "        \"Within-Regime (pre-COVID)\": (\"Pre-COVID\", \"Pre-COVID\"),\n",
        "        \"Within-Regime (COVID)\": (\"COVID\", \"COVID\"),\n",
        "        \"Cross-Regime (pre-COVID vs. COVID)\": (\"Pre-COVID\", \"COVID\"),\n",
        "        \"Cross-Regime (post-COVID vs. rate-hike)\": (\"Post-COVID\", \"Rate-Hike\")\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # --- 3. Run analysis for each experimental condition ---\n",
        "    for name, (regime1, regime2) in experiments.items():\n",
        "        logging.info(f\"\\n--- Analyzing Experiment: {name} ---\")\n",
        "\n",
        "        # --- a. Isolate data for the current experiment ---\n",
        "        regimes_to_include = list(set([regime1, regime2]))\n",
        "\n",
        "        # Filter the earnings predictions for the relevant regimes.\n",
        "        exp_preds_df = earnings_predictions_df[earnings_predictions_df['regime'].isin(regimes_to_include)]\n",
        "\n",
        "        if exp_preds_df.empty:\n",
        "            logging.warning(f\"No earnings data for experiment '{name}'. Skipping.\")\n",
        "            results.append({'Event Type': name, 'PCS': np.nan, 'TSV': np.nan})\n",
        "            continue\n",
        "\n",
        "        # Create the filtered artifacts needed for the metric computations.\n",
        "        _, exp_splits, exp_embeds = _filter_artifacts_for_tickers(\n",
        "            exp_preds_df.index.get_level_values('ticker').unique().tolist(),\n",
        "            exp_preds_df, # Pass the already filtered preds\n",
        "            data_splits,\n",
        "            embedding_features\n",
        "        )\n",
        "\n",
        "        # --- b. Re-compute metrics on the isolated data ---\n",
        "\n",
        "        # Compute PCS. This will average the PCS across all models in the specified regimes.\n",
        "        pcs_series = compute_pcs(\n",
        "            exp_preds_df, training_results, exp_splits, study_config,\n",
        "            vectorizer, st_model, tokenizer\n",
        "        )\n",
        "        # We take the overall mean PCS for this experimental condition.\n",
        "        pcs_score = pcs_series.mean()\n",
        "\n",
        "        # Compute TSV.\n",
        "        if regime1 == regime2: # Within-Regime TSV\n",
        "            tsv_series = compute_tsv(exp_embeds, exp_splits)\n",
        "            tsv_score = tsv_series.get(regime1, np.nan)\n",
        "        else: # Cross-Regime TSV\n",
        "            # Concatenate the sorted embeddings from both regimes' test sets.\n",
        "            embeds1 = pd.DataFrame(exp_embeds[regime1]['test'], index=exp_splits[regime1]['test'].index).sort_index(level='date')\n",
        "            embeds2 = pd.DataFrame(exp_embeds[regime2]['test'], index=exp_splits[regime2]['test'].index).sort_index(level='date')\n",
        "            combined_embeds = np.vstack([embeds1.values, embeds2.values])\n",
        "            if len(combined_embeds) < 2:\n",
        "                tsv_score = np.nan\n",
        "            else:\n",
        "                distances = np.linalg.norm(np.diff(combined_embeds, axis=0), axis=1)\n",
        "                tsv_score = distances.mean()\n",
        "\n",
        "        logging.info(f\"  - Computed PCS: {pcs_score:.4f}\")\n",
        "        logging.info(f\"  - Computed TSV: {tsv_score:.4f}\")\n",
        "\n",
        "        results.append({'Event Type': name, 'PCS': pcs_score, 'TSV': tsv_score})\n",
        "\n",
        "    # --- 4. Assemble and display the final table ---\n",
        "    final_table = pd.DataFrame(results).set_index('Event Type')\n",
        "\n",
        "    # Reorder to match the paper's Table 5.\n",
        "    final_order = [\n",
        "        \"Within-Regime (pre-COVID)\",\n",
        "        \"Within-Regime (COVID)\",\n",
        "        \"Cross-Regime (pre-COVID vs. COVID)\",\n",
        "        \"Cross-Regime (post-COVID vs. rate-hike)\"\n",
        "    ]\n",
        "    final_table = final_table.reindex(final_order)\n",
        "\n",
        "    logging.info(\"\\nControl Experiment Results (Reproducing Table 5):\\n\" + final_table.to_string(float_format='{:.2f}'.format))\n",
        "\n",
        "    logging.info(\"\\n>>> Control experiment completed successfully. <<<\")\n",
        "\n",
        "    return final_table\n"
      ],
      "metadata": {
        "id": "60v6nlY80knf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 31: Cross-Sector Data Preparation\n",
        "\n",
        "def prepare_cross_sector_datasets(\n",
        "    data_splits: Dict[str, Dict[str, pd.DataFrame]],\n",
        "    source_sector: str,\n",
        "    target_sector: str\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Prepares all necessary data subsets for a cross-sector generalization experiment.\n",
        "\n",
        "    This function takes the full, regime-based data splits and re-aggregates\n",
        "    them into sector-specific training, validation, and testing sets. It prepares\n",
        "    data for two distinct experiments:\n",
        "    1.  Cross-Sector: Training on the source sector, testing on the target sector.\n",
        "    2.  In-Sector Baseline: Training on the target sector, testing on the target sector.\n",
        "\n",
        "    Args:\n",
        "        data_splits: The nested dictionary of all 12 data splits.\n",
        "        source_sector: The name of the sector to train on (e.g., 'Financials').\n",
        "        target_sector: The name of the sector to test on (e.g., 'Health Care').\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the four key aggregated DataFrames:\n",
        "        - 'source_train': All training data from the source sector.\n",
        "        - 'source_val': All validation data from the source sector.\n",
        "        - 'target_train': All training data from the target sector (for baseline).\n",
        "        - 'target_val': All validation data from the target sector (for baseline).\n",
        "        - 'target_test': All testing data from the target sector.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the source and target sectors are the same, or if\n",
        "                    insufficient data is found for any key subset.\n",
        "    \"\"\"\n",
        "    logging.info(f\"--- Running Task 31: Cross-Sector Data Preparation ---\")\n",
        "    logging.info(f\"Source Sector: '{source_sector}', Target Sector: '{target_sector}'\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    if source_sector == target_sector:\n",
        "        raise ValueError(\"Source and target sectors must be different for a cross-sector experiment.\")\n",
        "\n",
        "    # --- 1. Isolate and Aggregate Data for Each Required Subset ---\n",
        "\n",
        "    # This dictionary will hold lists of DataFrame chunks before concatenation.\n",
        "    subset_chunks: Dict[str, List[pd.DataFrame]] = {\n",
        "        'source_train': [], 'source_val': [],\n",
        "        'target_train': [], 'target_val': [], 'target_test': []\n",
        "    }\n",
        "\n",
        "    # Iterate through all 12 original data splits.\n",
        "    for regime, splits in data_splits.items():\n",
        "        for split_name, df in splits.items():\n",
        "            if df.empty:\n",
        "                continue\n",
        "\n",
        "            # Filter for the source sector.\n",
        "            source_mask = df.index.get_level_values('sector') == source_sector\n",
        "            source_df_chunk = df[source_mask]\n",
        "\n",
        "            # Filter for the target sector.\n",
        "            target_mask = df.index.get_level_values('sector') == target_sector\n",
        "            target_df_chunk = df[target_mask]\n",
        "\n",
        "            # Append the filtered chunks to the correct lists.\n",
        "            if not source_df_chunk.empty:\n",
        "                if split_name == 'training':\n",
        "                    subset_chunks['source_train'].append(source_df_chunk)\n",
        "                elif split_name == 'validation':\n",
        "                    subset_chunks['source_val'].append(source_df_chunk)\n",
        "\n",
        "            if not target_df_chunk.empty:\n",
        "                if split_name == 'training':\n",
        "                    subset_chunks['target_train'].append(target_df_chunk)\n",
        "                elif split_name == 'validation':\n",
        "                    subset_chunks['target_val'].append(target_df_chunk)\n",
        "                elif split_name == 'testing':\n",
        "                    subset_chunks['target_test'].append(target_df_chunk)\n",
        "\n",
        "    # --- 2. Concatenate Chunks and Validate ---\n",
        "\n",
        "    # This dictionary will hold the final, aggregated DataFrames.\n",
        "    sector_datasets: Dict[str, pd.DataFrame] = {}\n",
        "\n",
        "    for name, chunks in subset_chunks.items():\n",
        "        if not chunks:\n",
        "            # Raise an error if a critical subset has no data.\n",
        "            raise ValueError(f\"No data found for '{name}' subset. Cannot proceed with cross-sector analysis.\")\n",
        "\n",
        "        # Concatenate all chunks for the given subset into a single DataFrame.\n",
        "        agg_df = pd.concat(chunks)\n",
        "\n",
        "        # Sort by date as a best practice.\n",
        "        agg_df.sort_index(level='date', inplace=True)\n",
        "\n",
        "        # Store the final aggregated DataFrame.\n",
        "        sector_datasets[name] = agg_df\n",
        "\n",
        "        logging.info(f\"Created '{name}' dataset with {len(agg_df)} samples.\")\n",
        "\n",
        "    # --- 3. Final Sanity Check ---\n",
        "    # Ensure there is no ticker overlap between the source training set and target test set.\n",
        "    source_tickers = set(sector_datasets['source_train'].index.get_level_values('ticker'))\n",
        "    target_tickers = set(sector_datasets['target_test'].index.get_level_values('ticker'))\n",
        "\n",
        "    if not source_tickers.isdisjoint(target_tickers):\n",
        "        logging.warning(\n",
        "            f\"Ticker overlap found between source train and target test sets: \"\n",
        "            f\"{source_tickers.intersection(target_tickers)}. This is unexpected \"\n",
        "            \"for distinct sectors but may occur with multi-sector companies.\"\n",
        "        )\n",
        "\n",
        "    logging.info(\"\\n>>> Cross-sector dataset preparation completed successfully. <<<\")\n",
        "\n",
        "    return sector_datasets\n"
      ],
      "metadata": {
        "id": "Nv8exPw_Js7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 32: Cross-Sector Performance Analysis\n",
        "\n",
        "def run_cross_sector_performance_analysis(\n",
        "    sector_datasets: Dict[str, pd.DataFrame],\n",
        "    study_config: Dict[str, Any],\n",
        "    vectorizer: TfidfVectorizer,\n",
        "    st_model: SentenceTransformer,\n",
        "    force_retrain: bool = False\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the cross-sector performance analysis (reproducing Table 4).\n",
        "\n",
        "    This function executes a series of experiments to measure the generalization\n",
        "    capability of different model architectures when trained on one domain\n",
        "    (source sector) and evaluated on another (target sector). It compares\n",
        "    this cross-sector performance to an in-sector baseline.\n",
        "\n",
        "    Args:\n",
        "        sector_datasets: A dictionary of DataFrames prepared by\n",
        "                         `prepare_cross_sector_datasets`.\n",
        "        study_config: The complete study configuration dictionary.\n",
        "        vectorizer: The globally fitted TfidfVectorizer.\n",
        "        st_model: The initialized SentenceTransformer model.\n",
        "        force_retrain: If True, retrains models even if checkpoints exist.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame summarizing the cross-sector and in-sector MSE results,\n",
        "        formatted to be comparable to Table 4.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Running Task 32: Cross-Sector Performance Analysis ---\")\n",
        "\n",
        "    # --- 1. Setup Environment and Artifacts ---\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        study_config['model_training']['architectures']['text_transformer']['base_model_identifier']\n",
        "    )\n",
        "\n",
        "    # Define the model types to be analyzed. The paper's Table 4 is slightly\n",
        "    # ambiguous. We interpret \"Text-Only\" as our `text_transformer` and\n",
        "    # \"Feature-Based\" as our `feature_transformer`.\n",
        "    model_types_to_analyze = ['text_transformer', 'feature_transformer']\n",
        "\n",
        "    # Define sectors\n",
        "    source_sector = 'Financials'\n",
        "    target_sector = 'Health Care'\n",
        "\n",
        "    # This dictionary will store the final MSE results.\n",
        "    results: Dict[str, Dict[str, float]] = {\n",
        "        'Cross-Sector MSE': {},\n",
        "        'In-Sector MSE (Baseline)': {}\n",
        "    }\n",
        "\n",
        "    # --- 2. Execute Training and Evaluation Runs ---\n",
        "    # We need to run four training experiments in total.\n",
        "\n",
        "    for model_type in model_types_to_analyze:\n",
        "        logging.info(f\"\\n--- Processing Model Type: {model_type} ---\")\n",
        "\n",
        "        # --- a. Cross-Sector Experiment (Train on Source, Test on Target) ---\n",
        "        logging.info(f\"  - Running Cross-Sector Experiment (Train: {source_sector}, Test: {target_sector})\")\n",
        "\n",
        "        # Define a unique checkpoint for this experiment.\n",
        "        checkpoint_dir = Path(f\"checkpoints/cross_sector/{source_sector}_trained\")\n",
        "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "        cross_sector_checkpoint_path = checkpoint_dir / f\"{model_type}.pth\"\n",
        "\n",
        "        # Train the model on the source sector data if needed.\n",
        "        if not cross_sector_checkpoint_path.exists() or force_retrain:\n",
        "            logging.info(f\"    - Training model on '{source_sector}' data...\")\n",
        "            execute_single_training_run(\n",
        "                model_type=model_type,\n",
        "                train_df=sector_datasets['source_train'],\n",
        "                val_df=sector_datasets['source_val'],\n",
        "                study_config=study_config,\n",
        "                checkpoint_path=cross_sector_checkpoint_path,\n",
        "                vectorizer=vectorizer, st_model=st_model, tokenizer=tokenizer, device=device\n",
        "            )\n",
        "        else:\n",
        "            logging.info(f\"    - Found existing checkpoint: {cross_sector_checkpoint_path}\")\n",
        "\n",
        "        # Evaluate the source-trained model on the target sector test data.\n",
        "        cross_sector_mse = execute_single_inference_run(\n",
        "            model_type=model_type,\n",
        "            checkpoint_path=cross_sector_checkpoint_path,\n",
        "            test_df=sector_datasets['target_test'],\n",
        "            study_config=study_config,\n",
        "            vectorizer=vectorizer, st_model=st_model, tokenizer=tokenizer, device=device\n",
        "        )\n",
        "        results['Cross-Sector MSE'][model_type] = cross_sector_mse\n",
        "        logging.info(f\"    - Cross-Sector MSE: {cross_sector_mse:.4f}\")\n",
        "\n",
        "        # --- b. In-Sector Baseline Experiment (Train on Target, Test on Target) ---\n",
        "        logging.info(f\"  - Running In-Sector Baseline (Train: {target_sector}, Test: {target_sector})\")\n",
        "\n",
        "        # Define a unique checkpoint for this experiment.\n",
        "        checkpoint_dir = Path(f\"checkpoints/cross_sector/{target_sector}_trained\")\n",
        "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "        in_sector_checkpoint_path = checkpoint_dir / f\"{model_type}.pth\"\n",
        "\n",
        "        # Train the model on the target sector data if needed.\n",
        "        if not in_sector_checkpoint_path.exists() or force_retrain:\n",
        "            logging.info(f\"    - Training model on '{target_sector}' data...\")\n",
        "            execute_single_training_run(\n",
        "                model_type=model_type,\n",
        "                train_df=sector_datasets['target_train'],\n",
        "                val_df=sector_datasets['target_val'],\n",
        "                study_config=study_config,\n",
        "                checkpoint_path=in_sector_checkpoint_path,\n",
        "                vectorizer=vectorizer, st_model=st_model, tokenizer=tokenizer, device=device\n",
        "            )\n",
        "        else:\n",
        "            logging.info(f\"    - Found existing checkpoint: {in_sector_checkpoint_path}\")\n",
        "\n",
        "        # Evaluate the target-trained model on the target sector test data.\n",
        "        in_sector_mse = execute_single_inference_run(\n",
        "            model_type=model_type,\n",
        "            checkpoint_path=in_sector_checkpoint_path,\n",
        "            test_df=sector_datasets['target_test'],\n",
        "            study_config=study_config,\n",
        "            vectorizer=vectorizer, st_model=st_model, tokenizer=tokenizer, device=device\n",
        "        )\n",
        "        results['In-Sector MSE (Baseline)'][model_type] = in_sector_mse\n",
        "        logging.info(f\"    - In-Sector MSE: {in_sector_mse:.4f}\")\n",
        "\n",
        "    # --- 3. Assemble and Format the Final Table ---\n",
        "    logging.info(\"\\nAssembling final cross-sector analysis table...\")\n",
        "\n",
        "    # Create the DataFrame from the results dictionary.\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Rename index for clarity in the final table.\n",
        "    results_df.index = results_df.index.map({\n",
        "        'text_transformer': 'Text-Only',\n",
        "        'feature_transformer': 'Feature-Based'\n",
        "    })\n",
        "    results_df.index.name = 'Model'\n",
        "\n",
        "    # The paper's Table 4 has a slightly different structure. We will present\n",
        "    # our more direct comparison.\n",
        "\n",
        "    logging.info(\"\\nCross-Sector Performance Analysis:\\n\" + results_df.to_string(float_format='{:.4f}'.format))\n",
        "\n",
        "    # Add a transferability ratio for more insight.\n",
        "    results_df['Transferability Ratio'] = results_df['Cross-Sector MSE'] / results_df['In-Sector MSE (Baseline)']\n",
        "\n",
        "    logging.info(\"\\nAnalysis with Transferability Ratio (Lower is Better):\\n\" + results_df.to_string(float_format='{:.4f}'.format))\n",
        "\n",
        "    logging.info(\"\\n>>> Cross-sector performance analysis completed successfully. <<<\")\n",
        "\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "eL0LmB-4LTA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 33: Multi-Sector Robustness Validation\n",
        "\n",
        "def run_multi_sector_robustness_validation(\n",
        "    data_splits: Dict[str, Dict[str, pd.DataFrame]],\n",
        "    study_config: Dict[str, Any],\n",
        "    vectorizer: TfidfVectorizer,\n",
        "    st_model: SentenceTransformer,\n",
        "    force_retrain: bool = False\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates a large-scale, multi-sector robustness validation experiment.\n",
        "\n",
        "    This function systematically evaluates the generalization capability of models\n",
        "    across all available GICS sectors. For each model type, it generates a full\n",
        "    N x N transferability matrix where N is the number of sectors. Each cell (i, j)\n",
        "    in the matrix represents the performance of a model trained on sector i and\n",
        "    evaluated on sector j.\n",
        "\n",
        "    Args:\n",
        "        data_splits: The nested dictionary of all 12 data splits.\n",
        "        study_config: The complete study configuration dictionary.\n",
        "        vectorizer: The globally fitted TfidfVectorizer.\n",
        "        st_model: The initialized SentenceTransformer model.\n",
        "        force_retrain: If True, retrains models even if checkpoints exist.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are model types and values are the corresponding\n",
        "        pandas DataFrame transferability matrices.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Running Task 33: Multi-Sector Robustness Validation ---\")\n",
        "\n",
        "    # --- 1. Setup Environment and Artifacts ---\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        study_config['model_training']['architectures']['text_transformer']['base_model_identifier']\n",
        "    )\n",
        "\n",
        "    # Identify all unique sectors present in the dataset.\n",
        "    all_sectors = sorted(list(\n",
        "        pd.concat([df for r in data_splits.values() for df in r.values()])\n",
        "        .index.get_level_values('sector').unique()\n",
        "    ))\n",
        "    logging.info(f\"Found {len(all_sectors)} unique sectors for analysis: {all_sectors}\")\n",
        "\n",
        "    model_types_to_analyze = ['text_transformer', 'feature_transformer']\n",
        "\n",
        "    # This list will store the long-format results before pivoting.\n",
        "    all_results_long = []\n",
        "\n",
        "    # --- 2. Main Experiment Loop: Iterate over all Sector Permutations ---\n",
        "    # `permutations` gives us all (source, target) pairs, including (A, A).\n",
        "    for source_sector, target_sector in tqdm(list(permutations(all_sectors, 2)), desc=\"Sector Pairs\"):\n",
        "        for model_type in model_types_to_analyze:\n",
        "\n",
        "            logging.info(f\"\\n--- Processing: [Model: {model_type}] | [Train: {source_sector}] -> [Test: {target_sector}] ---\")\n",
        "\n",
        "            try:\n",
        "                # --- a. Prepare the specific data slices for this pair ---\n",
        "                sector_datasets = prepare_cross_sector_datasets(data_splits, source_sector, target_sector)\n",
        "\n",
        "                # --- b. Train model on source sector (leveraging idempotency) ---\n",
        "                checkpoint_dir = Path(f\"checkpoints/multi_sector/{source_sector}_trained\")\n",
        "                checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "                checkpoint_path = checkpoint_dir / f\"{model_type}.pth\"\n",
        "\n",
        "                if not checkpoint_path.exists() or force_retrain:\n",
        "                    logging.info(f\"    - Training model on '{source_sector}' data...\")\n",
        "                    execute_single_training_run(\n",
        "                        model_type=model_type,\n",
        "                        train_df=sector_datasets['source_train'],\n",
        "                        val_df=sector_datasets['source_val'],\n",
        "                        study_config=study_config,\n",
        "                        checkpoint_path=checkpoint_path,\n",
        "                        vectorizer=vectorizer, st_model=st_model, tokenizer=tokenizer, device=device\n",
        "                    )\n",
        "                else:\n",
        "                    logging.info(f\"    - Found existing checkpoint for '{source_sector}'-trained model.\")\n",
        "\n",
        "                # --- c. Evaluate the trained model on the target sector ---\n",
        "                logging.info(f\"    - Evaluating on '{target_sector}' data...\")\n",
        "                mse = execute_single_inference_run(\n",
        "                    model_type=model_type,\n",
        "                    checkpoint_path=checkpoint_path,\n",
        "                    test_df=sector_datasets['target_test'],\n",
        "                    study_config=study_config,\n",
        "                    vectorizer=vectorizer, st_model=st_model, tokenizer=tokenizer, device=device\n",
        "                )\n",
        "\n",
        "                # Append the result to our long-format list.\n",
        "                all_results_long.append({\n",
        "                    'model_type': model_type,\n",
        "                    'source_sector': source_sector,\n",
        "                    'target_sector': target_sector,\n",
        "                    'mse': mse\n",
        "                })\n",
        "\n",
        "            except ValueError as e:\n",
        "                # Gracefully handle cases where a sector has insufficient data.\n",
        "                logging.warning(f\"Skipping pair ({source_sector}, {target_sector}) for model {model_type} due to data issue: {e}\")\n",
        "                all_results_long.append({\n",
        "                    'model_type': model_type,\n",
        "                    'source_sector': source_sector,\n",
        "                    'target_sector': target_sector,\n",
        "                    'mse': np.nan\n",
        "                })\n",
        "\n",
        "    # --- 3. Assemble and Process the Final Transferability Matrices ---\n",
        "    if not all_results_long:\n",
        "        logging.error(\"No cross-sector results were generated.\")\n",
        "        return {}\n",
        "\n",
        "    results_df = pd.DataFrame(all_results_long)\n",
        "\n",
        "    final_matrices = {}\n",
        "    for model_type in model_types_to_analyze:\n",
        "        logging.info(f\"\\n--- Assembling Transferability Matrix for: {model_type} ---\")\n",
        "\n",
        "        model_results_df = results_df[results_df['model_type'] == model_type]\n",
        "\n",
        "        # Pivot the long-format data to create the MSE matrix.\n",
        "        mse_matrix = model_results_df.pivot_table(\n",
        "            index='source_sector',\n",
        "            columns='target_sector',\n",
        "            values='mse'\n",
        "        )\n",
        "\n",
        "        # The diagonal of this matrix is the in-sector baseline MSE.\n",
        "        baseline_mses = pd.Series(np.diag(mse_matrix), index=mse_matrix.index)\n",
        "\n",
        "        # Calculate the Transferability Ratio matrix.\n",
        "        # Ratio = Cross-Sector MSE / In-Sector Baseline MSE\n",
        "        # We divide each column (target sector) by its corresponding baseline MSE.\n",
        "        transferability_matrix = mse_matrix.div(baseline_mses, axis=1)\n",
        "\n",
        "        final_matrices[model_type] = transferability_matrix\n",
        "\n",
        "        # --- 4. Visualize the Matrix ---\n",
        "        plt.figure(figsize=(14, 10))\n",
        "        sns.heatmap(\n",
        "            transferability_matrix,\n",
        "            annot=True,\n",
        "            fmt=\".2f\",\n",
        "            cmap=\"coolwarm\",\n",
        "            linewidths=.5,\n",
        "            center=1.0 # Center the colormap at 1.0 (perfect transfer)\n",
        "        )\n",
        "        plt.title(f\"Cross-Sector Transferability Matrix (Ratio) for {model_type}\", fontsize=16)\n",
        "        plt.xlabel(\"Target Sector (Evaluation)\", fontsize=12)\n",
        "        plt.ylabel(\"Source Sector (Training)\", fontsize=12)\n",
        "        plt.show()\n",
        "\n",
        "    logging.info(\"\\n>>> Multi-sector robustness validation completed successfully. <<<\")\n",
        "\n",
        "    return final_matrices\n"
      ],
      "metadata": {
        "id": "l1bBE7RsNnO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Orchestrator Function\n",
        "\n",
        "def run_full_research_pipeline(\n",
        "    raw_df: pd.DataFrame,\n",
        "    study_config: Dict[str, Any],\n",
        "    results_dir: Path = Path(\"results\"),\n",
        "    force_rerun: bool = False\n",
        ") -> Dict[str, Path]:\n",
        "    \"\"\"\n",
        "    Executes the complete end-to-end research pipeline from data validation\n",
        "    to final analysis, reproducing the study \"Quantifying Semantic Shift\".\n",
        "\n",
        "    This master orchestrator function integrates all 33 previously defined tasks\n",
        "    into a single, sequential, and auditable workflow. It is designed to be\n",
        "    idempotent, leveraging checkpointing in the training-intensive steps to\n",
        "    allow for resumption without losing progress.\n",
        "\n",
        "    Args:\n",
        "        raw_df: The initial, raw pandas DataFrame containing all market and\n",
        "                news data.\n",
        "        study_config: The complete study configuration dictionary.\n",
        "        results_dir: The root directory to save all artifacts.\n",
        "        force_rerun: If True, ignores existing artifacts and re-runs all steps.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping the names of key final artifacts to their file paths.\n",
        "    \"\"\"\n",
        "    # --- Initialize a dictionary to store the final results ---\n",
        "    final_artifacts = {}\n",
        "\n",
        "    # ==========================================================================\n",
        "    # PHASE 1: VALIDATION AND CLEANSING (Tasks 1-3)\n",
        "    # ==========================================================================\n",
        "    logging.info(\"\\n\" + \"=\"*80 + \"\\nPHASE 1: CONFIGURATION VALIDATION AND DATA QUALITY ASSURANCE\\n\" + \"=\"*80)\n",
        "\n",
        "    # Task 1: Validate the study configuration dictionary.\n",
        "    run_config_validation_suite(study_config)\n",
        "\n",
        "    # Task 2: Validate the structure and integrity of the raw DataFrame.\n",
        "    run_dataframe_validation_suite(raw_df)\n",
        "\n",
        "    # Task 3: Perform data quality checks and cleanse the data.\n",
        "    df_cleaned = run_data_quality_and_cleansing_suite(raw_df)\n",
        "\n",
        "    # ==========================================================================\n",
        "    # PHASE 2 & 3: PARTITIONING AND FEATURE ENGINEERING (Tasks 4-9)\n",
        "    # ==========================================================================\n",
        "    logging.info(\"\\n\" + \"=\"*80 + \"\\nPHASE 2 & 3: DATA PARTITIONING AND FEATURE ENGINEERING\\n\" + \"=\"*80)\n",
        "\n",
        "    # Define path for a key intermediate artifact.\n",
        "    data_splits_path = results_dir / \"data_splits.pkl\"\n",
        "    artifact_paths['data_splits'] = data_splits_path\n",
        "\n",
        "    if not data_splits_path.exists() or force_rerun:\n",
        "        df_regimes = run_regime_assignment_suite(df_cleaned, study_config)\n",
        "        data_splits = run_chronological_splitting_suite(df_regimes, study_config)\n",
        "        pd.to_pickle(data_splits, data_splits_path)\n",
        "    else:\n",
        "        logging.info(f\"Loading existing data splits from '{data_splits_path}'...\")\n",
        "        data_splits = pd.read_pickle(data_splits_path)\n",
        "\n",
        "    # Feature engineering steps are fast, so we can re-run them.\n",
        "    vectorizer, tfidf_features = run_tfidf_vectorization_suite(data_splits, study_config)\n",
        "    embedding_features = run_embedding_extraction_suite(data_splits, study_config)\n",
        "    combined_features = run_feature_concatenation_suite(tfidf_features, embedding_features, data_splits, study_config)\n",
        "\n",
        "    # ==========================================================================\n",
        "    # PHASE 4 & 5: MODEL TRAINING (Tasks 10-15)\n",
        "    # ==========================================================================\n",
        "    logging.info(\"\\n\" + \"=\"*80 + \"\\nPHASE 4 & 5: MODEL ARCHITECTURE AND TRAINING\\n\" + \"=\"*80)\n",
        "    # Tasks 10-12 are the class definitions, which are implicitly used here.\n",
        "\n",
        "    # Task 15: Run the master training orchestrator for all 12 models.\n",
        "    training_results = run_regime_specific_training_pipeline(\n",
        "        data_splits=data_splits,\n",
        "        tfidf_features=tfidf_features,\n",
        "        embedding_features=embedding_features,\n",
        "        combined_features=combined_features,\n",
        "        study_config=study_config,\n",
        "        force_retrain=force_rerun\n",
        "    )\n",
        "    training_results_path = results_dir / \"training_results.pkl\"\n",
        "    pd.to_pickle(training_results, training_results_path)\n",
        "    artifact_paths['training_results'] = training_results_path\n",
        "\n",
        "    # ==========================================================================\n",
        "    # PHASE 6: PREDICTION AND BASELINE EVALUATION (Tasks 16-18)\n",
        "    # ==========================================================================\n",
        "    logging.info(\"\\n\" + \"=\"*80 + \"\\nPHASE 6: PREDICTION GENERATION AND BASELINE EVALUATION\\n\" + \"=\"*80)\n",
        "\n",
        "    enriched_predictions_path = results_dir / \"enriched_predictions.pkl\"\n",
        "    artifact_paths['enriched_predictions'] = enriched_predictions_path\n",
        "\n",
        "    if not enriched_predictions_path.exists() or force_rerun:\n",
        "        predictions_df = run_inference_pipeline(\n",
        "            training_results=training_results,\n",
        "            data_splits=data_splits,\n",
        "            tfidf_features=tfidf_features,\n",
        "            embedding_features=embedding_features,\n",
        "            combined_features=combined_features,\n",
        "            study_config=study_config\n",
        "        )\n",
        "        enriched_predictions_df = enrich_and_store_predictions(\n",
        "            predictions_df=predictions_df, export_path=enriched_predictions_path,\n",
        "            metadata={'experiment_id': 'full_run_v1', 'run_timestamp': datetime.utcnow().isoformat()}\n",
        "        )\n",
        "    else:\n",
        "        logging.info(f\"Loading existing enriched predictions from '{enriched_predictions_path}'...\")\n",
        "        enriched_predictions_df = load_and_validate_predictions(enriched_predictions_path)\n",
        "\n",
        "    # Task 17: Compute and display the main MSE results table.\n",
        "    mse_table = run_mse_evaluation_suite(enriched_predictions_df, study_config)\n",
        "    final_artifacts['mse_table'] = mse_table\n",
        "\n",
        "    # ==========================================================================\n",
        "    # PHASE 7-11: FULL ANALYSIS SUITE (Tasks 19-33)\n",
        "    # ==========================================================================\n",
        "    logging.info(\"\\n\" + \"=\"*80 + \"\\nPHASE 7-11: FULL ANALYSIS SUITE\\n\" + \"=\"*80)\n",
        "\n",
        "    # --- Load artifacts needed for the full analysis suite ---\n",
        "    st_model = initialize_sentence_transformer(study_config['feature_engineering']['sentence_embeddings']['model_identifier'])\n",
        "    tokenizer = AutoTokenizer.from_pretrained(study_config['model_training']['architectures']['text_transformer']['base_model_identifier'])\n",
        "\n",
        "    # --- Run all analyses ---\n",
        "    # Each of these functions is an orchestrator for a major analytical task.\n",
        "\n",
        "    # Task 19-24: Main diagnostic metrics\n",
        "    robustness_profile = run_diagnostic_metrics_orchestrator(\n",
        "        predictions_df=enriched_predictions_df, study_config=study_config, data_splits=data_splits,\n",
        "        embedding_features=embedding_features, training_results=training_results, vectorizer=vectorizer,\n",
        "        st_model=st_model, tokenizer=tokenizer\n",
        "    )\n",
        "    robustness_profile_path = results_dir / \"robustness_profile.csv\"\n",
        "    robustness_profile.to_csv(robustness_profile_path)\n",
        "    artifact_paths['robustness_profile'] = robustness_profile_path\n",
        "\n",
        "    # Task 25: J-S Divergence\n",
        "    js_matrix = compute_js_divergence_matrix(data_splits, study_config)\n",
        "    js_matrix_path = results_dir / \"js_divergence_matrix.csv\"\n",
        "    js_matrix.to_csv(js_matrix_path)\n",
        "    artifact_paths['js_divergence_matrix'] = js_matrix_path\n",
        "\n",
        "    # Task 26: t-SNE\n",
        "    run_tsne_visualization_suite(data_splits, embedding_features)\n",
        "\n",
        "    # Task 27: Correlation Analysis\n",
        "    correlation_analysis = run_cross_regime_analysis_suite(js_matrix, robustness_profile)\n",
        "    correlation_path = results_dir / \"correlation_analysis.csv\"\n",
        "    correlation_analysis.to_csv(correlation_path)\n",
        "    artifact_paths['correlation_analysis'] = correlation_path\n",
        "\n",
        "    # Task 28: Case Study\n",
        "    case_study = run_stock_specific_case_study(\n",
        "        target_tickers=['JPM', 'AAPL'], predictions_df=enriched_predictions_df, data_splits=data_splits,\n",
        "        embedding_features=embedding_features, training_results=training_results, study_config=study_config,\n",
        "        vectorizer=vectorizer, st_model=st_model, tokenizer=tokenizer\n",
        "    )\n",
        "    case_study_path = results_dir / \"case_study_results.csv\"\n",
        "    case_study.to_csv(case_study_path)\n",
        "    artifact_paths['case_study_results'] = case_study_path\n",
        "\n",
        "    # Task 29: Ablation Studies (computational parts)\n",
        "    metric_ablation = perform_metric_ablation_analysis(robustness_profile)\n",
        "    metric_ablation_path = results_dir / \"metric_ablation.csv\"\n",
        "    metric_ablation.to_csv(metric_ablation_path)\n",
        "    artifact_paths['metric_ablation'] = metric_ablation_path\n",
        "\n",
        "    # Generate the file needed for the manual part of the entailment ablation\n",
        "    benchmark_file_path = results_dir / \"nli_benchmark_for_annotation.csv\"\n",
        "    create_nli_benchmark_file(enriched_predictions_df, benchmark_file_path)\n",
        "    artifact_paths['nli_benchmark_for_annotation'] = benchmark_file_path\n",
        "    logging.warning(f\"ACTION REQUIRED: The file '{benchmark_file_path}' has been created. It must be manually annotated before running the entailment ablation analysis.\")\n",
        "\n",
        "    # Task 31-33: Cross-Sector Analysis\n",
        "    transferability_matrices = run_multi_sector_robustness_validation(\n",
        "        data_splits=data_splits, study_config=study_config, vectorizer=vectorizer,\n",
        "        st_model=st_model, force_retrain=force_rerun\n",
        "    )\n",
        "    transferability_path = results_dir / \"transferability_matrices.pkl\"\n",
        "    pd.to_pickle(transferability_matrices, transferability_path)\n",
        "    artifact_paths['transferability_matrices'] = transferability_path\n",
        "\n",
        "    logging.info(\"\\n\" + \"=\"*80 + \"\\n====== FULL AUTOMATED RESEARCH PIPELINE COMPLETED SUCCESSFULLY ======\\n\" + \"=\"*80)\n",
        "\n",
        "    return artifact_paths\n",
        "\n",
        "\n",
        "def run_entailment_ablation_analysis(\n",
        "    enriched_predictions_df_path: Path,\n",
        "    annotated_benchmark_path: Path,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Executes the entailment model comparison using a human-annotated file.\n",
        "\n",
        "    This function should be run ONLY AFTER the main pipeline is complete and\n",
        "    the benchmark CSV file it generates has been manually filled out by a\n",
        "    human expert.\n",
        "\n",
        "    Args:\n",
        "        enriched_predictions_df_path: Path to the saved 'enriched_predictions.pkl' artifact.\n",
        "        annotated_benchmark_path: Path to the CSV file after human annotation.\n",
        "        study_config: The complete study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame summarizing the entailment model comparison.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Running Task 29, Step 3: Entailment Model Comparison ---\")\n",
        "\n",
        "    # Load the necessary prediction data.\n",
        "    predictions_df = load_and_validate_predictions(enriched_predictions_df_path)\n",
        "\n",
        "    # Run the ablation study using the annotated file.\n",
        "    entailment_ablation_results = perform_entailment_model_ablation(\n",
        "        predictions_df=predictions_df,\n",
        "        annotated_benchmark_path=annotated_benchmark_path,\n",
        "        study_config=study_config\n",
        "    )\n",
        "\n",
        "    return entailment_ablation_results\n"
      ],
      "metadata": {
        "id": "-H4R6ENEY8Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 34: Comprehensive Results Assembly\n",
        "\n",
        "def _style_table(\n",
        "    df: pd.DataFrame,\n",
        "    caption: str,\n",
        "    precision: int = 2,\n",
        "    highlight_min_axis: Optional[int] = None\n",
        ") -> pd.io.formats.style.Styler:\n",
        "    \"\"\"\n",
        "    Applies professional, publication-quality styling to a pandas DataFrame.\n",
        "\n",
        "    This generic helper function takes a DataFrame and returns a pandas Styler\n",
        "    object with a consistent set of presentation-focused formatting rules. It is\n",
        "    used to generate the final tables for the research report.\n",
        "\n",
        "    The applied styles include:\n",
        "    - A descriptive caption for the table.\n",
        "    - Uniform numerical precision for all cells.\n",
        "    - A standard representation for missing values ('N/A').\n",
        "    - Centered text alignment for headers and data cells.\n",
        "    - Optionally, highlighting the minimum value in each row or column.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame to be styled.\n",
        "        caption (str): The title caption to be displayed above the table.\n",
        "        precision (int, optional): The number of decimal places to format\n",
        "                                   numerical values to. Defaults to 2.\n",
        "        highlight_min_axis (Optional[int], optional): The axis along which to\n",
        "            highlight the minimum value. `0` for columns, `1` for rows. If\n",
        "            `None`, no highlighting is applied. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        pd.io.formats.style.Styler: A pandas Styler object. This object can be\n",
        "            rendered directly in environments like Jupyter notebooks or can be\n",
        "            further processed to generate HTML or LaTeX output.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the input is a pandas DataFrame.\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(f\"Input `df` must be a pandas DataFrame, but got {type(df)}.\")\n",
        "\n",
        "    # --- 1. Initialize the Styler and Set Basic Formatting ---\n",
        "    # `df.style` creates the Styler object.\n",
        "    styler = df.style\n",
        "\n",
        "    # Set the main title for the table.\n",
        "    styler = styler.set_caption(caption)\n",
        "\n",
        "    # Apply global formatting to all cells. This sets the number of decimal\n",
        "    # places for floats and defines how NaN values should be displayed.\n",
        "    styler = styler.format(precision=precision, na_rep=\"N/A\")\n",
        "\n",
        "    # --- 2. Apply CSS Styles for Professional Appearance ---\n",
        "    # `set_table_styles` allows for applying custom CSS to different parts of the table.\n",
        "    styler = styler.set_table_styles([\n",
        "        # Style for table headers (<th> tags in HTML).\n",
        "        {'selector': 'th', 'props': [\n",
        "            ('text-align', 'center'),\n",
        "            ('font-weight', 'bold'),\n",
        "            ('background-color', '#f2f2f2')\n",
        "        ]},\n",
        "        # Style for data cells (<td> tags in HTML).\n",
        "        {'selector': 'td', 'props': [\n",
        "            ('text-align', 'center'),\n",
        "            ('padding', '5px')\n",
        "        ]},\n",
        "        # Style for the table caption.\n",
        "        {'selector': 'caption', 'props': [\n",
        "            ('font-size', '1.2em'),\n",
        "            ('font-weight', 'bold'),\n",
        "            ('margin', '10px 0px')\n",
        "        ]}\n",
        "    ])\n",
        "\n",
        "    # --- 3. Apply Conditional Highlighting ---\n",
        "    # Check if the `highlight_min_axis` argument was provided.\n",
        "    if highlight_min_axis is not None:\n",
        "        # `highlight_min` is a built-in Styler method that finds the minimum\n",
        "        # value along the specified axis (0 for columns, 1 for rows) and\n",
        "        # applies the given CSS properties to that cell.\n",
        "        styler = styler.highlight_min(\n",
        "            axis=highlight_min_axis,\n",
        "            props='font-weight: bold; color: #0055A4;' # e.g., bold and blue\n",
        "        )\n",
        "\n",
        "    # Return the final, configured Styler object.\n",
        "    return styler\n",
        "\n",
        "def run_comprehensive_results_assembly(\n",
        "    artifact_paths: Dict[str, Path],\n",
        "    entailment_ablation_results: Optional[pd.DataFrame] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Assembles, validates, and presents all computed results in publication-quality tables.\n",
        "\n",
        "    This master reporting function loads all key artifacts generated by the main\n",
        "    research pipeline, validates their presence, and then generates a series of\n",
        "    styled tables that reproduce the main findings presented in the paper.\n",
        "\n",
        "    Args:\n",
        "        artifact_paths: A dictionary mapping artifact names to their file paths,\n",
        "                        as returned by `run_full_research_pipeline`.\n",
        "        entailment_ablation_results: An optional DataFrame containing the results\n",
        "                                     from the human-in-the-loop entailment model\n",
        "                                     comparison.\n",
        "\n",
        "    Returns:\n",
        "        A \"master results database\" dictionary containing all the key loaded\n",
        "        DataFrames for further interactive analysis.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If a required artifact is not found at its specified path.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Running Task 34: Comprehensive Results Assembly ---\")\n",
        "\n",
        "    # --- Step 1: Aggregate All Computed Metrics and Results (by loading) ---\n",
        "    logging.info(\"Step 1: Loading all computed result artifacts...\")\n",
        "\n",
        "    master_results_db: Dict[str, Any] = {}\n",
        "\n",
        "    # Load each required artifact, checking for existence first.\n",
        "    for name, path in artifact_paths.items():\n",
        "        if not path.exists():\n",
        "            raise FileNotFoundError(f\"Required artifact '{name}' not found at path: {path}\")\n",
        "\n",
        "        if path.suffix == '.pkl':\n",
        "            master_results_db[name] = pd.read_pickle(path)\n",
        "        elif path.suffix == '.csv':\n",
        "            # Attempt to read with a multi-index if appropriate.\n",
        "            try:\n",
        "                df = pd.read_csv(path, index_col=[0, 1])\n",
        "            except (IndexError, ValueError):\n",
        "                try:\n",
        "                    df = pd.read_csv(path, index_col=0)\n",
        "                except (IndexError, ValueError):\n",
        "                    df = pd.read_csv(path)\n",
        "            master_results_db[name] = df\n",
        "\n",
        "    # Add the optional, externally computed entailment ablation results.\n",
        "    if entailment_ablation_results is not None:\n",
        "        master_results_db['entailment_ablation'] = entailment_ablation_results\n",
        "\n",
        "    logging.info(f\"Successfully loaded {len(master_results_db)} artifacts into the master results database.\")\n",
        "\n",
        "    # --- Step 2: Create Master Results Database (already done by loading) ---\n",
        "    # The `master_results_db` dictionary is our implementation of this concept.\n",
        "    # We can now save this entire collection as a single artifact for ultimate portability.\n",
        "    master_db_path = Path(artifact_paths.get('robustness_profile', Path(\"results/\")).parent) / \"master_results_database.pkl\"\n",
        "    pd.to_pickle(master_results_db, master_db_path)\n",
        "    logging.info(f\"Step 2: Master results database saved to '{master_db_path}'.\")\n",
        "\n",
        "    # --- Step 3: Generate Publication-Quality Tables and Figures ---\n",
        "    logging.info(\"\\nStep 3: Generating publication-quality tables...\")\n",
        "\n",
        "    # --- Table 3: MSE Comparison ---\n",
        "    mse_table = master_results_db['robustness_profile'][['MSE']].reset_index().pivot(\n",
        "        index='regime', columns='model_type', values='MSE'\n",
        "    )\n",
        "    display(_style_table(mse_table, \"Table 3 (Reproduced): Mean Squared Error (MSE)\", highlight_min_axis=1))\n",
        "\n",
        "    # --- Full Robustness Profile Table ---\n",
        "    display(_style_table(master_results_db['robustness_profile'], \"Full Robustness Profile\", precision=3))\n",
        "\n",
        "    # --- Table 6: Case Study ---\n",
        "    case_study_df = master_results_db['case_study_results']\n",
        "    # To match the paper's format, we can display one table per model.\n",
        "    for model_type in case_study_df.index.get_level_values('model_type').unique():\n",
        "        table_6_like = case_study_df.xs(model_type, level='model_type')\n",
        "        display(_style_table(table_6_like, f\"Table 6 (Reproduced): Case Study for Model '{model_type}'\", precision=3))\n",
        "\n",
        "    # --- Table 7: Metric Ablation ---\n",
        "    metric_ablation_df = master_results_db['metric_ablation']\n",
        "    display(_style_table(metric_ablation_df, \"Table 7 (Reproduced): Metric Ablation Analysis\", precision=2, na_rep=\"N/A\"))\n",
        "\n",
        "    # --- Table 8: Feature Augmentation Ablation ---\n",
        "    feature_ablation_df = master_results_db.get('feature_ablation')\n",
        "    if feature_ablation_df is not None:\n",
        "         display(_style_table(feature_ablation_df, \"Table 8 (Reproduced): Feature Augmentation Ablation\", precision=3))\n",
        "\n",
        "    # --- Table 9: Entailment Model Comparison ---\n",
        "    entailment_ablation_df = master_results_db.get('entailment_ablation')\n",
        "    if entailment_ablation_df is not None:\n",
        "        display(_style_table(entailment_ablation_df, \"Table 9 (Reproduced): Entailment Model Comparison\", precision=2))\n",
        "    else:\n",
        "        logging.warning(\"Entailment ablation results not provided; skipping Table 9.\")\n",
        "\n",
        "    # --- Transferability Matrix Visualization ---\n",
        "    transfer_matrices = master_results_db.get('transferability_matrices')\n",
        "    if transfer_matrices is not None:\n",
        "        for model_type, matrix in transfer_matrices.items():\n",
        "            plt.figure(figsize=(14, 10))\n",
        "            sns.heatmap(matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=.5, center=1.0)\n",
        "            plt.title(f\"Cross-Sector Transferability Matrix (Ratio) for {model_type}\", fontsize=16)\n",
        "            plt.xlabel(\"Target Sector (Evaluation)\", fontsize=12)\n",
        "            plt.ylabel(\"Source Sector (Training)\", fontsize=12)\n",
        "            plt.show()\n",
        "\n",
        "    logging.info(\"\\n>>> Comprehensive results assembly and reporting completed successfully. <<<\")\n",
        "\n",
        "    return master_results_db\n"
      ],
      "metadata": {
        "id": "Pg7Rp_xxexht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 35: Results Validation and Quality Assurance\n",
        "\n",
        "def run_results_validation_and_synthesis(\n",
        "    master_results_db: Dict[str, Any],\n",
        "    enriched_predictions_df: pd.DataFrame\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Performs a final validation, statistical testing, and synthesis of all results.\n",
        "\n",
        "    This master analytical function serves as the final step of the research\n",
        "    pipeline. It takes the database of all computed results and:\n",
        "    1.  Validates key findings against benchmarks from the source paper.\n",
        "    2.  Performs statistical significance tests on key comparisons.\n",
        "    3.  Generates a comprehensive, structured text report summarizing the\n",
        "        study's findings, limitations, and implications.\n",
        "\n",
        "    Args:\n",
        "        master_results_db: The dictionary containing all result DataFrames.\n",
        "        enriched_predictions_df: The DataFrame with per-sample predictions\n",
        "                                 and errors, required for t-tests.\n",
        "\n",
        "    Returns:\n",
        "        A formatted string containing the full analytical report.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Running Task 35: Results Validation and Quality Assurance ---\")\n",
        "\n",
        "    # Initialize a list to build the report string.\n",
        "    report_parts = [\"=\"*80, \"FINAL ANALYTICAL REPORT: Quantifying Semantic Shift in Financial NLP\", \"=\"*80 + \"\\n\"]\n",
        "\n",
        "    # --- Step 1: Validate Results Against Paper Benchmarks ---\n",
        "    report_parts.append(\"\\n--- 1. Validation Against Paper Benchmarks ---\\n\")\n",
        "\n",
        "    # Define benchmark values from the paper's tables.\n",
        "    benchmarks = {\n",
        "        'MSE': {\n",
        "            ('Pre-COVID', 'lstm'): 3.08,\n",
        "            ('COVID', 'text_transformer'): 40.95,\n",
        "        },\n",
        "        'Feature_Ablation_Cross_MSE': {\n",
        "            'Text Only': 0.501,\n",
        "            'Feature Enhanced': 0.469\n",
        "        }\n",
        "    }\n",
        "    tolerance = 0.15 # Allow for 15% tolerance due to stochasticity.\n",
        "\n",
        "    # Validate MSE benchmarks\n",
        "    mse_table = master_results_db['mse_table']\n",
        "    for (regime, model), bench_val in benchmarks['MSE'].items():\n",
        "        try:\n",
        "            comp_val = mse_table.loc[regime, model]\n",
        "            is_close = np.isclose(comp_val, bench_val, rtol=tolerance)\n",
        "            status = \"PASSED\" if is_close else \"FAILED\"\n",
        "            report_parts.append(\n",
        "                f\"  - MSE Benchmark Check for ({regime}, {model}): \"\n",
        "                f\"Computed={comp_val:.2f}, Paper={bench_val}, Tolerance={tolerance:.0%}. Status: {status}\"\n",
        "            )\n",
        "        except KeyError:\n",
        "            report_parts.append(f\"  - MSE Benchmark Check for ({regime}, {model}): FAILED (Computed value not found)\")\n",
        "\n",
        "    # Validate Feature Ablation pattern\n",
        "    feature_ablation = master_results_db.get('feature_ablation')\n",
        "    if feature_ablation is not None:\n",
        "        text_only_mse = feature_ablation.loc['Text Only', 'Cross-Sector MSE']\n",
        "        feature_enhanced_mse = feature_ablation.loc['Feature Enhanced', 'Cross-Sector MSE']\n",
        "        pattern_holds = feature_enhanced_mse < text_only_mse\n",
        "        status = \"PASSED\" if pattern_holds else \"FAILED\"\n",
        "        report_parts.append(\n",
        "            f\"  - Feature Ablation Pattern Check (Feature Enhanced < Text Only): \"\n",
        "            f\"{feature_enhanced_mse:.3f} < {text_only_mse:.3f}. Status: {status}\"\n",
        "        )\n",
        "\n",
        "    # --- Step 2: Implement Statistical Significance Testing ---\n",
        "    report_parts.append(\"\\n--- 2. Statistical Significance Analysis ---\\n\")\n",
        "\n",
        "    # --- Paired t-tests for MSE comparisons within a regime ---\n",
        "    report_parts.append(\"  - Paired t-tests on MSE differences (p < 0.05 is significant):\")\n",
        "    regimes = enriched_predictions_df['regime'].unique()\n",
        "    models = enriched_predictions_df['model_type'].unique()\n",
        "\n",
        "    for regime in regimes:\n",
        "        regime_df = enriched_predictions_df[enriched_predictions_df['regime'] == regime]\n",
        "        if len(models) > 1:\n",
        "            # Compare the first two models as an example.\n",
        "            model1, model2 = models[0], models[1]\n",
        "            errors1 = regime_df[regime_df['model_type'] == model1]['squared_error']\n",
        "            errors2 = regime_df[regime_df['model_type'] == model2]['squared_error']\n",
        "\n",
        "            # Ensure samples are aligned for pairing.\n",
        "            aligned_errors = pd.DataFrame({'m1': errors1, 'm2': errors2}).dropna()\n",
        "            if len(aligned_errors) > 10:\n",
        "                t_stat, p_value = stats.ttest_rel(aligned_errors['m1'], aligned_errors['m2'])\n",
        "                report_parts.append(\n",
        "                    f\"    - In '{regime}', {model1} vs. {model2}: p-value = {p_value:.4f}\"\n",
        "                )\n",
        "\n",
        "    # --- Correlation Significance ---\n",
        "    corr_analysis = master_results_db.get('correlation_analysis')\n",
        "    if corr_analysis is not None:\n",
        "        report_parts.append(\"\\n  - Significance of Semantic Drift Correlation (Spearman's Rho):\")\n",
        "        for model_type, row in corr_analysis.iterrows():\n",
        "            report_parts.append(\n",
        "                f\"    - For '{model_type}': rho = {row['spearman_rho']:.3f}, p-value = {row['p_value']:.4f}\"\n",
        "            )\n",
        "\n",
        "    # --- Step 3: Create Results Interpretation Guidelines & Summary ---\n",
        "    report_parts.append(\"\\n--- 3. Synthesis and Interpretation ---\\n\")\n",
        "\n",
        "    # --- Key Findings ---\n",
        "    report_parts.append(\"  - Key Findings:\")\n",
        "    # Programmatically find the most volatile regime and most sensitive model.\n",
        "    most_volatile_regime = mse_table.unstack().idxmax()[1]\n",
        "    most_sensitive_model = mse_table.std(axis=0).idxmax()\n",
        "    most_stable_model = mse_table.std(axis=0).idxmin()\n",
        "\n",
        "    report_parts.append(f\"    1. Model performance degrades significantly during crisis periods, with the '{most_volatile_regime}' regime showing the highest MSE for the '{mse_table.unstack().idxmax()[0]}' model.\")\n",
        "    report_parts.append(f\"    2. The '{most_sensitive_model}' model exhibits the highest variance in performance across regimes, making it the most sensitive to semantic drift.\")\n",
        "    report_parts.append(f\"    3. The '{most_stable_model}' model shows the most consistent performance, making it the most robust to regime shifts.\")\n",
        "    if corr_analysis is not None and (corr_analysis['p_value'] < 0.1).any():\n",
        "        report_parts.append(\"    4. There is a statistically significant positive correlation between semantic drift (J-S Divergence) and model error (MSE degradation), confirming the paper's central hypothesis.\")\n",
        "    if feature_ablation is not None and pattern_holds:\n",
        "        report_parts.append(\"    5. Feature enhancement (combining TF-IDF and embeddings) improves cross-sector generalization compared to using text features alone.\")\n",
        "\n",
        "    # --- Limitations ---\n",
        "    report_parts.append(\"\\n  - Limitations:\")\n",
        "    report_parts.append(\"    1. The correlation analysis is based on a small sample of regime pairs (N=6), limiting its statistical power. Results should be seen as indicative.\")\n",
        "    report_parts.append(\"    2. Causal metrics (FCAS, PCS) rely on simplified keyword-based methods, which may not capture the full nuance of financial narratives.\")\n",
        "    report_parts.append(\"    3. The study is limited to a specific set of models and time periods; findings may not generalize to all market conditions or architectures.\")\n",
        "\n",
        "    # --- Implications ---\n",
        "    report_parts.append(\"\\n  - Implications for Model Governance:\")\n",
        "    report_parts.append(\"    1. Static models trained on historical data are unreliable in non-stationary financial markets. Continuous monitoring of performance and data drift is essential.\")\n",
        "    report_parts.append(\"    2. The diagnostic metric suite (FCAS, PCS, TSV, NLICS) provides a powerful toolkit for model validation, stress testing, and identifying failure modes beyond simple accuracy metrics.\")\n",
        "    report_parts.append(\"    3. An effective governance system must include triggers for model retraining or recalibration based on quantitative measures of semantic drift (like TSV or J-S Divergence).\")\n",
        "\n",
        "    # --- Final Assembly ---\n",
        "    final_report = \"\\n\".join(report_parts)\n",
        "    logging.info(\"\\n\" + final_report)\n",
        "\n",
        "    # Save the report to a file.\n",
        "    report_path = Path(master_results_db.get('robustness_profile', pd.DataFrame()).index.name if hasattr(master_results_db.get('robustness_profile'), 'index') and master_results_db.get('robustness_profile').index.name is not None else \"results\") / \"final_analytical_report.txt\"\n",
        "    report_path.parent.mkdir(exist_ok=True)\n",
        "    with open(report_path, \"w\") as f:\n",
        "        f.write(final_report)\n",
        "    logging.info(f\"Final analytical report saved to '{report_path}'\")\n",
        "\n",
        "    return final_report\n"
      ],
      "metadata": {
        "id": "CNoUmB1dnJPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator\n",
        "\n",
        "def execute_quantifying_semantic_shift_study(\n",
        "    raw_df: pd.DataFrame,\n",
        "    study_config: Dict[str, Any],\n",
        "    results_dir: Path = Path(\"results\"),\n",
        "    run_entailment_ablation: bool = False,\n",
        "    annotated_benchmark_filename: str = \"nli_benchmark_annotated.csv\",\n",
        "    force_rerun_main_training: bool = False,\n",
        "    force_rerun_cross_sector: bool = False\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete, end-to-end \"Quantifying Semantic Shift\" research study.\n",
        "\n",
        "    This top-level orchestrator serves as the main entry point for the entire\n",
        "    project. It manages the full workflow from initial data validation to the\n",
        "    generation of the final analytical report. It is designed to be robust,\n",
        "    resumable, and methodologically rigorous.\n",
        "\n",
        "    The workflow is as follows:\n",
        "    1.  Calls `run_full_research_pipeline` to perform all automated computational\n",
        "        tasks, from data validation and feature engineering to model training,\n",
        "        inference, and all subsequent analyses (Tasks 1-33). This step\n",
        "        leverages extensive caching and idempotency for robustness.\n",
        "    2.  Optionally, if `run_entailment_ablation` is True, it proceeds with the\n",
        "        human-in-the-loop analysis. It verifies the existence of a properly\n",
        "        annotated benchmark file and runs the entailment model comparison.\n",
        "    3.  Calls `run_comprehensive_results_assembly` to load all generated artifacts\n",
        "        and produce publication-quality tables and figures.\n",
        "    4.  Calls `run_results_validation_and_synthesis` to perform final statistical\n",
        "        tests and generate a comprehensive, text-based analytical summary.\n",
        "\n",
        "    Args:\n",
        "        raw_df: The initial, raw pandas DataFrame.\n",
        "        study_config: The complete study configuration dictionary.\n",
        "        results_dir: The root directory to save all artifacts.\n",
        "        run_entailment_ablation: If True, the function will attempt to run the\n",
        "            final entailment model ablation study, which requires the\n",
        "            human-annotated benchmark file to be present and valid.\n",
        "        annotated_benchmark_filename: The filename of the human-annotated\n",
        "            benchmark CSV file, expected to be inside the `results_dir`.\n",
        "        force_rerun_main_training: If True, forces retraining of the 12 main models.\n",
        "        force_rerun_cross_sector: If True, forces retraining of cross-sector models.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the key final outputs of the study.\n",
        "    \"\"\"\n",
        "    logging.info(\"=\"*100)\n",
        "    logging.info(\"STARTING TOP-LEVEL ORCHESTRATOR: QUANTIFYING SEMANTIC SHIFT STUDY\")\n",
        "    logging.info(\"=\"*100)\n",
        "\n",
        "    # This dictionary will hold all final results.\n",
        "    final_study_outputs: Dict[str, Any] = {}\n",
        "\n",
        "    # --- Step i: Run the Full Automated Research Pipeline ---\n",
        "    # This function handles Tasks 1-33 (excluding the manual part of Task 29).\n",
        "    # It is idempotent and will use cached artifacts where possible.\n",
        "    artifact_paths = run_full_research_pipeline(\n",
        "        raw_df=raw_df,\n",
        "        study_config=study_config,\n",
        "        results_dir=results_dir,\n",
        "        force_rerun=(force_rerun_main_training or force_rerun_cross_sector)\n",
        "    )\n",
        "    final_study_outputs['artifact_paths'] = artifact_paths\n",
        "\n",
        "    # --- Step ii: Optionally Run the Entailment Ablation Analysis ---\n",
        "    entailment_ablation_results = None\n",
        "    if run_entailment_ablation:\n",
        "        logging.info(\"\\n--- Attempting to run Entailment Ablation Analysis ---\")\n",
        "        # This is the rigorous, non-interactive check. We verify, we don't ask.\n",
        "        annotated_benchmark_path = results_dir / annotated_benchmark_filename\n",
        "        enriched_predictions_path = artifact_paths['enriched_predictions']\n",
        "\n",
        "        try:\n",
        "            # This function contains its own validation for the annotated file.\n",
        "            entailment_ablation_results = run_entailment_ablation_analysis(\n",
        "                enriched_predictions_df_path=enriched_predictions_path,\n",
        "                annotated_benchmark_path=annotated_benchmark_path,\n",
        "                study_config=study_config\n",
        "            )\n",
        "            final_study_outputs['entailment_ablation_results'] = entailment_ablation_results\n",
        "        except (FileNotFoundError, ValueError) as e:\n",
        "            # If the file is missing or invalid, we log a critical warning and continue.\n",
        "            # This makes the pipeline robust.\n",
        "            logging.error(f\"Could not run entailment ablation analysis: {e}\")\n",
        "            logging.warning(\"Please ensure the benchmark file has been correctly annotated and saved.\")\n",
        "    else:\n",
        "        logging.info(\"\\nSkipping Entailment Ablation Analysis as per configuration.\")\n",
        "\n",
        "    # --- Step iii & iv: Prepare and Run Comprehensive Results Assembly ---\n",
        "    # This function loads all artifacts and generates all final tables/figures.\n",
        "    master_results_db = run_comprehensive_results_assembly(\n",
        "        artifact_paths=artifact_paths,\n",
        "        entailment_ablation_results=entailment_ablation_results\n",
        "    )\n",
        "    final_study_outputs['master_results_database'] = master_results_db\n",
        "\n",
        "    # --- Step v: Run Final Validation and Synthesis ---\n",
        "    # This function performs statistical tests and generates the final text report.\n",
        "    final_analytical_report = run_results_validation_and_synthesis(\n",
        "        master_results_db=master_results_db,\n",
        "        enriched_predictions_df=master_results_db['enriched_predictions_df']\n",
        "    )\n",
        "    final_study_outputs['final_analytical_report'] = final_analytical_report\n",
        "\n",
        "    # --- Step vi: Return Final Outputs ---\n",
        "    logging.info(\"\\n\" + \"=\"*100)\n",
        "    logging.info(\"TOP-LEVEL ORCHESTRATOR FINISHED SUCCESSFULLY.\")\n",
        "    logging.info(\"=\"*100)\n",
        "\n",
        "    return final_study_outputs\n",
        "\n"
      ],
      "metadata": {
        "id": "sIbaIVGJxZDI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}